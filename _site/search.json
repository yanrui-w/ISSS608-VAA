[
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html",
    "title": "Take-Home Exercise 3: Be Weatherwise or Otherwise",
    "section": "",
    "text": "Here is an office report as shown in the infographic below, after reviewing it, certain figures and statements have captured my attention.\n\n\n\nClimate_change\n\n\nAs shown in “DAILY TEMPERATURE” sector:\n\nFrom 1948 to 2016, annual mean temperatures rose at an average rate of 0.25℃ per decade.\nDaily mean temperatures are projected to increase by 1.4℃ to 4.6℃.\n\nThe annual mean temperatures rose in a slow speed at 0.25℃ per every 10 years. However, the daily mean temperatures are expected to increase by at least 1.4℃ which is nearly equal to the increasement of annual mean temperatures in 60 years. This sounds a bit confusing and need in-depth study."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#install-and-load-packages",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#install-and-load-packages",
    "title": "Take-Home Exercise 3: Be Weatherwise or Otherwise",
    "section": "3.1 Install and load packages",
    "text": "3.1 Install and load packages\nIn this study, I’ll use several R packages shown as below.\n\npacman::p_load(tidyverse,ggdist, ggridges, \n               ggthemes,colorspace,ungeviz,\n               plotly,crosstalk,DT,gganimate,\n               gifski, gapminder)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#data-wrangling",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#data-wrangling",
    "title": "Take-Home Exercise 3: Be Weatherwise or Otherwise",
    "section": "3.2 Data Wrangling",
    "text": "3.2 Data Wrangling\nFirst, I’ll import all data downloaded from website.\n\n\nCode\ndata_1983 &lt;- read_csv(\"data/DAILYDATA_S24_198312.csv\")\ndata_1993 &lt;- read_csv(\"data/DAILYDATA_S24_199312.csv\")\ndata_2003 &lt;- read_csv(\"data/DAILYDATA_S24_200312.csv\")\ndata_2013 &lt;- read_csv(\"data/DAILYDATA_S24_201312.csv\")\ndata_2023 &lt;- read_csv(\"data/DAILYDATA_S24_202312.csv\")\n\n\nThen, I’ll check all variables in these files.\n\n\nCode\ndata_1983\ndata_1993\ndata_2003\ndata_2013\ndata_2023\n\n\nUpon reviewing the data, there are some issues need to be addressed.\n1. Re-match the values with the correct column\nFor the year 1983, 1993, 2003, 2013, the values of “Mean Temperature” are wrongly listed in column “Highest 30 Min Rainfall (mm)” due to the missing values on “Highest 30 Min Rainfall (mm)” in these years and the treatment of missing values when imported into R. The values need to be re-matched to column “Mean Temperature”.\n2. Remove special characters “???” from the values of “Mean Temperature”\nFor the year 1983, 1993, 2003, 2013, the values of “Mean Temperature” start with “???” which need to be removed.\nThe codes are shown as below:\n\n\nCode\n# Remove \"???\"\ndata_1983$\"Highest 30 Min Rainfall (mm)\" &lt;- \n  as.numeric(gsub(\"^\\\\?+\", \"\", data_1983$\"Highest 30 Min Rainfall (mm)\")) \ndata_1993$\"Highest 30 Min Rainfall (mm)\" &lt;- \n  as.numeric(gsub(\"^\\\\?+\", \"\", data_1993$\"Highest 30 Min Rainfall (mm)\"))\ndata_2003$\"Highest 30 Min Rainfall (mm)\" &lt;- \n  as.numeric(gsub(\"^\\\\?+\", \"\", data_2003$\"Highest 30 Min Rainfall (mm)\"))\ndata_2013$\"Highest 30 Min Rainfall (mm)\" &lt;- \n  as.numeric(gsub(\"^\\\\?+\", \"\", data_2013$\"Highest 30 Min Rainfall (mm)\")) \n\n# Re-match values to correct column\ndata_1983 &lt;- data_1983 %&gt;%\n  mutate(`Mean Temperature`=`Highest 30 Min Rainfall (mm)`)\ndata_1993 &lt;- data_1993 %&gt;%\n  mutate(`Mean Temperature`=`Highest 30 Min Rainfall (mm)`)\ndata_2003 &lt;- data_2003 %&gt;%\n  mutate(`Mean Temperature`=`Highest 30 Min Rainfall (mm)`)\ndata_2013 &lt;- data_2013 %&gt;%\n  mutate(`Mean Temperature`=`Highest 30 Min Rainfall (mm)`)\n\n\nAfter pre-processing on data, we move on to prepare these data for further analysis.\n1. Combine the 5 files and focus on the variables need to be analyzed\nHere I pick “Station”, “Year”, “Month”, “Day”, “Mean Temperature” to do the further processing and remove all the other variables.\n\n\nCode\n# Uniform column names among all files\ncolnames(data_2023)[6] &lt;- \"Highest 30 Min Rainfall (mm)\"\ncolnames(data_2023)[7] &lt;- \"Highest 60 Min Rainfall (mm)\"\ncolnames(data_2023)[8] &lt;- \"Highest 120 Min Rainfall (mm)\"\n\n# Combine files and remove columns\ntemperature &lt;- rbind(data_1983, data_1993, data_2003, data_2013, data_2023)\ntemperature &lt;- temperature[, -c(5,6,7,8,10,11,12,13)]\n\n\n2. Combine Year, Month and Day into Date\n\n\nCode\ntemperature$Date &lt;- as.Date(paste(temperature$Year, \n                                  temperature$Month, \n                                  temperature$Day, \n                                  sep = \"-\"), \n                            \"%Y-%m-%d\")\n\n\n3. Calculate the change on daily mean temperature\nTo study on the increasment of daily mean temperature, I create a new column called “Mean_Change” to observe the variance on daily mean temerature. This variable is obtained by subtracting today’s daily mean temperature from that of yesterday.\n\n\nCode\ntemperature$Mean_Change &lt;- c(0, diff(temperature$\"Mean Temperature\"))\n\n\n4. Calculate the mean, standard deviation and standard error of daily mean temperature\nHere I apply statistic methods to assess the central tendency and interval estimate regarding the variance on daily mean temperature. I’ll calculate statistics such as mean, standard deviation and standard error first and add them to the plot later on.\n\n\nCode\nmy_summary &lt;- temperature %&gt;%\n  group_by(Year) %&gt;%\n  summarise(\n    n=n(),\n    mean=mean(Mean_Change),\n    sd=sd(Mean_Change)\n  ) %&gt;%\n  mutate(se=sd/sqrt(n-1))"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#methods",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#methods",
    "title": "Take-Home Exercise 3: Be Weatherwise or Otherwise",
    "section": "4.1 Methods",
    "text": "4.1 Methods\nI choose confidence interval plotting in this study because the aim of this study is to assess the estimate on the increasement or the change on daily mean temperature (Δ℃) and the uncertainty of the estimate as well.\n\nMean of change on daily mean temperature: will show the estimated increasement or change on daily mean temperature.\nConfidence interval of change on daily mean temperature: will provide a range which the estimated change on daily mean temperature will fall in due to uncertainty."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#principle",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#principle",
    "title": "Take-Home Exercise 3: Be Weatherwise or Otherwise",
    "section": "4.2 Principle",
    "text": "4.2 Principle\n\n4.2.1 Clarity\n\nX axis\nneed to be labeled to be the year 1983, 1993, 2003, 2013, 2023.\nY axis\nin most cases, the quantitative scale on y axis should begin at 0. But in this case, since I need to assess the variation on daily mean temperature, it might be negative or positive depending on whether today’s mean temperature is rising or dropping. So I’ll set the quantitative scale on y axis starting with negative numbers. Y axis title might be “Variation on daily mean temp (Δ℃)”.\nGraph title and subtitle\ncreate an attention-grabbing title together with a descriptive subtitle to bring better understanding to readers.\nCaption\nneed to mark the source of data.\nCall-outs of important information\nImportant information here refer to the mean and confidence interval of the change on daily mean temperature. I’ll apply interactivity to visualize the figures on the plot.\n\n\n\n4.2.2 Aesthetic\n\nData ink\nmake the plot clear to emphasize information\nGrid line\nremove irrelevant grid lines"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#final-plot",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#final-plot",
    "title": "Take-Home Exercise 3: Be Weatherwise or Otherwise",
    "section": "4.3 Final Plot",
    "text": "4.3 Final Plot\n\n\nCode\nshared_df = SharedData$new(my_summary)\n\np &lt;- ggplotly((ggplot(shared_df) +\n                 geom_errorbar(aes(\n                   x=Year,\n                   ymin=mean-2.58*se, \n                   ymax=mean+2.58*se), \n                   width=0.2, \n                   colour=\"black\", \n                   alpha=0.9, \n                   size=0.5) +\n                 geom_point(aes(\n                   x=Year, \n                   y=mean, \n                   text = paste(\"Year:\", `Year`, \n                                \"&lt;br&gt;N:\", `n`,\n                                \"&lt;br&gt;Avg. Scores:\", round(mean, digits = 2),\n                                \"&lt;br&gt;99% CI:[\", \n                                round((mean-2.58*se), digits = 2), \",\",\n                                round((mean+2.58*se), digits = 2),\"]\")),\n                   stat=\"identity\", \n                   color=\"red\", \n                   size = 1.5, \n                   alpha=1) + \n                 ylim(c(-0.5,0.8))+\n                 scale_x_continuous(breaks = c(1983, 1993, 2003, 2013, 2023),\n                                    labels = c(\"1983\", \"1993\", \"2003\", \"2013\", \"2023\"))+ \n                 theme_minimal(base_size = 7)+\n                 theme(axis.title.x = element_blank(),\n                       axis.title.y = element_blank(),\n                       panel.grid.major.x = element_blank(),\n                       panel.grid.minor.x = element_blank(),\n                       panel.grid.minor.y = element_blank())), \n              tooltip = \"text\")\np &lt;- layout(p,\n            annotations = list(\n              list(\n                text = \"The fluctuation in daily mean temperature exhibits extremely minor variations around 0℃.\\nAt 99% CI, the estimated change in daily mean temperature falls within a range of ±0.5℃.\\nSubtle changes over the period may still contribute to the overall global warming trend.\",\n                x = 0.5,\n                y = 0.97,\n                xref = \"paper\",\n                yref = \"paper\",\n                showarrow = FALSE,\n                font = list(size = 10)\n              ),\n              list(\n                text = \"Insight on variation of daily mean temperature in Singapore\",\n                x = 0.5,\n                y = 1.05,\n                xref = \"paper\",\n                yref = \"paper\",\n                showarrow = FALSE,\n                font = list(size = 20,\n                            family = \"Aptos Black\",\n                            weight = \"bold\")\n              ),\n              list(\n                text = \"Source: Meteorological Service Singapore\",\n                x = 1,\n                y = -0.072,\n                showarrow = FALSE,\n                xref = \"paper\",\n                yref = \"paper\",\n                font = list(size = 8)),\n              list(\n                text = \"Variation on daily \\nmean temp (Δ℃)\",\n                x = -0.045,\n                y = 0.855,\n                showarrow = FALSE,\n                xref = \"paper\",\n                yref = \"paper\",\n                font = list(size = 8)),\n              list(\n                text = \"Year\",\n                x = 0.5,\n                y = -0.072,\n                showarrow = FALSE,\n                xref = \"paper\",\n                yref = \"paper\",\n                font = list(size = 8))))\np"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html",
    "title": "Take-Home Exercise 2: DataVis Makeover",
    "section": "",
    "text": "In this take-home exercise, I will continue the topic on take-home exercise 1, select one of the outcome from my classmates, critic it in terms of clarity and aesthetics, prepare a sketch for the alternative design and remake the original design by using relevant R packages.\nFind the works I select from one of my classmates: Take-home Exercise 1 from ZHENG KAIXIN"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#original-design",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#original-design",
    "title": "Take-Home Exercise 2: DataVis Makeover",
    "section": "3.1 Original Design",
    "text": "3.1 Original Design\nThe original design is shown as below:  The 3 histograms show the distribution of average score of Math, Reading and Science."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#clarity",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#clarity",
    "title": "Take-Home Exercise 2: DataVis Makeover",
    "section": "3.2 Clarity",
    "text": "3.2 Clarity\n\n3.2.1 X axis Label\nThe x-axis scales on the three graphs are not aligned. While the Average Math Score has the highest mean and median values, the lines representing the mean and median are positioned to the left when compared to the lines for Average Reading Score and Average Science Score.\n\n\n3.2.2 Y axis Title\nThe Y axis title named as “Count” without specifying what is being counted might be a little vague.\n\n\n3.2.3 Graph Title\nGraph title could be enhanced to be more eye-catching for readers. Sometimes, an attention-grabbing title together with a descriptive subtitle can bring better understanding to readers."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#aesthetic",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#aesthetic",
    "title": "Take-Home Exercise 2: DataVis Makeover",
    "section": "3.3 Aesthetic",
    "text": "3.3 Aesthetic\n\n3.3.1 Font Size\nIn comparison to the graphs, the font size of x and y axis title is slightly larger, which might somehow divert people’s attention away from the graphs.\n\n\n3.3.2 Display of graph and key numbers\nIn this plot, 3 histograms are arranged vertically, leading to compression of each histogram. As a result, the differences in histogram presentations, including the comparison between the median and mean reference lines, appear less distinct due to the compression.\nI try to retain all the information my classmate want to convey (distribution graph, as well as key statistics: mean and median) and change the layout to better present all the parts: put 3 distributions together in one graph and show the statistics in another graph beside it by using boxplot."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#sketch-of-proposed-design",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#sketch-of-proposed-design",
    "title": "Take-Home Exercise 2: DataVis Makeover",
    "section": "3.4 Sketch of proposed design",
    "text": "3.4 Sketch of proposed design\n\n\n\ncritique1_after"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#data-wrangling",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#data-wrangling",
    "title": "Take-Home Exercise 2: DataVis Makeover",
    "section": "3.5 Data Wrangling",
    "text": "3.5 Data Wrangling\nTo display 3 density distributions in one graph, I need to create a new column called “Subject” to capture all the subject names of each student and a new column called “Score” to capture all the score values of each student. I use pivot_longer() to convert the dataframe from wide format into long format.\n\nnew_combined &lt;- combined[c(\"CNTSTUID\",\"Avg_Math\",\"Avg_Reading\",\"Avg_Science\")]\n# change to long format\nscore_combined &lt;- new_combined %&gt;%\n  pivot_longer(cols = starts_with(\"Avg_\"), \n               names_to = \"Subject\", \n               values_to = \"Score\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#final-design",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#final-design",
    "title": "Take-Home Exercise 2: DataVis Makeover",
    "section": "3.6 Final Design",
    "text": "3.6 Final Design\n\n\nCode\nmedian_values &lt;- aggregate(Score ~ Subject, data = score_combined, FUN = median)\nmean_values &lt;- aggregate(Score ~ Subject, data = score_combined, FUN = mean)\n\nplot1 &lt;- ggplot(data=score_combined, \n                aes(x = Score, \n                    colour = Subject)) +\n  geom_density()+\n  labs(x = \"Score\",y=\"No.% of\\nStudents\",\n       title = \"Insights from Performance in Math, Reading and Science\",\n       subtitle= \"Singaporean 15-year-olds excel in Science, with scores concentrated around 571 points, and demonstrate strong\\nMath performance, with many students exceeding the 574-point mean. Reading scores are slightly lower in comparison.\\n\")+\n  theme_minimal(base_size = 7)+\n  theme(legend.position = \"bottom\",\n        panel.grid.major.x = element_blank(),\n        panel.grid.minor.x = element_blank(),\n        panel.grid.minor.y = element_blank(),\n        axis.title.y= element_text(angle=0),\n        plot.title = element_text(size=10, face=\"bold\", hjust = 0.2),\n        plot.subtitle = element_text(size=7, hjust = 0.2))+\n  scale_color_manual(\n    breaks = c(\"Avg_Math\", \"Avg_Reading\", \"Avg_Science\"),\n    labels = c(\"Math\", \"Reading\", \"Science\"),\n    values = c(\"lightpink3\",\"seagreen3\",\"royalblue3\"))\n\nplot2 &lt;- ggplot(data=score_combined, \n                aes(y = Score, x= Subject,\n                    colour = Subject)) +\n  geom_boxplot() +\n  stat_summary(geom = \"point\",       \n               fun.y=\"mean\",         \n               colour =\"orangered\",        \n               size=1)+\n  geom_text(\n    data = median_values,\n    aes(x = Subject, y = Score, label = paste(\"Median:\",round(Score, 2))),\n    vjust = -0.5,  \n    colour = \"orangered3\",  \n    size = 1.8)+\n  geom_text(\n    data = mean_values,\n    aes(x = Subject, y = Score, label = paste(\"Mean:\",round(Score, 2))),\n    vjust = 1.2,  \n    colour = \"orangered\",  \n    size = 1.8)+\n  theme_minimal(base_size = 7)+\n  labs(caption = \"Source: PISA 2022\")+\n  guides(color = guide_legend(\n    title = \"Subject\",\n    label.hjust = 0,   \n    label.vjust = 0,  \n    nrow = 1))+\n  theme(legend.position = \"bottom\",\n        axis.text.x = element_blank(),\n        axis.title.y= element_text(angle=0))+\n  scale_color_manual(\n    breaks = c(\"Avg_Math\", \"Avg_Reading\", \"Avg_Science\",\"Median:\",\"Mean:\"),\n    labels = c(\"Math\", \"Reading\", \"Science\",\"Median:\",\"Mean:\"),\n    values = c(\"lightpink3\",\"seagreen3\",\"royalblue3\",\"orangered3\",\"orangered\"))\n\nplot1+plot2"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#original-design-1",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#original-design-1",
    "title": "Take-Home Exercise 2: DataVis Makeover",
    "section": "4.1 Original Design",
    "text": "4.1 Original Design\nThe original design is shown as below:  This graph shows the distributions of average score of all the 3 subjects across all socioeconomic groups."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#clarity-1",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#clarity-1",
    "title": "Take-Home Exercise 2: DataVis Makeover",
    "section": "4.2 Clarity",
    "text": "4.2 Clarity\n\n4.2.1 Y axis label\nY axis is labelled as something like “1e-03”, “5e-04” might cause confusing to readers.\n\n\n4.2.2 Socioeconomic Groups\nThe socioeconomic hierarchy is divided into 5 levels: 1-5. It might be more user-friendly to rename them by verbal description such as “Lower Group”, “Mid-Lower Group”, “Middle Group”, “Upper-Middle Group”, “Higher Group”.\n\n\n4.2.3 Graph Title\nGraph title could be enhanced to be more eye-catching for readers. Sometimes, an attention-grabbing title together with a descriptive subtitle can bring better understanding to readers."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#aesthetic-1",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#aesthetic-1",
    "title": "Take-Home Exercise 2: DataVis Makeover",
    "section": "4.3 Aesthetic",
    "text": "4.3 Aesthetic\n\n4.3.1 Display of graph\nSince the graphs of 5 groups are listed vertically and slightly similar at the first glance, it would be a little hard for readers to tell the difference by tracing the movement of the peak point of 5 graphs.\nI try to convert the graphs by showing the 90% confident interval of the mean of each graph. In addition, I will use interactive tool to show the value of mean so that the slight difference might be more distinct by directly presenting the mean and showing the value."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#sketch-of-proposed-design-1",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#sketch-of-proposed-design-1",
    "title": "Take-Home Exercise 2: DataVis Makeover",
    "section": "4.4 Sketch of proposed design",
    "text": "4.4 Sketch of proposed design\n\n\n\ncritique2_after"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#data-wrangling-1",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#data-wrangling-1",
    "title": "Take-Home Exercise 2: DataVis Makeover",
    "section": "4.5 Data Wrangling",
    "text": "4.5 Data Wrangling\nBefore plotting, I need to rename the socioeconomic groups.\n\ncombined$socioeconomic_group &lt;- factor(combined$socioeconomic_group,\n                                       levels = c(1, 2, 3, 4, 5),\n                                       labels = c(\"Lower\\nGroup\", \"Mid-Lower\\nGroup\", \"Middle\\nGroup\", \"Upper-Middle\\nGroup\", \"Higher\\nGroup\"))"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#final-design-1",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#final-design-1",
    "title": "Take-Home Exercise 2: DataVis Makeover",
    "section": "4.6 Final Design",
    "text": "4.6 Final Design\n\n\nCode\npacman::p_load(ggiraph, plotly, \n               patchwork, DT)\ntooltip &lt;- function(y, ymax, accuracy = .01) {\n  mean &lt;- scales::number(y, accuracy = accuracy)\n  sem &lt;- scales::number(ymax - y, accuracy = accuracy)\n  paste(\"Mean scores:\", mean, \"+/-\", sem)\n}\n  \ngg_point &lt;- ggplot(data=combined, \n                    aes(x = socioeconomic_group)) +\n  stat_summary(aes(y = Total_Avg, \n                   tooltip = after_stat(\n                     tooltip(y, ymax))),  \n               fun.data = \"mean_se\", \n               geom = GeomInteractiveCol,  \n               fill = \"light blue\") +\n  stat_summary(aes(y = Total_Avg),\n               fun.data = mean_se,\n               geom = \"errorbar\", width = 0.2,\n               size = 0.2)+\n  coord_cartesian(ylim = c(0,2000))+\n  labs(y= 'Mean\\nScore', x= 'Socioeconomic Group',\n       title = \"Educational Equity is Bridging Socioeconomic Divides in Education\",\n       subtitle= \"Over 6,000 15-year-old Singaporean students across socioeconomic groups displayed consistent academic\\nperformance with the Upper-Middle Group slightly outshining the rest yet the Lower Group lagging a little.\",\n       caption = \"Source: PISA 2022\")+\n  theme_minimal(base_size = 10)+\n  theme(plot.title = element_text(size=15, face=\"bold\", hjust = 0.5),\n        plot.subtitle = element_text(size=10, hjust = 0.5),\n        axis.title.y = element_text(angle = 0, vjust = 0.5),\n        panel.background = element_blank(),\n        panel.grid.major = element_line(color = \"grey90\"),\n        panel.grid.minor = element_blank(),\n        plot.caption = element_text(hjust = 1))\n  \ngirafe(ggobj = gg_point,\n       width_svg = 8,\n       height_svg = 8*0.618)"
  },
  {
    "objectID": "In-Class_Ex/In-Class_Ex01/In-Class_Ex01.html",
    "href": "In-Class_Ex/In-Class_Ex01/In-Class_Ex01.html",
    "title": "In-Class Exercise 1: Now You See It!",
    "section": "",
    "text": "In this hands-on exercise, two R packages will be used. They are:\n\ntidyverse, and\nhaven\n\nThe code chunk used is as follows:\n\npacman::p_load(tidyverse, haven)"
  },
  {
    "objectID": "In-Class_Ex/In-Class_Ex01/In-Class_Ex01.html#loading-r-packages",
    "href": "In-Class_Ex/In-Class_Ex01/In-Class_Ex01.html#loading-r-packages",
    "title": "In-Class Exercise 1: Now You See It!",
    "section": "",
    "text": "In this hands-on exercise, two R packages will be used. They are:\n\ntidyverse, and\nhaven\n\nThe code chunk used is as follows:\n\npacman::p_load(tidyverse, haven)"
  },
  {
    "objectID": "In-Class_Ex/In-Class_Ex01/In-Class_Ex01.html#importing-pisa-data",
    "href": "In-Class_Ex/In-Class_Ex01/In-Class_Ex01.html#importing-pisa-data",
    "title": "In-Class Exercise 1: Now You See It!",
    "section": "Importing PISA data",
    "text": "Importing PISA data\nThe code chunk below uses read_sas() of haven to import PISA data into R environment.\n\nstu_qqq &lt;- read_sas(\"data/cy08msp_stu_qqq.sas7bdat\")\n\n\nstu_qqq_SG &lt;- stu_qqq %&gt;%\n  filter(CNT == \"SGP\")\n\n\nwrite_rds(stu_qqq_SG,\n          \"data/stu_qqq_SG.rds\")\n\n\nstu_qqq_SG &lt;- read_rds(\"data/stu_qqq_SG.rds\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html",
    "title": "Hands-on Exercise 4: Fundamentals Statistics of Visual Analytics",
    "section": "",
    "text": "roadmap_4"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#getting-started",
    "title": "Hands-on Exercise 4: Fundamentals Statistics of Visual Analytics",
    "section": "2.1 Getting Started",
    "text": "2.1 Getting Started\n\n2.1.1 Install and Load R packages\n\ntidyverse for data science process\nggridges a ggplot2 extension specially designed for plotting ridgeline plots\nggdist for visualising distribution and uncertainty\n\n\npacman::p_load(ggdist, ggridges, tidyverse,\n               ggthemes,colorspace)\n\n\n\n2.1.2 Import data\nExam_data.csv will be used in this exercise.\n\nexam &lt;- read_csv(\"data/Exam_data.csv\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#visualize-distribution-with-ridgeline-plot",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#visualize-distribution-with-ridgeline-plot",
    "title": "Hands-on Exercise 4: Fundamentals Statistics of Visual Analytics",
    "section": "2.2 Visualize Distribution with Ridgeline Plot",
    "text": "2.2 Visualize Distribution with Ridgeline Plot\nRidgeline plot(also called Joyplot): for revealing the distribution of a numeric value for several groups\n\nis used when the number of group is large\nis used when the number of group to represent is medium to high\nuse space more efficiently\nis used when there is a clear pattern in the result, an obvious ranking in groups\n\n\n2.2.1 ggridges package\n\ngeom_ridgeline(): creates plots that look like a series of mountain ridges. creates a line that represents the density of the distribution of your data.\ngeom_density_ridges(): In addition to what geom_ridgeline() does, geom_density_ridges() adds a shaded area under the lines, which represents the same density information in a more visually filled way\n\nThe ridgeline plot below is plotted by using geom_density_ridges()\n\nggplot(exam,\n       aes(x=ENGLISH,\n           y=CLASS))+\n  geom_density_ridges(\n    scale=3,\n    rel_min_height=0.01,\n    bandwidth=3.4,\n    fill=lighten(\"#7097BB\",.3),\n    color=\"white\"\n  )+\n  scale_x_continuous(\n    name=\"English grades\",\n    expand=c(0,0)\n  )+\n  scale_y_discrete(name=NULL,\n                   expand=expansion(add=c(0.2,2.6)))+\n  theme_ridges()\n\n\n\n\nobservation: each peak point of each group, the spread of each group and the tendency of the line linking all peak points\n\n\n\n\n\n\nCode Notes\n\n\n\n\nscale: controls the vertical scaling of the density curves for each category, a larger value increases the vertical spacing between the curves, making them easier to distinguish from one another\nrel_min_height: sets a minimum relative height for each density curve. It’s a way to ensure that curves representing very small density values don’t become invisible\nbandwith: controls the degree of smoothness in the density estimation. A larger bandwidth results in smoother curves with less detail, while a smaller bandwidth leads to more jagged curves with more detail\nfill: color of shade\ncolor: color of outline\nlighten(): gives a lighter color of the specific color\nscale_x_continuous(): sets the scale for the x-axis as continuous\n7.1 name=\"English grades\": title of x axis. If you want to remove the label of axis, then use name=NULL\n7.2 expand=c(0,0):\n7.2.1 1st 0: the multiplier that determines how much space to add around both of the axis based on the range of the data, e.g.expand=c(0.1,0) will look at the range of data points in terms of x axis and add a space equal to 10% of that range on both sides of the axis. Same as y axis.\n7.2.2 2nd 0: the fixed value adding blank space beyond the min and max of your actual data on y axis, e.g. y-axis goes from 20 to 80, your plot might show just that range, if with expand=c(0,10), y-axis starts from 10 and ends at 90, but the data points themselves don’t change at all.\nscale_y_discrete(): sets the scale for the y-axis as discrete\n\n\n\n\n\n2.2.2 Change fill colors\n\ngeom_ridgeline_gradient(): make the area under a ridgeline filled with colors that vary in some form along the x axis (do not allow for alpha transparency in the fill)\ngeom_density_ridges_gradient(): make the area under a ridgeline filled with colors that vary in some form along the x axis (do not allow for alpha transparency in the fill)\n\n\nggplot(exam,\n       aes(x=ENGLISH,\n           y=CLASS,\n           fill=stat(x)))+\n  geom_density_ridges_gradient(\n    scale=3,\n    rel_min_height=0.01)+\n  scale_fill_viridis_c(name=\"Temp. [F]\",\n                       option=\"C\")+\n  scale_x_continuous(\n    name=\"English grades\",\n    expand=c(0,0))+\n  scale_y_discrete(name=NULL,\n                   expand=expansion(add=c(0.2,2.6)))+\n  theme_ridges()\n\n\n\n\n\n\n\n\n\n\nCode Notes\n\n\n\n\nfill=stat(x): fills the ridges based on the statistical transformation of the x, in the context of geom_density_ridges(), it fills the ridges based on the density of the English scores\nscale_fill_viridis_c(): applies a color scale from the viridis palette to the fill aesthetic 2.1 name=\"Temp. [F]\": sets the name for the color scale in the legend 2.2 option=\"C\": allows for variations in the color mapping. The C option stands for one of the color maps available in the viridis package. Each option (A, B, C, D, E) provides a different color scheme. Note the color used representing quantile across all group so that we can observe the difference of distribution of all groups by color without checking the absolute numbers.\n\n\n\n\n\n2.2.3 Map the probabilities onto color\n\nstat_density_ridges(): provides a stat function\n\n\nggplot(exam,\n       aes(x=ENGLISH,\n           y=CLASS,\n           fill=0.5-abs(0.5-stat(ecdf))))+\n  stat_density_ridges(geom=\"density_ridges_gradient\",\n                      calc_ecdf=TRUE)+\n  scale_fill_viridis_c(name=\"Tail probability\",\n                       direction = -1)+\n  theme_ridges()\n\n\n\n\n\n\n\n\n\n\nCode Notes\n\n\n\n\nstat(ecdf): computes the empirical cumulative distribution function 1.1 0.5-abs(0.5-stat(ecdf)): This is a way to visualize how far each value is from the median. The closer the fill color is to the center of the color scale, the closer the ECDF at that point is to 0.5, indicating values near the median.\nstat_density_ridges(): adds a statistical transformation layer that calculates the density ridgelines and also the empirical cumulative distribution function (when calc_ecdf=TRUE) 2.1 geom=\"density_ridges_gradient\": indicates that a gradient fill will be used, which will reflect the fill aesthetic defined earlier 2.2 calc_ecdf=TRUE: indicates that the function should calculate the empirical cumulative distribution function for each group of data.\ndirection = -1: reverses the color scale, so that high values are colored with what would normally be low-end colors and vice versa. This is commonly done to match intuitive color associations (e.g., red for higher values, blue for lower values)\n\n\n\n\n\n2.2.4 Add quantile lines to ridgeline plots\n\nggplot(exam,\n       aes(x = ENGLISH, \n           y = CLASS, \n           fill = factor(stat(quantile))\n           )) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = 4,\n    quantile_lines = TRUE) +\n  scale_fill_viridis_d(name = \"Quartiles\") +\n  theme_ridges()\n\n\n\n\nInstead of using number to define the quantiles, we can also specify quantiles by cut points such as 2.5% and 97.5% tails to colour the ridgeline plot as shown in the figure below.\n\nggplot(exam,\n       aes(x=ENGLISH,\n           y=CLASS,\n           fill=factor(stat(quantile))))+\n  stat_density_ridges(\n    geom=\"density_ridges_gradient\",\n    calc_ecdf=TRUE,\n    quantiles=c(0.025,0.975))+\n  scale_fill_manual(\n    name=\"Probability\",\n    values=c(\"#FF0000A0\", \"#A0A0A0A0\", \"#0000FFA0\"),\n    labels=c(\"(0, 0.025]\", \"(0.025, 0.975]\", \"(0.975, 1]\"))+\n  theme_ridges()\n\n\n\n\n\n\n\n\n\n\nCode Notes\n\n\n\n\nfactor(stat(quantile)): computes quantiles and be treated as discrete factors, so each quantile will get a different color in the plot.\nquantiles=c(0.025,0.975): specifies which quantiles to calculate for the fill aesthetic."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#visualize-distribution-with-raincloud-plot",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#visualize-distribution-with-raincloud-plot",
    "title": "Hands-on Exercise 4: Fundamentals Statistics of Visual Analytics",
    "section": "2.3 Visualize Distribution with Raincloud Plot",
    "text": "2.3 Visualize Distribution with Raincloud Plot\nRaincloud Plot produces a half-density to a distribution plot.\n\nused to compare the distribution of a continuous variable in terms of different groups\nhighlight multiple modalities (an indicator that groups may exist)\n\n\n2.3.1 Plot a Half Eye graph\n\nstat_halfeye(): produces a Half Eye visualization, which is contains a half-density and a slab-interval.\n\n\nggplot(exam,\n       aes(x=RACE,\n           y=ENGLISH))+\n  stat_halfeye(adjust=0.5,\n               justification=-0.2,\n               .width=0,\n               point_colour=NA)\n\n\n\n\n\n\n\n\n\n\nCode Notes\n\n\n\n\nadjust=0.5: control the bandwidth of the kernel density estimate, which affects the smoothness of the density shape. A smaller value would make the shape more sensitive to small changes.\njustification=-0.2: control the alignment or positioning of the “half-eye” in relation to the categorical axis. A negative value suggests it is shifted to the right of each category\n.width=0: remove the slab interval (data distribution and confidence interval).\npoint_colour=NA: set the color of the individual points within the “half-eye”. Setting it to ‘NA’ means that the points will not be colored.\n\n\n\n\n\n2.3.2 Add the boxplot with geom_boxplot()\n\nggplot(exam,\n       aes(x=RACE,\n           y=ENGLISH))+\n  stat_halfeye(adjust=0.5,\n               justification=-0.2,\n               .width=0,\n               point_colour=NA)+\n  geom_boxplot(width=.20,\n               outlier.shape=NA)\n\n\n\n\n\n\n2.3.3 Add the dot plot with geom_dots()\n\nstat_dots(): produces a half-dotplot, which is similar to a histogram that indicates the number of data points in each bin\n\n\nggplot(exam,\n       aes(x=RACE,\n           y=ENGLISH))+\n  stat_halfeye(adjust=0.5,\n               justification=-0.2,\n               .width=0,\n               point_colour=NA)+\n  geom_boxplot(width=.20,\n               outlier.shape=NA)+\n  stat_dots(side=\"left\",\n            justification=1.2,\n            binwidth=.5,\n            dotsize=2)\n\n\n\n\n\n\n\n\n\n\nNotes\n\n\n\n\nHalfeye alone just shows the distribution, check the amount of dots by using stat_dots() can have a better understanding of each group, it shows the size of exact dataset.\n\n\n\n\n\n2.3.4 Flip horizontally\n\nggplot(exam,\n       aes(x=RACE,\n           y=ENGLISH))+\n  stat_halfeye(adjust=0.5,\n               justification=-0.2,\n               .width=0,\n               point_colour=NA)+\n  geom_boxplot(width=.20,\n               outlier.shape=NA)+\n  stat_dots(side=\"left\",\n            justification=1.2,\n            binwidth=.5,\n            dotsize=2)+\n  coord_flip()+\n  theme_economist()"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#getting-started-1",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#getting-started-1",
    "title": "Hands-on Exercise 4: Fundamentals Statistics of Visual Analytics",
    "section": "3.1 Getting Started",
    "text": "3.1 Getting Started\n\n3.1.1 Install and Launch R package\n\nggstatsplot: create graphics with details from statistical tests included in the information-rich plots themselves\n\nLearn more about ggstatsplot on ggstatsplot\n\npacman::p_load(ggstatsplot, tidyverse)\n\n\n\n3.1.2 Import Data\n\nexam &lt;- read.csv(\"Data/Exam_data.csv\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#one-sample-test-gghistostats-method",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#one-sample-test-gghistostats-method",
    "title": "Hands-on Exercise 4: Fundamentals Statistics of Visual Analytics",
    "section": "3.2 One-sample test: gghistostats() method",
    "text": "3.2 One-sample test: gghistostats() method\n\nset.seed(1234)\n\ngghistostats(\n  data=exam,\n  x=ENGLISH,\n  type=\"bayes\",\n  test.value=60,\n  xlab=\"English scores\"\n)\n\n\n\n\n\n\n\n\n\n\nCode Notes\n\n\n\n\nset.seed(1234): sets a seed for random number generation to ensures that if the code involves any randomness (like random sampling), it will produce the same results every time you run it. The number 1234 is just a starting point for generating random numbers.\ntype=\"bayes: the type of analysis this code wants to perform. Bayesian statistics is a different approach than traditional statistics, allowing you to update your beliefs about something based on new evidence.\ntest.value=60: hypothesis test related to English score: how the data supports or contradicts a value of 60\n\n\n\n\n\n\n\n\n\nAnalysis\n\n\n\n\nlog(BF01) = -31.45: The Bayes Factor comparing the null hypothesis to the alternative is given in log scale. A negative value suggests that the data are more likely under the alternative hypothesis.\nPosterior difference = 7.16: After analyzing the data and taking into account any prior knowledge, the analysis estimates that the difference we are interested in (like the average score) is about 7.16 units. This is not a fixed number but an estimate that can change with more information.\nETI 95% [5.44, 8.75]: ETI stands for “Equal-Tailed Interval,” which is a range of values that the true difference is likely to fall within. The “95%” part means we can be 95% confident that the true value is somewhere between 5.44 and 8.75. It’s like saying, “We’re pretty sure the true value is in this range, but we don’t know exactly where.”\nµMAP = 74.74: This is the maximum a posteriori estimate, which is the most credible value for the mean score given the data.\nConclusion: there is evidence to suggest that the mean English score is not 60."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#bayes-factor",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#bayes-factor",
    "title": "Hands-on Exercise 4: Fundamentals Statistics of Visual Analytics",
    "section": "3.3 Bayes Factor",
    "text": "3.3 Bayes Factor\nA Bayes factor is the ratio (positive number) of the likelihood of one particular hypothesis to the likelihood of another. It can be interpreted as a measure of the strength of evidence in favor of one theory among two competing theories\nExample:\n\nYou have two competing options or hypotheses. Let’s call them Option A and Option B.\nYou collect some data or evidence. The Bayes Factor helps you assess how well the data supports Option A versus Option B.\nThe Bayes Factor gives you a ratio. If the Bayes Factor is greater than 1, it means the data supports Option A more than Option B. If it’s less than 1, it means the data supports Option B more than Option A.\nThe bigger the Bayes Factor, the stronger the evidence in favor of one option. A Bayes Factor of 10 means the data strongly supports one option over the other.\nThe smaller the Bayes Factor, the weaker the evidence. A Bayes Factor of 0.1 means the data weakly supports one option over the other."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#two-sample-mean-test-ggbetweenstats-method",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#two-sample-mean-test-ggbetweenstats-method",
    "title": "Hands-on Exercise 4: Fundamentals Statistics of Visual Analytics",
    "section": "3.4 Two-sample mean test: ggbetweenstats() method",
    "text": "3.4 Two-sample mean test: ggbetweenstats() method\n\nggbetweenstats(\n  data=exam,\n  x=GENDER,\n  y=MATHS,\n  type=\"np\",\n  message=FALSE\n)\n\n\n\n\n\n\n\n\n\n\nCode Notes\n\n\n\n\ntype=\"np\": refer to “non-parametric,” suggesting that you’re doing a type of analysis that doesn’t assume your data follows a specific mathematical distribution.\nmessage=FALSE: is used to control whether or not you want informational messages to be displayed while creating the plot.\n\n\n\n\n\n\n\n\n\nAnalysis\n\n\n\n\nMann-Whitney U test: This is a statistical test comparing the two groups. The value W = 13011.00, p = 0.91 indicates the test statistic and the p-value. A p-value higher than 0.05 (like 0.91 here) typically means that there’s no statistical evidence of a difference in math scores between females and males.\nRank-biserial correlation: r = 7.04e-03 is very close to zero, suggesting a very small effect size, if any, means almost no correlation between gender and score.\nConfidence Interval: CI95% [-0.12, 0.13]: indicates the 95% confidence interval for the rank-biserial correlation coefficient, which includes zero, suggesting no effect.\nn_obs = 322: Number of Observations tells us the total number of observations (students) included in the analysis.\nConclusion: there does not appear to be a significant difference in math scores between female and male students in this particular sample."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#oneway-anova-test-ggbetweenstats-method",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#oneway-anova-test-ggbetweenstats-method",
    "title": "Hands-on Exercise 4: Fundamentals Statistics of Visual Analytics",
    "section": "3.5 Oneway ANOVA test: ggbetweenstats() method",
    "text": "3.5 Oneway ANOVA test: ggbetweenstats() method\n\nggbetweenstats(\n  data=exam,\n  x=RACE,\n  y=ENGLISH,\n  type=\"p\",\n  mean.ci=TRUE,\n  pairwise.comparisons=TRUE,\n  pairwise.display = \"s\",\n  pairwise.method=\"fdr\",\n  messages=FALSE\n)\n\n\n\n\n\n\n\n\n\n\nCode Notes\n\n\n\n\ntype=\"p\": refer to “parametric,” suggesting that you’re doing an analysis that assumes your data follows a specific mathematical distribution.\nmean.ci=TRUE: indicates that you want to include confidence intervals around the mean values in your plot.\npairwise.comparisons=TRUE: means that you want to compare the groups pairwise, which allows you to see if there are statistically significant differences between any two racial groups.\npairwise.display = \"s\": specify how you want the pairwise comparisons to be displayed. “s” might stand for “summary,” indicating that you want a summarized view of the comparisons.\n4.1 “ns” → only non-significant\n4.2 “s” → only significant\n4.3 “all” → everything\npairwise.method=\"fdr\": specifying the method for adjusting for multiple comparisons. “fdr” might refer to the “False Discovery Rate,” which is a way to control for the risk of making false discoveries when you’re comparing multiple groups.\n\n\n\n\n\n\n\n\n\nAnalysis\n\n\n\n\nF(3, 23.8) = 10.15, p = 1.71e-04: indicates that there are statistically significant differences between the groups’ means. The F statistic is 10.15, and the p-value is 0.000171, which is well below the common threshold of 0.05 for significance.\nthe omega squared effect size=0.50: suggests a moderate to large effect size. However, the confidence interval for this effect size includes 0, which can introduce some uncertainty about the effect size estimate.\nP_FDR-adj: The pairwise test results are indicated by lines connecting the groups. The presence of lines (with p-value annotations) suggests that specific group comparisons were made. The pink dots on the lines indicate that the differences were statistically significant after adjusting for multiple comparisons using the False Discovery Rate (FDR) method.\nConclusion: Mean score of “Others” shows significant difference with mean score of “Chinese”,“Indian” and “Malay”.(as denoted by the pink dots and lines)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#significant-test-of-correlation-ggscatterstats",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#significant-test-of-correlation-ggscatterstats",
    "title": "Hands-on Exercise 4: Fundamentals Statistics of Visual Analytics",
    "section": "3.6 Significant test of Correlation: ggscatterstats()",
    "text": "3.6 Significant test of Correlation: ggscatterstats()\n\nggscatterstats(\n  data=exam,\n  x=MATHS,\n  y=ENGLISH,\n  marginal=TRUE\n)\n\n\n\n\n\n\n\n\n\n\nCode Notes\n\n\n\n\nmarginal: extra plots or histograms along the sides of the scatter plot.\n\n\n\n\n\n\n\n\n\nAnalysis\n\n\n\n\nCI 95% [0.79, 0.86]: A coefficient of 0.83 indicates a strong positive relationship between math and English scores.\nt(320) = 26.72, p = 1.70e-83: indicates the results of a t-test that tests the hypothesis of no correlation. The t-statistic is 26.72 and the p-value is extremely small (1.70 x 10^-83), which provides strong evidence against the null hypothesis of no correlation.\nlog(BF01) = -183.55: provides the Bayes factor for testing the null hypothesis of no relationship (which is the alternative hypothesis here). The negative value is very large, suggesting strong evidence against the null hypothesis.\n(eta squared) = 1.41: effect size, the value being greater than 1 is unusual for eta squared, which should be between 0 and 1. This might be a typographical error in the annotation.\nConclusion: the plot suggests a strong positive relationship between math and English scores, meaning students who score high in math also tend to score high in English, and this relationship is statistically significant."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#significant-test-of-association-ggbarstats",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#significant-test-of-association-ggbarstats",
    "title": "Hands-on Exercise 4: Fundamentals Statistics of Visual Analytics",
    "section": "3.7 Significant test of Association: ggbarstats()",
    "text": "3.7 Significant test of Association: ggbarstats()\n\ndata wrangling: the Maths scores is binned into a 4-class variable by using cut()\n\n\nexam1 &lt;- exam %&gt;%\n  mutate(MATHS_bins = \n           cut(MATHS, \n               breaks = c(0,60,75,85,100)))\n\n\nAssociation test\n\n\nggbarstats(exam1,\n           x=MATHS_bins,\n           y=GENDER)\n\n\n\n\n\n\n\n\n\n\nAnalysis\n\n\n\n\nChi-Squared(3),p = 0.79,Cramér’s V = 0.00: suggests a test of independence was performed to see if there’s a relationship between gender and math score bins. The p-value of 0.79 is high, indicating there’s no statistical evidence of association between the two variables, which is further supported by a Cramér’s V value of 0, showing no effect size.\nlog_e(BF01) = 4.73: A positive value suggests evidence for the null hypothesis.\np = 0.62 for Females, p = 0.83 for Males: both for male and female, the distribution across the score bins appears to be random or as expected, not showing any particular trend or bias.\nConclusion: that there is no significant association between gender and math scores."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#getting-started-2",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#getting-started-2",
    "title": "Hands-on Exercise 4: Fundamentals Statistics of Visual Analytics",
    "section": "4.1 Getting Started",
    "text": "4.1 Getting Started\n\n4.1.1 Install and load R packages\n\nperformance: for assessing and checking the quality of statistical models. It contains functions for computing various performance metrics, model diagnostics, and checks for regression models, among others.\nparameters: for processing the parameters of statistical models. It provides functions to summarize, manipulate, and plot model parameters and supports a wide range of models.\nsee: to create plots and visualizations for statistical models and data structures. It works well with the performance and parameters packages, among others, to create attractive and informative plots.\n\n\npacman::p_load(readxl,performance,parameters,see)\n\n\n\n4.1.2 Import Data\n\ncar_resale &lt;- read_xls(\"Data/ToyotaCorolla.xls\",\"data\")\ncar_resale\n\n# A tibble: 1,436 × 38\n      Id Model    Price Age_08_04 Mfg_Month Mfg_Year     KM Quarterly_Tax Weight\n   &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;         &lt;dbl&gt;  &lt;dbl&gt;\n 1    81 TOYOTA … 18950        25         8     2002  20019           100   1180\n 2     1 TOYOTA … 13500        23        10     2002  46986           210   1165\n 3     2 TOYOTA … 13750        23        10     2002  72937           210   1165\n 4     3  TOYOTA… 13950        24         9     2002  41711           210   1165\n 5     4 TOYOTA … 14950        26         7     2002  48000           210   1165\n 6     5 TOYOTA … 13750        30         3     2002  38500           210   1170\n 7     6 TOYOTA … 12950        32         1     2002  61000           210   1170\n 8     7  TOYOTA… 16900        27         6     2002  94612           210   1245\n 9     8 TOYOTA … 18600        30         3     2002  75889           210   1245\n10    44 TOYOTA … 16950        27         6     2002 110404           234   1255\n# ℹ 1,426 more rows\n# ℹ 29 more variables: Guarantee_Period &lt;dbl&gt;, HP_Bin &lt;chr&gt;, CC_bin &lt;chr&gt;,\n#   Doors &lt;dbl&gt;, Gears &lt;dbl&gt;, Cylinders &lt;dbl&gt;, Fuel_Type &lt;chr&gt;, Color &lt;chr&gt;,\n#   Met_Color &lt;dbl&gt;, Automatic &lt;dbl&gt;, Mfr_Guarantee &lt;dbl&gt;,\n#   BOVAG_Guarantee &lt;dbl&gt;, ABS &lt;dbl&gt;, Airbag_1 &lt;dbl&gt;, Airbag_2 &lt;dbl&gt;,\n#   Airco &lt;dbl&gt;, Automatic_airco &lt;dbl&gt;, Boardcomputer &lt;dbl&gt;, CD_Player &lt;dbl&gt;,\n#   Central_Lock &lt;dbl&gt;, Powered_Windows &lt;dbl&gt;, Power_Steering &lt;dbl&gt;, …"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#build-multiple-regression-model-using-lm",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#build-multiple-regression-model-using-lm",
    "title": "Hands-on Exercise 4: Fundamentals Statistics of Visual Analytics",
    "section": "4.2 Build Multiple Regression Model using lm()",
    "text": "4.2 Build Multiple Regression Model using lm()\n\nmodel &lt;- lm(Price ~ Age_08_04 + Mfg_Year + KM + Weight + Guarantee_Period, data = car_resale)\nmodel\n\n\nCall:\nlm(formula = Price ~ Age_08_04 + Mfg_Year + KM + Weight + Guarantee_Period, \n    data = car_resale)\n\nCoefficients:\n     (Intercept)         Age_08_04          Mfg_Year                KM  \n      -2.637e+06        -1.409e+01         1.315e+03        -2.323e-02  \n          Weight  Guarantee_Period  \n       1.903e+01         2.770e+01  \n\n\nThe output equation is: Price = -14.09Age_08_04+1315Mfg_Year-232.3KM+19.03Weight+27.7Guarantee_Period\n\n\n\n\n\n\nCode Notes\n\n\n\n\nlm(): to create linear regression model, lm(y~x1+x2+x3+…, data=…)."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#model-diagnostic-check-multicolinearity",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#model-diagnostic-check-multicolinearity",
    "title": "Hands-on Exercise 4: Fundamentals Statistics of Visual Analytics",
    "section": "4.3 Model Diagnostic: check multicolinearity",
    "text": "4.3 Model Diagnostic: check multicolinearity\nMulticolinearity: several independent varaiables in a model are correlated\n\ncheck_collinearity(model)\n\n# Check for Multicollinearity\n\nLow Correlation\n\n             Term  VIF     VIF 95% CI Increased SE Tolerance Tolerance 95% CI\n               KM 1.46 [ 1.37,  1.57]         1.21      0.68     [0.64, 0.73]\n           Weight 1.41 [ 1.32,  1.51]         1.19      0.71     [0.66, 0.76]\n Guarantee_Period 1.04 [ 1.01,  1.17]         1.02      0.97     [0.86, 0.99]\n\nHigh Correlation\n\n      Term   VIF     VIF 95% CI Increased SE Tolerance Tolerance 95% CI\n Age_08_04 31.07 [28.08, 34.38]         5.57      0.03     [0.03, 0.04]\n  Mfg_Year 31.16 [28.16, 34.48]         5.58      0.03     [0.03, 0.04]\n\n\n\n\n\n\n\n\nAnalysis\n\n\n\n\nTerm: predictor variables.\nVIF: Variance Inflation Factor. This measures how much the variance of the estimated regression coefficients is increased due to multicollinearity. A VIF value of 1 means there is no correlation among the kth predictor and the remaining predictor variables, and hence, no multicollinearity. As a rule of thumb, a VIF greater than 5 or 10 indicates a problematic amount of multicollinearity.\nTolerance: the inverse of VIF (1/VIF) and indicates the proportion of variance of the predictor that is not explained by the other predictors. A small tolerance (close to 0) indicates that the variable is highly correlated with other variables.\nConclusion: Age_08_04 and Mfg_Year are highly correlated, can consider removing one of the highly correlated variables or combining them into a single variable.\n\n\n\nPlot the collinearity in terms of VIF\n\ncheck_c &lt;- check_collinearity(model)\nplot(check_c)\n\n\n\n\n\n\n\n\n\n\nCode Notes\n\n\n\n\nwhy the function can be plotted directly?: some functions from statistics model can return an object of a specific type. check_collinearity() here returns a list of multiple components. One of them is dataframe, others might be metadata used for plotting or model summary."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#model-diagnostic-check-normality-assumption",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#model-diagnostic-check-normality-assumption",
    "title": "Hands-on Exercise 4: Fundamentals Statistics of Visual Analytics",
    "section": "4.4 Model Diagnostic: Check normality assumption",
    "text": "4.4 Model Diagnostic: Check normality assumption\nNormality assumption: when performing statistical tests that rely on the assumption of normally distributed residuals (like t-tests for coefficients), it is required to check if the data points follow a standard normal distribution in advance.\n\nmodel1 &lt;- lm(Price ~ Age_08_04+KM+Weight+Guarantee_Period,\n             data=car_resale)\ncheck_n &lt;- check_normality(model1)\nplot(check_n)\n\n\n\n\n\n\n\n\n\n\nAnalysis\n\n\n\n\nQ-Q plot: quantile-quantile plot, to assess if a set of data plausibly came from some theoretical distribution such as a Normal distribution.\nx-axis: represents the quantiles from a standard normal distribution, which is the theoretical distribution we are comparing the data against.\ny-axis: shows the quantiles from the data being analyzed—in this case, the residuals from the linear regression model.\nresiduals: the differences between the observed values and the values predicted by the model. Each data point has one residual. On the Q-Q plot, each dot represents one of these residuals, not the actual data points. The purpose of this plot is to see how the residuals compare to a normal distribution.\nreference line: x-axis is theoretical distribution quantiles (standard normal distribution quantiles), y-axis is sample quantile deviations. If sample data follows standard normal distribution, it should be a horizontal line at y=0, which is the reference line in this case.\nConclusion: the normality assumption of the linear regression model is not fully met.\n\n\nMiddle Range: In the middle range of the data (between about -2 and 2 on the x-axis), the residuals mostly follow the reference line. This suggests that in this range, the residuals are behaving as we would expect if they were normally distributed.\nTails: at both ends of the plot (the tails), the residuals deviate from the reference line. This is indicated by the points curving away from the line. In the left tail, the residuals are lower than expected for a normal distribution, and in the right tail, they are higher than expected. The residuals have heavier tails than a normal distribution, which means there are more extreme values (outliers) than we would expect if the residuals were truly normal.\nIt might still be okay to use the model for predictions or to understand relationships between variables, especially if it’s for exploratory purposes or the sample size is large. However, if you are performing statistical tests that rely on the assumption of normally distributed residuals (like t-tests for coefficients), the results of those tests might not be entirely reliable. You might need to consider transformations of the data, robust statistical methods, or other models that do not assume normality of residuals."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#model-diagnostic-check-homogeneity-of-variances",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#model-diagnostic-check-homogeneity-of-variances",
    "title": "Hands-on Exercise 4: Fundamentals Statistics of Visual Analytics",
    "section": "4.5 Model Diagnostic: Check homogeneity of variances",
    "text": "4.5 Model Diagnostic: Check homogeneity of variances\nhomogeneity of variances means “equal spread” or “equal variability” of different groups. It refers to the consistency of the spread or variance of the residuals across all levels of the independent variables.\nwhen you are doing an experiment or a study, you often want to make sure that the groups you are comparing are similar in terms of how spread out their data is. If one group has data that is all over the place and another group’s data is very tightly packed, it could be a problem for certain types of statistical tests which assume that the variances are equal across all groups being compared.\nExample:\nImagine you’re trying to predict the scores of students on a math test using the number of hours they studied. After everyone takes the test, you compare your predictions to the actual scores—the differences are called “residuals.”\nIf your predictions are equally reliable for students who studied a little and students who studied a lot, then the spread of those residuals will be pretty consistent. This is like saying whether a student studied for 2 hours or 10 hours, your prediction might be off by about 3 points on average either way. This consistency in prediction error is what we mean by “homogeneity of variance.”\nIf you notice that for students who studied a little, your predictions are really close, but for students who studied a lot, your predictions are way off—maybe you’re underestimating their scores. This means the spread of the residuals isn’t consistent. For those who studied a little, the residuals are small and clustered close together. For those who studied a lot, the residuals are large and spread out. This inconsistency is what we’d call “heteroscedasticity.”\n“homogeneity of variance” means your prediction mistakes are about the same size no matter how much someone studied. If that’s not the case, and your mistakes vary a lot depending on how much they studied, then there’s a problem with “heteroscedasticity”.\nReasons for heteroscedasticity:\n\nNon-Linear Relationship: The relationship between study time and scores isn’t straight-line (linear).\nMissing Factors: There could be other factors that affect scores in addition to study time.\nStudy Time Measurement: Perhaps “study time” isn’t measured in the best way. For example, just counting hours might not take into account the quality of the study.\nExtreme Values: There might be students with very high or very low study times that are affecting the prediction errors. These could be outliers, and they might need special attention in your analysis.\nIncorrect Model: You might be using the wrong type of model to make your predictions. Maybe the relationship between study time and scores is more complex than the model you’re using can handle.\n\n\ncheck_h &lt;- check_heteroscedasticity(model1)\nplot(check_h)\n\n\n\n\n\n\n\n\n\n\nAnalysis\n\n\n\n\nx-axis (Fitted Values): the predicted values by your regression model.\ny-axis (Standardized residuals): the residuals from your model that have been standardized. Standardization typically means subtracting the mean and dividing by the standard deviation, so these values are expressed in terms of how many standard deviations they are from the mean residual (which is typically 0).\nConclusion: the plot shows a pattern where the spread of residuals seems to increase with the fitted values — the residuals are quite close together on the left (for smaller fitted values) and spread out more on the right (for larger fitted values). This pattern is called heteroscedasticity, which means the variability of the errors is not consistent across the range of data. This can be problematic for regression models because they assume that the residuals are equally spread across all levels of the independent variables (homoscedasticity)."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#model-diagnostic-complete-check",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#model-diagnostic-complete-check",
    "title": "Hands-on Exercise 4: Fundamentals Statistics of Visual Analytics",
    "section": "4.6 Model Diagnostic: Complete check",
    "text": "4.6 Model Diagnostic: Complete check\nTo create a composite checks on a statistical model.\n\ncheck_model(model1)\n\n\n\n\n\n\n\n\n\n\nAnalysis\n\n\n\n\nLinearity: The reference line should be flat and horizontal if the relationship between the independent variables and the dependent variable is linear. If the residuals are randomly dispersed around the horizontal line, it suggests linearity. A pattern or systematic structure in the residuals would suggest non-linearity.\nPosterior Predictive Check: This plot compares the distribution of observed data against the distribution of data predicted by the model. Ideally, the model-predicted data should closely match the observed data, which would mean the model is accurately capturing the data’s distribution. In the plot, if the lines for observed and predicted data are close together, it indicates a good fit.\nInfluential Observations: This plot, often called a Cook’s distance plot, helps identify influential observations. Points that fall outside the dashed lines (contour lines of Cook’s distance) can have a disproportionate influence on the model’s parameters."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#visualize-regression-parameters-see-methods",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#visualize-regression-parameters-see-methods",
    "title": "Hands-on Exercise 4: Fundamentals Statistics of Visual Analytics",
    "section": "4.7 Visualize Regression Parameters: see methods",
    "text": "4.7 Visualize Regression Parameters: see methods\nTo display the point estimates and confidence intervals of the parameters (coefficients) from a statistical model.\n\nplot(parameters(model1))\n\n\n\n\n\n\n\n\n\n\nAnalysis\n\n\n\n\ndot-and-whisker plot: to display the point estimates and confidence intervals of the parameters (coefficients) from a statistical model.\nx-axis: represents the value of the estimated coefficients for each variable. These coefficients indicate the expected change in the dependent variable for a one-unit change in the predictor, assuming all other variables are held constant.\nDots: Each dot represents the estimated coefficient for the corresponding variable. The position on the horizontal axis shows whether the effect is positive or negative and the size of the effect.(relationship between this independent variable and the dependent variable)\nWhiskers (Horizontal Lines): lines extending from the dots represent the confidence intervals for these estimates, often at a 95% confidence level. If a line crosses the vertical zero line (the dashed line), it suggests that we cannot be confident that the variable has a statistically significant effect on the dependent variable at the chosen confidence level.(either negative or positive is ok, only when crossing the zero line means insignificant effect)\nConclusion:\n\nAge_08_04: negative and does not cross the zero line, meaning negative relationship (the dependent variable decreases as “Age_08_04” increases)\nWeight: positive relationship\nGuarantee Period: the coefficient is positive, but its confidence interval does cross the zero line, suggesting that the relationship between ‘Guarantee Period’ and the dependent variable is not statistically significant at the given level of confidence.\nKM: dot is at zero line, estimated effect is very small or negligible."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#visualize-regression-parameters-ggcoefstats-methods",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#visualize-regression-parameters-ggcoefstats-methods",
    "title": "Hands-on Exercise 4: Fundamentals Statistics of Visual Analytics",
    "section": "4.8 Visualize Regression Parameters: ggcoefstats() methods",
    "text": "4.8 Visualize Regression Parameters: ggcoefstats() methods\n\npacman::p_load(ggstatsplot)\nggcoefstats(model1, output=\"plot\")\n\n\n\n\n\n\n\n\n\n\nAnalysis\n\n\n\n\nvalues next to the dots: 1.1 estimated β: estimated coefficient. These values are the “effect sizes,” showing how much the dependent variable is expected to change with a one-unit change in the predictor, holding other variables constant. 1.2 t-value: the coefficient divided by its standard error 1.3 degrees of freedom: suggests the sample size 1.4 p-value: indicates the probability of observing the data if the null hypothesis were true. A p-value less than 0.05 typically indicates statistical significance.\nconclusion: 2.1 intercept: The intercept is significant with a large negative coefficient 2.2 Age_08_04: has the most substantial negative effect on the dependent variable, with a highly significant p-value (almost 0). 2.3 Weight: has a significant positive effect on the dependent variable, with a very low p-value, indicating strong evidence against the null hypothesis. 2.4 KM: has a small, but significant, negative effect. 2.5 Guarantee_Period: has a negative effect, though smaller in magnitude than “KM”. 2.6 AIC/BIC: measures of the model’s quality, with lower values generally indicating a better fit."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#concept",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#concept",
    "title": "Hands-on Exercise 4: Fundamentals Statistics of Visual Analytics",
    "section": "5.1 Concept",
    "text": "5.1 Concept\nEstimate: a best guess. In order to learn something about the whole population, we take a smaller group (sample), collect data, look at that, and then make an educated guess or estimate about the larger population based on what we see in the sample.\n\nPoint Estimate: the single best guess.\nInterval Estimate: a range of numbers where the true answer might fall.\n\nUncertainty: about how confident we are in our estimates after multiple sampling tests.\n\nConfidence Interval: indicate the uncertainty of estimated value, meaning whether the results will point to a certain range after many times of sampling test.\n\nLarge confidence interval: the result of multiple tests is more discrete which is pointing to a large range, so the uncertainty of that estimate is large\nSmall confidence interval: the result of multiple tests is more concentrated which is pointing to a small range, so the uncertainty of that estimate is small\nNote: uncertainty of estimated value is not equal to accuracy of estimated value to the actual value.\n\n\nAccuracy: how close is the distance between estimated values or ranges and the actual values.\n\nConfidence Level: indicate the accuracy of estimated value to the actual value, meaning how many times the estimated results of sampling tests include the actual values\n\nHigh confidence level: the actual values fall in almost every estimated ranges of each test\nLow confidence level: the actual values fall in few estimated ranges of tests\n\n\nConfidence Level vs Confidence Interval:\n\nThe higher the confidence level, the wider the confidence interval\nConfidence level of 99% doesn’t mean to be better than that of 95%"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#getting-started-3",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#getting-started-3",
    "title": "Hands-on Exercise 4: Fundamentals Statistics of Visual Analytics",
    "section": "5.2 Getting Started",
    "text": "5.2 Getting Started\n\n5.2.1 Install and load packages\nThe packages used for this exercise include:\n\ntidyverse, a family of R packages for data science process\nplotly for creating interactive plot\ngganimate for creating animation plot\nDT for displaying interactive html table\ncrosstalk for for implementing cross-widget interactions (currently, linked brushing and filtering)\nggdist for visualising distribution and uncertainty\n\n\ndevtools::install_github(\"wilkelab/ungeviz\")\n\n\n\n\n\n\n\nCode Notes\n\n\n\n\ndevtools: package to install another package from other sources, meaning the package is not available on the Comprehensive R Archive Network (CRAN).\ninstall_github(): to install R packages that are hosted on GitHub\n\"wilkelab/ungeviz\": specifies the GitHub repository where the package is located. The format is “username/repository”, so in this case, “wilkelab” is the username or account name on GitHub, and “ungeviz” is the name of the repository where the package code resides.\n\n\n\n\npacman::p_load(ungeviz,plotly,crosstalk,DT,\n               ggdist,ggridges,colorspace,\n               gganimate,tidyverse)\n\n\n\n5.2.2 Import data\n\nexam &lt;- read_csv(\"data/Exam_data.csv\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#visualize-uncertainty-ggplot2-methods",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#visualize-uncertainty-ggplot2-methods",
    "title": "Hands-on Exercise 4: Fundamentals Statistics of Visual Analytics",
    "section": "5.3 Visualize uncertainty: ggplot2 methods",
    "text": "5.3 Visualize uncertainty: ggplot2 methods\nUncertainty of point estimate is not equal to confidence interval. Uncertainty of point estimate is not equal to Variation in the sample.\n\nUncertainty of point estimate: quantifies how much the estimate might vary if different samples were taken from the same population.\nConfidence interval: quantifies the uncertainty about the true parameter value by providing a range of plausible values.\nVariation in the sample: is commonly measured by standard deviation.\n\n\n5.3.1 Derive necessary summary statistics from data\nCalculate mean, standard deviation and standard error of the mean from data.\n\nmy_sum &lt;- exam %&gt;%\n  group_by(RACE) %&gt;%\n  summarise(\n    n=n(),\n    mean=mean(MATHS),\n    sd=sd(MATHS)\n  ) %&gt;%\n  mutate(se=sd/sqrt(n-1))\n\n\n\n\n\n\n\nCode Notes\n\n\n\n\nsummarise(): is used to compute the count of observations, mean, standard deviation. It can be used to created new columns of specified summary statistics: n, mean, sd\nn=n(): the number of observations (or rows) in each group.\n\n\n\nDisplay tibble data frame in an html table format.\nAn HTML table format refers to the way tables are structured and displayed in HTML (Hypertext Markup Language), which is the standard markup language for creating web pages and web applications. Different from tables created in non-web-based environments, HTML tables can be styled extensively with CSS and manipulated with JavaScript, serves as interactive components in forms, data visualizations, and more.\n\nknitr::kable(head(my_sum),format = 'html')\n\n\n\n\nRACE\nn\nmean\nsd\nse\n\n\n\n\nChinese\n193\n76.50777\n15.69040\n1.132357\n\n\nIndian\n12\n60.66667\n23.35237\n7.041005\n\n\nMalay\n108\n57.44444\n21.13478\n2.043177\n\n\nOthers\n9\n69.66667\n10.72381\n3.791438\n\n\n\n\n\n\n\n\n\n5.3.2 Plot standard error bars of point estimates\n\nggplot(my_sum)+\n  geom_errorbar(\n    aes(x=RACE,\n        ymin=mean-se,\n        ymax=mean+se),\n    width=0.2,\n    colour=\"black\",\n    alpha=0.9,\n    size=0.5)+\n  geom_point(aes(x=RACE,y=mean),\n             stat=\"identity\",\n             color=\"red\",\n             size=1.5,\n             alpha=1)+\n  ggtitle(\"Standard error of mean maths score by race\")\n\n\n\n\n\n\n\n\n\n\nCode Notes\n\n\n\n\nGraphs: standard error of mean can be considered as two graphs combined in one plot: graph of standard error and graph of mean. Defination of Y axis value of these two graphs is different, so we put aes() inside each geom_() instead of putting it in ggplot()\nstat=\"identity\": This argument within geom_point specifies that the values for x and y should be taken directly from the data without any additional statistical transformation. It is the default statistical transformation for geom_point(). If you want to change the default behavior with a different statistical transformation (e.g., counting occurrences of discrete values), you need to define it within geom_point() by using stat=\"count\"\n\n\n\n\n\n5.3.3 Plot confidence interval\n\nggplot(my_sum)+\n  geom_errorbar(\n    aes(x=reorder(RACE,-mean),\n        ymin=mean-1.96*se,\n        ymax=mean+1.96*se),\n    width=0.2,\n    colour=\"black\",\n    alpha=0.9,\n    size=0.5)+\n  geom_point(aes(x=RACE,y=mean),\n             stat=\"identity\",\n             color=\"red\",\n             size=1.5,\n             alpha=1)+\n  labs(x=\"Math score\",title=\"95% confidence interval of mean maths score by race\")\n\n\n\n\n\n\n\n\n\n\nCode Notes\n\n\n\n\nreorder(RACE,-mean): is used to change the order of levels of a factor based on the values of another variable. In this case, it reorders the levels of RACE based on the values of mean in descending order.\n\n\n\n\n\n5.3.4 Plot confidence interval interactive with table\n\nshared_df = SharedData$new(my_sum)\n\nbscols(widths = c(4,8),\n       ggplotly((ggplot(shared_df) +\n                   geom_errorbar(aes(\n                     x=reorder(RACE, -mean),\n                     ymin=mean-2.58*se, \n                     ymax=mean+2.58*se), \n                     width=0.2, \n                     colour=\"black\", \n                     alpha=0.9, \n                     size=0.5) +\n                   geom_point(aes(\n                     x=RACE, \n                     y=mean, \n                     text = paste(\"Race:\", `RACE`, \n                                  \"&lt;br&gt;N:\", `n`,\n                                  \"&lt;br&gt;Avg. Scores:\", round(mean, digits = 2),\n                                  \"&lt;br&gt;95% CI:[\", \n                                  round((mean-2.58*se), digits = 2), \",\",\n                                  round((mean+2.58*se), digits = 2),\"]\")),\n                     stat=\"identity\", \n                     color=\"red\", \n                     size = 1.5, \n                     alpha=1) + \n                   xlab(\"Race\") + \n                   ylab(\"Average Scores\") + \n                   theme_minimal() + \n                   theme(axis.text.x = element_text(\n                     angle = 45, vjust = 0.5, hjust=1)) +\n                   ggtitle(\"99% Confidence interval of average /&lt;br&gt;maths scores by race\")), \n                tooltip = \"text\"), \n       DT::datatable(shared_df, \n                     rownames = FALSE, \n                     class=\"compact\", \n                     width=\"100%\", \n                     options = list(pageLength = 10,\n                                    scrollX=T), \n                     colnames = c(\"No. of pupils\", \n                                  \"Avg Scores\",\n                                  \"Std Dev\",\n                                  \"Std Error\")) %&gt;%\n         formatRound(columns=c('mean', 'sd', 'se'),\n                     digits=2))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode Notes\n\n\n\n\nshared_df = SharedData$new(my_sum): to wrap the ‘my_sum’ data frame into an object that can be shared between different interactive visualizations or data presentations, maintain synchronization between them or to manage data more efficiently.\nbscols(): is used to create a column layout within a Shiny application. It allows you to specify the width proportions of the columns in a Bootstrap grid layout.\nwidths=c(4,8): the first column will occupy 4 units of width which is used for ggplotly, while the second column will occupy 8 units of width used for datatable in the Bootstrap grid system.\nformatRound(columns=c('mean', 'sd', 'se'), digits=2): used within the DT package to format numerical columns in the DataTable with values rounded to 2 decimal places."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#visualize-uncertainty-ggdist-methods",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#visualize-uncertainty-ggdist-methods",
    "title": "Hands-on Exercise 4: Fundamentals Statistics of Visual Analytics",
    "section": "5.4 Visualize uncertainty: ggdist methods",
    "text": "5.4 Visualize uncertainty: ggdist methods\nggdist is designed for both frequentist and Bayesian uncertainty visualization, taking the view that uncertainty visualization can be unified through the perspective of distribution visualization.\n\nFrequentist Model: probabilities are based on the frequency of events. It relies on the idea that if you repeat an experiment infinitely many times, the relative frequency of an event will converge to its true probability. It relies solely on observed data for inference.\nBayesian Model: probabilities represent a degree of belief or uncertainty. It incorporates prior beliefs about the parameters being estimated along with the observed data to update and refine those beliefs. It incorporates both prior beliefs and observed data, updating beliefs as new data becomes available.\n\n\n5.4.1 Visualize confidence interval of mean: ggdist methods (stat_pointinterval)\n\nexam %&gt;%\n  ggplot(aes(x=RACE,\n             y=MATHS))+\n  stat_pointinterval()+\n  labs(\n    title=\"Visualizing confidence intervals of mean maths score\",\n    subtitle=\"Mean Point + Multiple-interval plot\")\n\n\n\n\n\n\n\n\n\n\nCode Notes\n\n\n\n\nstat_pointinterval(): is used to build a visual for displaying distribution of mean with confidence intervals\n\n\n\nAdd arguments within stat_pointinterval()\n\nexam %&gt;%\n  ggplot(aes(x=RACE,\n             y=MATHS))+\n  stat_pointinterval(.width=0.95,\n                     .point=median,\n                     .interval=qi)+\n  labs(\n    title=\"Visualizing confidence intervals of median maths score\",\n    subtitle=\"Median Point + Multiple-interval plot\")\n\n\n\n\n\n\n\n\n\n\nCode Notes\n\n\n\n\n.interval=qi: specifies the method used to calculate the confidence intervals for the median math score. qi is a method for calculating quantile intervals, computing the confidence intervals by taking the 2.5th and 97.5th percentiles of the sample distribution. This corresponds to a 95% confidence interval, which is the default when no specific confidence level is specified.\n\n\n\n\n\n5.4.2 Visualize confidence interval of mean in different confidence level: ggdist methods (stat_pointinterval)\n\nexam %&gt;%\n  ggplot(aes(x = RACE, \n             y = MATHS)) +\n  stat_pointinterval(\n    conf.level = 0.90,\n    show.legend = FALSE) +   \n  labs(\n    title = \"Visualising confidence intervals of mean math score\",\n    subtitle = \"Mean Point + Multiple-interval plot\")\n\n\n\n\n\n\n5.4.3 Visualize confidence interval of mean: ggdist methods (stat_gradientinterval)\n\nexam %&gt;%\n  ggplot(aes(x = RACE, \n             y = MATHS)) +\n  stat_gradientinterval(   \n    fill = \"skyblue\",      \n    show.legend = TRUE     \n  ) +                        \n  labs(\n    title = \"Visualising confidence intervals of mean math score\",\n    subtitle = \"Gradient + interval plot\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#visualize-uncertainty-hypothetical-outcome-plots-hops",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#visualize-uncertainty-hypothetical-outcome-plots-hops",
    "title": "Hands-on Exercise 4: Fundamentals Statistics of Visual Analytics",
    "section": "5.5 Visualize uncertainty: Hypothetical Outcome Plots (HOPs)",
    "text": "5.5 Visualize uncertainty: Hypothetical Outcome Plots (HOPs)\nHypothetical Outcome Plots (HOPs) are a way of showing how different decisions or actions might lead to different outcomes. They’re often used in decision-making or scenario analysis. They’re useful for exploring scenarios, making decisions, and understanding the potential consequences of our choices.\n\nlibrary(ungeviz)\n\nggplot(data = exam, \n       (aes(x = factor(RACE), y = MATHS))) +\n  geom_point(position = position_jitter(\n    height = 0.3, width = 0.05), \n    size = 0.4, color = \"#0072B2\", alpha = 1/2) +\n  geom_hpline(data = sampler(25, group = RACE), height = 0.6, color = \"#D55E00\") +\n  theme_bw() + \n  # `.draw` is a generated column indicating the sample draw\n  transition_states(.draw, 1, 3)\n\n\n\n\n\n\n\n\n\n\nCode Notes\n\n\n\n\nposition_jitter(): is used to jitter the points slightly along the x-axis and y-axis to prevent overplotting, making it easier to see the density of points. Note that position_jitter() is different from normal point plot. Normal point plot adds points to the plot representing actual individual data points, while position_jitter() add points with randomly moving each point a small distance along the x-axis and/or y-axis. In other words, position_jitter() doesn’t represent the actual data points, instead, it represents the data points with slight “error”.\ngeom_hpline(): adds horizontal lines to the plot. These lines are generated from the sampler() function, which creates samples of the dataset exam. sampler(25, group = RACE) means that it will create a subset of the original dataset ‘exam’ with 25 observations from each group defined by the ’RACE’variable.\nheight = 0.6: sets the height of horizontal lines\ntransition_states(.draw, 1, 3): This sets up animation using gganimate. It specifies that the animation should transition between different states based on the column .draw in the dataset, going from draw 1 to draw 3. This means the animation will cycle through different subsets of the data, creating the effect of movement or change over time. In this case, it will go like 1,2,3,1,2,3,1,2,3…\ndraw: In statistical or simulation contexts, a “draw” or “iteration” usually refers to a single execution or realization of a process or simulation. Each draw represents one instance of the simulation or resampling procedure."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#getting-started-4",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#getting-started-4",
    "title": "Hands-on Exercise 4: Fundamentals Statistics of Visual Analytics",
    "section": "6.1 Getting Started",
    "text": "6.1 Getting Started\n\n6.1.1 Install and load packages\nThe packages used in this exercise include:\n\nreadr for importing csv into R.\nFunnelPlotR for creating funnel plot.\nggplot2 for creating funnel plot manually.\nknitr for building static html table.\nplotly for creating interactive funnel plot.\n\n\npacman::p_load(tidyverse,FunnelPlotR,plotly,knitr)\n\n\n\n6.1.2 Import data\n\ncovid19 &lt;- read_csv(\"data/COVID-19_DKI_Jakarta.csv\") %&gt;%\n  mutate_if(is.character, as.factor)\n\nFor this hands-on exercise, we are going to compare the cumulative COVID-19 cases and death by sub-district (i.e. kelurahan) as at 31st July 2021, DKI Jakarta."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#funnelplotr-methods",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#funnelplotr-methods",
    "title": "Hands-on Exercise 4: Fundamentals Statistics of Visual Analytics",
    "section": "6.2 FunnelPlotR methods",
    "text": "6.2 FunnelPlotR methods\nFunnelPlotR package uses ggplot to generate funnel plots. It requires a numerator (events of interest), denominator (population to be considered) and group. The key arguments selected for customisation are:\n\nlimit: specify the plot limits, either at the 95% confidence level or the 99% confidence level. This determines the width of the funnel plot and the corresponding confidence intervals around the summary measure.\nlabel_outliers: When set to true, this argument will label outliers on the funnel plot.\nPoisson_limits: Setting this argument to true adds Poisson limits to the plot. Poisson limits are calculated based on the assumption that the events of interest follow a Poisson distribution. These limits help assess whether observed variation in event rates is within the range of random chance.\nPoisson Distribution: describe the number of events that occur within a fixed interval of time or space, given a known average rate of occurrence and the assumption that the events happen independently of each other. When dealing with funnel plots, we often want to assess whether observed variations in event rates (like mortality rates, complication rates, etc.) are within the range of random chance or if they might indicate something significant. Poisson limits in the context of funnel plots help us with this assessment by setting boundaries to check whether the observed data points are within what we’d expect from random variation alone. If data points fall outside these Poisson limits, it suggests that there might be factors other than chance affecting the event rates and warrants further investigation.\nOD_adjust: This argument adds overdispersed limits to the plot when set to true. Overdispersion occurs when there is more variability in the data than expected based on a Poisson distribution.\nxrange and yrange: specify the range to display for the x-axis and y-axis, respectively. They act as a zoom function, allowing you to focus on a specific region of interest within the funnel plot.\n\n\n6.2.1 FunnelPlotR methods: The basic plot\n\nfunnel_plot(\n  numerator = covid19$Positive,\n  denominator = covid19$Death,\n  group = covid19$`Sub-district`\n)\n\n\n\n\nA funnel plot object with 267 points of which 0 are outliers. \nPlot is adjusted for overdispersion. \n\n\n\n\n6.2.2 FunnelPlotR methods: Makeover 1\n\nfunnel_plot(\n  numerator = covid19$Death,\n  denominator = covid19$Positive,\n  group = covid19$`Sub-district`,\n  data_type = \"PR\",     #&lt;&lt;\n  xrange = c(0, 6500),  #&lt;&lt;\n  yrange = c(0, 0.05)   #&lt;&lt;\n)\n\n\n\n\nA funnel plot object with 267 points of which 7 are outliers. \nPlot is adjusted for overdispersion. \n\n\n\n\n\n\n\n\nCode Notes\n\n\n\n\ndata_type = \"PR\": determines the type of data being plotted.\nPR stands for “Proportion” or “Prevalence Rate”, while SR(by default) stands for “Standardized Rate” or “Standardized Ratio”.\n\nExample:\nCity A:\nTotal population: 100,000; Number of people with diabetes: 10,000.\nCity B:\nTotal population: 200,000; Number of people with diabetes: 20,000.\nPrevalence Rate (PR):\n\nCity A: (Number of people with diabetes in City A) / (Total population of City A) = 10,000 / 100,000 = 0.1 or 10%\nCity B: (Number of people with diabetes in City A) / (Total population of City A) = 10,000 / 100,000 = 0.1 or 10%\n\nStandardized Rate (SR):\n\nStandard Population: Total population of City A + Total population of City B = 100,000 + 200,000 = 300,000\nCity A: (Number of people with diabetes in City A) / (Total standard population) * 100,000 = (10,000 / 300,000) * 100,000 = 3,333.33 cases per 100,000 population\nCity B: (Number of people with diabetes in City B) / (Total standard population) * 100,000 = (20,000 / 300,000) * 100,000 = 6,666.67 cases per 100,000 population\n\n\n\n\n\n6.2.3 FunnelPlotR methods: Makeover 2\n\nfunnel_plot(\n  numerator = covid19$Death,\n  denominator = covid19$Positive,\n  group = covid19$`Sub-district`,\n  data_type = \"PR\",   \n  xrange = c(0, 6500),  \n  yrange = c(0, 0.05),\n  label = NA,\n  title = \"Cumulative COVID-19 Fatality Rate by Cumulative Total Number of COVID-19 Positive Cases\", #&lt;&lt;           \n  x_label = \"Cumulative COVID-19 Positive Cases\", #&lt;&lt;\n  y_label = \"Cumulative Fatality Rate\"  #&lt;&lt;\n)\n\n\n\n\nA funnel plot object with 267 points of which 7 are outliers. \nPlot is adjusted for overdispersion."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#funnel-plot-for-fair-visual-comparison-ggplot2-methods",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#funnel-plot-for-fair-visual-comparison-ggplot2-methods",
    "title": "Hands-on Exercise 4: Fundamentals Statistics of Visual Analytics",
    "section": "6.3 Funnel Plot for Fair Visual Comparison: ggplot2 methods",
    "text": "6.3 Funnel Plot for Fair Visual Comparison: ggplot2 methods\n\n6.3.1 Compute the basic derived fields\nDerive cumulative death rate and standard error of cumulative death rate.\n\ndf &lt;- covid19 %&gt;%\n  mutate(rate = Death / Positive) %&gt;%\n  mutate(rate.se = sqrt((rate*(1-rate)) / (Positive))) %&gt;%\n  filter(rate &gt; 0)\n\nDerive weighted mean of death rate.\n\nfit.mean &lt;- weighted.mean(df$rate, 1/df$rate.se^2)\n\n\n\n\n\n\n\nCode Notes\n\n\n\n\nweighted.mean(df$rate, 1/df$rate.se^2): computes the weighted mean of the rates in the “rate” column, where the weights are given by the inverses of the squared standard errors. This means that rates with smaller standard errors contribute more to the weighted mean, reflecting higher confidence in those estimates, while rates with larger standard errors contribute less.\n\n\n\n\n\n6.3.2 Calculate lower and upper limits for 95% and 99.9% CI\n\nnumber.seq &lt;- seq(1, max(df$Positive), 1)\nnumber.ll95 &lt;- fit.mean - 1.96 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \nnumber.ul95 &lt;- fit.mean + 1.96 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \nnumber.ll999 &lt;- fit.mean - 3.29 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \nnumber.ul999 &lt;- fit.mean + 3.29 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \ndfCI &lt;- data.frame(number.ll95, number.ul95, number.ll999, \n                   number.ul999, number.seq, fit.mean)\n\n\n\n\n\n\n\nCode Notes\n\n\n\n\nseq(1, max(df$Positive), 1): creates a sequence of numbers from 1 to the maximum value of the “Positive” column in the data frame ‘df’, with a step size of 1. It seems to be generating a sequence of numbers representing potential counts or sample sizes.\n\n\n\n\n\n6.3.3 Plot a static funnel plot\n\np &lt;- ggplot(df, aes(x = Positive, y = rate)) +\n  geom_point(aes(label=`Sub-district`), \n             alpha=0.4) +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ll95), \n            size = 0.4, \n            colour = \"grey40\", \n            linetype = \"dashed\") +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ul95), \n            size = 0.4, \n            colour = \"grey40\", \n            linetype = \"dashed\") +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ll999), \n            size = 0.4, \n            colour = \"grey40\") +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ul999), \n            size = 0.4, \n            colour = \"grey40\") +\n  geom_hline(data = dfCI, \n             aes(yintercept = fit.mean), \n             size = 0.4, \n             colour = \"grey40\") +\n  coord_cartesian(ylim=c(0,0.05)) +\n  annotate(\"text\", x = 1, y = -0.13, label = \"95%\", size = 3, colour = \"grey40\") + \n  annotate(\"text\", x = 4.5, y = -0.18, label = \"99%\", size = 3, colour = \"grey40\") + \n  ggtitle(\"Cumulative Fatality Rate by Cumulative Number of COVID-19 Cases\") +\n  xlab(\"Cumulative Number of COVID-19 Cases\") + \n  ylab(\"Cumulative Fatality Rate\") +\n  theme_light() +\n  theme(plot.title = element_text(size=12),\n        legend.position = c(0.91,0.85), \n        legend.title = element_text(size=7),\n        legend.text = element_text(size=7),\n        legend.background = element_rect(colour = \"grey60\", linetype = \"dotted\"),\n        legend.key.height = unit(0.3, \"cm\"))\np\n\n\n\n\n\n\n6.3.4 Interactive Funnel Plot: plotly + ggplot2\n\nfp_ggplotly &lt;- ggplotly(p,\n  tooltip = c(\"label\", \n              \"x\", \n              \"y\"))\nfp_ggplotly"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html",
    "title": "Take-home Exercise 1: Study on Singapore students’ performance",
    "section": "",
    "text": "Singapore’s education has been praised for achieving high standards without major gaps between rich and poor, as per a BBC report. However, there’s still concern over inequalities among different schools, genders and social groups."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#install-r-packages",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#install-r-packages",
    "title": "Take-home Exercise 1: Study on Singapore students’ performance",
    "section": "3.1 Install R Packages",
    "text": "3.1 Install R Packages\nUse following code chunk to install relating R packages:\n\n\nCode\npacman::p_load(ggrepel, patchwork,\n               ggthemes, hrbrthemes, \n               tidyverse, haven, ggpattern, ggridges)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#dataset-overview",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#dataset-overview",
    "title": "Take-home Exercise 1: Study on Singapore students’ performance",
    "section": "3.2 Dataset Overview",
    "text": "3.2 Dataset Overview\nIn this study, the dataset used is the student questionnaire data file provided from PISA 2022 survey. It contains the personal and school information of 15-year-old students across the world, their responses to the questionnaires, and the assessment assigned by PISA experts to their performance in mathematics, science, and reading based on their answers.\nBased on the objectives of this study, which focuses on analyzing the performance of Singaporean students in mathematics, science, and reading, as well as the impact of school, gender, and socioeconomic status on these achievements, the dataset was filtered to retain the following variables for research analysis.\n:::{.panel-tabset group=“dataset_overview”}"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#retained-variables",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#retained-variables",
    "title": "Take-home Exercise 1: Study on Singapore students’ performance",
    "section": "Retained Variables",
    "text": "Retained Variables\n\n\n\n\n\n\n\n\nCol Name\nLabel\nUsed for\n\n\n\n\nSTRATUM\nSchool Type\nvariable of school type\n\n\nST004D01T\nGender\nvariable of gender\n\n\nESCS\nIndex of economic, social and cultural status\nvariable of socioeconomic status\n\n\nPV1MATH-PV10MATH\nPerformance in Math\nvariable of performance in Math\n\n\nPV1READ-PV10READ\nPerformance in Reading\nvariable of performance in Reading\n\n\nPV1SCIE-PV10SCIE\nPerformance in Science\nvariable of performance in Science"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#code-chunk",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#code-chunk",
    "title": "Take-home Exercise 1: Study on Singapore students’ performance",
    "section": "Code chunk",
    "text": "Code chunk\n\n#Filter SGP students\nstu_qqq &lt;- read_sas(\"data/cy08msp_stu_qqq.sas7bdat\")\nstu_qqq_SG &lt;- stu_qqq %&gt;%\n  filter(CNT == \"SGP\")\n\n#select columns relating to this analysis\nselect_df &lt;- stu_qqq_SG[c(3:4,7,26,1039,1167:1196)]"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#check-missing-values",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#check-missing-values",
    "title": "Take-home Exercise 1: Study on Singapore students’ performance",
    "section": "4.1 Check missing values",
    "text": "4.1 Check missing values\nMissing values will have adverse impact on subsequent analysis, so it’s essential to check missing values at the first stage of the study. Use colSums(is.na()) to check if each column has missing value.\n\n\nCode\n#check missing value\ncolSums(is.na(select_df))\n\n\n CNTSCHID  CNTSTUID   STRATUM ST004D01T      ESCS   PV1MATH   PV2MATH   PV3MATH \n        0         0         0         0        47         0         0         0 \n  PV4MATH   PV5MATH   PV6MATH   PV7MATH   PV8MATH   PV9MATH  PV10MATH   PV1READ \n        0         0         0         0         0         0         0         0 \n  PV2READ   PV3READ   PV4READ   PV5READ   PV6READ   PV7READ   PV8READ   PV9READ \n        0         0         0         0         0         0         0         0 \n PV10READ   PV1SCIE   PV2SCIE   PV3SCIE   PV4SCIE   PV5SCIE   PV6SCIE   PV7SCIE \n        0         0         0         0         0         0         0         0 \n  PV8SCIE   PV9SCIE  PV10SCIE \n        0         0         0 \n\n\nIt turns out that 47 data points in variable “ESCS” are missing, which account for 0.7% of variable “ESCS”. Since the missing percentage is quite small, we can ignore it when analyzing."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#combine-10-pvs-for-each-subject",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#combine-10-pvs-for-each-subject",
    "title": "Take-home Exercise 1: Study on Singapore students’ performance",
    "section": "4.2 Combine 10 PVs for each subject",
    "text": "4.2 Combine 10 PVs for each subject\nAccording to PISA, PV (plausible value) is the assessment of performance given by experts based on students’ responses to questions. For each subject, experts will give out 10 PVs representing 10 well-picked estimated points draw from a certain distribution which represents the range of ability or score the student might have in one subject. Multiple PVs in one subject can help to reduce the uncertainty of the test measurement. In order to retain the progress experts made, in this study, all 10 original PVs will be taken into account when forming distribution and exploring relationship between performance and school/gender/social status without being averaged.\nBased on this consideration, the current wide format need to be converted into long format, so that all the 10 PVs of each subject of each students can be listed in one column and be plotted easily.\nUse pivot_longer to convert wide format into long format.\n\n\nCode\n#Convert wide-format into long-format\nmath_long &lt;- select_df %&gt;%\n  pivot_longer(\n    cols = contains(\"MATH\"),\n    names_to = \"MATH\",\n    values_to = \"MATH_score\"\n  )\nmath_long &lt;- math_long %&gt;% \n  arrange(CNTSCHID, CNTSTUID, STRATUM, ST004D01T, ESCS)\nmath_long_selected &lt;- select(math_long, -contains(\"READ\"), -contains(\"SCIE\"))\n\nread_long &lt;- select_df %&gt;%\n  pivot_longer(\n    cols = contains(\"READ\"),\n    names_to = \"READ\",\n    values_to = \"READ_score\"\n  )\nread_long &lt;- read_long %&gt;% \n  arrange(CNTSCHID, CNTSTUID, STRATUM, ST004D01T, ESCS)\nread_long_selected &lt;- select(read_long, -contains(\"MATH\"), -contains(\"SCIE\"), -one_of(names(math_long_selected)))\n\nscie_long &lt;- select_df %&gt;%\n  pivot_longer(\n    cols = contains(\"SCIE\"),\n    names_to = \"SCIENCE\",\n    values_to = \"SCIENCE_score\"\n  )\nscie_long &lt;- scie_long %&gt;% \n  arrange(CNTSCHID, CNTSTUID, STRATUM, ST004D01T, ESCS)\nscie_long_selected &lt;- select(scie_long, -contains(\"MATH\"), -contains(\"READ\"), -one_of(names(math_long_selected)))\n\ncombined_long_df &lt;- bind_cols(math_long_selected, read_long_selected, scie_long_selected)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#group-esc-status",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#group-esc-status",
    "title": "Take-home Exercise 1: Study on Singapore students’ performance",
    "section": "4.3 Group ESC status",
    "text": "4.3 Group ESC status\nIn PISA report, ESC status is shown as index. For better view on the relationship study, the ESC status will be first converted into percentage according to the min and max index, then will be segmented into 4 groups: 0-25% as Low class, 25%-50% as Lower-middle class, 50%-75% as Upper-middle class, 75%-100% as High class."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#translate-column-names",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#translate-column-names",
    "title": "Take-home Exercise 1: Study on Singapore students’ performance",
    "section": "4.4 Translate column names",
    "text": "4.4 Translate column names\nFor better understanding on each variable, we will change the column names to the ones everyone is easy to read and understand.\nUse mutate() to segment values into different groups and change column names.\n\n\nCode\n#translate column names\nschool_map &lt;- c(\"SGP01\" = \"Public/Secondary\",\n                \"SGP02\" = \"Public/Post-secondary\",\n                \"SGP03\" = \"Private/Secondary\",\n                \"SGP97\" = \"Undisclosed\")\ngender_map &lt;- c(\"1\" = \"Female\",\n                \"2\" = \"Male\")\nclean_df &lt;- combined_long_df %&gt;%\n  mutate(SCHOOL = school_map[STRATUM],\n         GENDER = gender_map[ST004D01T],\n         Math = as.numeric(MATH_score),\n         Read = as.numeric(READ_score),\n         Science = as.numeric(SCIENCE_score),\n         ESC_status = round(((ESCS - min(ESCS, na.rm = TRUE))/\n                              (max(ESCS, na.rm = TRUE)-min(ESCS, na.rm = TRUE)))*100, digits = 0),\n         ESC_status = case_when(\n           ESC_status &gt;= 0 & ESC_status &lt; 25 ~ \"Low\",\n           ESC_status &gt;= 25 & ESC_status &lt; 50 ~ \"Lower-Middle\",\n           ESC_status &gt;= 50 & ESC_status &lt; 75 ~ \"Upper-Middle\",\n           ESC_status &gt;= 75 & ESC_status &lt;= 100 ~ \"High\",\n           TRUE ~ as.character(ESC_status)\n         ))\n\n#remove columns which are not used to plot\nclean_short_df &lt;- clean_df %&gt;%\n  select(-c(CNTSCHID, STRATUM, ST004D01T, ESCS, MATH_score, READ_score, SCIENCE_score))"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#distribution-of-performance-on-each-subject",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#distribution-of-performance-on-each-subject",
    "title": "Take-home Exercise 1: Study on Singapore students’ performance",
    "section": "5.1 Distribution of Performance on each subject",
    "text": "5.1 Distribution of Performance on each subject\nUse ggplot() and geom_histogram to plot distribution of performance on each subject. In addition, draw lines of median, 1st quantile, 3rd quantile and outliers on each histogram to reveal statistics summary of each distribution.\n\n\nCode\n#Calculate median,Q1,Q3,outliers for Math\nmedian_math &lt;- median(clean_short_df$Math)\nquantiles_math &lt;- quantile(clean_short_df$Math, probs = c(0.25, 0.75))\niqr_math &lt;- quantiles_math[2] - quantiles_math[1]\nupper_bound_math &lt;- quantiles_math[2] + 1.5 * iqr_math\nlower_bound_math &lt;- quantiles_math[1] - 1.5 * iqr_math\noutliers_math &lt;- subset(clean_short_df, Math &gt; upper_bound_math | Math &lt; lower_bound_math)\n#Calculate median,Q1,Q3,outliers for Read\nmedian_read &lt;- median(clean_short_df$Read)\nquantiles_read &lt;- quantile(clean_short_df$Read, probs = c(0.25, 0.75))\niqr_read &lt;- quantiles_read[2] - quantiles_read[1]\nupper_bound_read &lt;- quantiles_read[2] + 1.5 * iqr_read\nlower_bound_read &lt;- quantiles_read[1] - 1.5 * iqr_read\noutliers_read &lt;- subset(clean_short_df, Read &gt; upper_bound_read | Read &lt; lower_bound_read)\n#Calculate median,Q1,Q3,outliers for Science\nmedian_science &lt;- median(clean_short_df$Science)\nquantiles_science &lt;- quantile(clean_short_df$Science, probs = c(0.25, 0.75))\niqr_science &lt;- quantiles_science[2] - quantiles_science[1]\nupper_bound_science &lt;- quantiles_science[2] + 1.5 * iqr_science\nlower_bound_science &lt;- quantiles_science[1] - 1.5 * iqr_science\noutliers_science &lt;- subset(clean_short_df, Science &gt; upper_bound_science | Science &lt; lower_bound_science)\n\n#Plot composite histograms for Math, Read, Science\np1 &lt;- ggplot(data = clean_short_df,\n             aes(x= Math))+\n  geom_histogram(bins=15,\n                 color = \"grey40\",\n                 fill=\"grey95\",\n                 size = 0.5)+\n  geom_vline(aes(xintercept=median_math), \n             color=c(\"#660000\"), \n             size=0.5, \n             linetype=\"dashed\") +\n  geom_text(x = median_math, \n            y = 400, \n            label = paste(\"Median=581\"), \n            color = c(\"#660000\"),\n            size = 2) +\n  geom_vline(xintercept = quantiles_math, \n             color = c(\"#FF6666\"), \n             size = 0.5, \n             linetype = \"dashed\") +\n  geom_text(x = quantiles_math[1], \n            y = -300, \n            label = paste(\"Q1=504\"), \n            hjust = 1, \n            color = c(\"#FF6666\"),\n            size = 2) +\n  geom_text(x = quantiles_math[2], \n            y = -300, \n            label = paste(\"Q3=648\"), \n            hjust = 0, \n            color = c(\"#FF6666\"),\n            size = 2) +\n  geom_point(data = outliers_math, \n             aes(x = Math, y = 0), \n             color = c(\"#FFCC99\"), \n             size = 0.8)+\n  ylab(\"\")+\n  theme_minimal(base_size=8)\n\np2 &lt;- ggplot(data = clean_short_df,\n             aes(x= Read))+\n  geom_histogram(bins=15,\n                 color = \"grey40\",\n                 fill=\"grey95\",\n                 size = 0.5)+\n  geom_vline(aes(xintercept=median_read), \n             color=c(\"#660000\"), \n             size=0.5, \n             linetype=\"dashed\") +\n  geom_text(x = median_read, \n            y = 450, \n            label = paste(\"Median=551\"), \n            color = c(\"#660000\"),\n            size = 1.8) +\n  geom_vline(xintercept = quantiles_read, \n             color = c(\"#FF6666\"), \n             size = 0.5, \n             linetype = \"dashed\") +\n  geom_text(x = quantiles_read[1], \n            y = -350, \n            label = paste(\"Q1=473\"), \n            hjust = 1, \n            color = c(\"#FF6666\"),\n            size = 1.8) +\n  geom_text(x = quantiles_read[2], \n            y = -350, \n            label = paste(\"Q3=618\"), \n            hjust = 0, \n            color = c(\"#FF6666\"),\n            size = 1.8) +\n  geom_point(data = outliers_read, \n             aes(x = Read, y = 0), \n             color = c(\"#FFCC99\"), \n             size = 0.8)+\n  labs(y=\"\",title=\"Distribution of Performance\")+\n  theme_minimal(base_size=8)\n\np3 &lt;- ggplot(data = clean_short_df,\n             aes(x= Science))+\n  geom_histogram(bins=15,\n                 color = \"grey40\",\n                 fill=\"grey95\",\n                 size = 0.5)+\n  geom_vline(aes(xintercept=median_science), \n             color=c(\"#660000\"), \n             size=0.5, \n             linetype=\"dashed\") +\n  geom_text(x = median_science, \n            y = 450, \n            label = paste(\"Median=568\"), \n            color = c(\"#660000\"),\n            size = 1.8) +\n  geom_vline(xintercept = quantiles_science, \n             color = c(\"#FF6666\"), \n             size = 0.5, \n             linetype = \"dashed\") +\n  geom_text(x = quantiles_science[1], \n            y = -350, \n            label = paste(\"Q1=495\"), \n            hjust = 1, \n            color = c(\"#FF6666\"),\n            size = 1.8) +\n  geom_text(x = quantiles_science[2], \n            y = -350, \n            label = paste(\"Q3=631\"), \n            hjust = 0, \n            color = c(\"#FF6666\"),\n            size = 1.8) +\n  geom_point(data = outliers_science, \n             aes(x = Science, y = 0), \n             color = c(\"#FFCC99\"), \n             size = 0.8)+\n  ylab(\"\")+\n  theme_minimal(base_size=8)\n\n(p2/p3)|p1\n\n\n\n\n\n\n\n\n\n\n\nAnalysis\n\n\n\n\nPerformance of most students in three subjects is all around 600 (max score is 1000).\nMost students performance well in math, followed by science, then reading.\nAll the distribution of performance shows relatively normal, without significant skewness."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#relationship-bw-gender-and-performance",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#relationship-bw-gender-and-performance",
    "title": "Take-home Exercise 1: Study on Singapore students’ performance",
    "section": "5.2 Relationship b/w Gender and Performance",
    "text": "5.2 Relationship b/w Gender and Performance\nUse ggplot() and geom_density to plot distribution of performance on each subject in terms of gender.\n\n\nCode\n##Gender and Subject\n#1. Math\n#1.1 Create two ribbon areas for Math\nfemale_dens_math_1 &lt;- density(clean_short_df$Math[clean_short_df$GENDER == \"Female\"], from = 420, to = 635, n = 512)\nmale_dens_math_1 &lt;- density(clean_short_df$Math[clean_short_df$GENDER == \"Male\"], from = 420, to = 635, n = 512)\nribbon_data_1 &lt;- data.frame(\n  x = female_dens_math_1$x,\n  ymin = pmin(female_dens_math_1$y, male_dens_math_1$y),\n  ymax = pmax(female_dens_math_1$y, male_dens_math_1$y)\n)\n\nfemale_dens_math_2 &lt;- density(clean_short_df$Math[clean_short_df$GENDER == \"Female\"], from = 640, to = 850, n = 512)\nmale_dens_math_2 &lt;- density(clean_short_df$Math[clean_short_df$GENDER == \"Male\"], from = 640, to = 850, n = 512)\nribbon_data_2 &lt;- data.frame(\n  x = female_dens_math_2$x,\n  ymin = pmin(male_dens_math_2$y, female_dens_math_2$y),\n  ymax = pmax(male_dens_math_2$y, female_dens_math_2$y)\n)\n\n#1.2 Plot density graph with marked ribbon areas for Math\np4 &lt;- ggplot(data = clean_short_df, \n             aes(x = Math,\n                 fill = GENDER)) +\n  geom_density(alpha=0.4) +\n  coord_cartesian(xlim = c(0, 1000), ylim = c(0, 0.005)) +\n  labs(y = \"Density\", x = \"Math\") +\n  theme_minimal(base_size = 8) +\n  geom_ribbon_pattern(data = subset(ribbon_data_1, x &gt;= 420 & x &lt;= 635),\n                      aes(x = x, ymin = ymin, ymax = ymax),\n                      pattern = 'stripe', pattern_angle = 45, pattern_density = 0.1,\n                      pattern_spacing = 0.02, pattern_key_scale_factor = 0.5,\n                      fill = \"orange\",alpha=0.01, pattern_colour =c(\"#CC0066\"))+\n  geom_text(aes(x = 350, \n                y = 0.0042, \n                label = \"Femals lead mid scores↓\"), \n            color = c(\"#CC0066\"), \n            angle = 0, \n            vjust = 0.5,\n            size = 2.2)+\n  geom_ribbon_pattern(data = subset(ribbon_data_2, x &gt;= 640 & x &lt;= 850),\n                      aes(x = x, ymin = ymin, ymax = ymax),\n                      pattern = 'stripe', pattern_angle = 135, pattern_density = 0.1,\n                      pattern_spacing = 0.02, pattern_key_scale_factor = 0.5,\n                      fill = \"orange\",alpha=0.01, pattern_colour =c(\"#003366\"))+\n  geom_text(aes(x = 790, \n                y = 0.0035, \n                label = \"Males lead \n                ↙high scores\"), \n            color = c(\"#003366\"), \n            angle = 0, \n            vjust = 0.5,\n            size = 2.2)\n\n#2. Science\np5 &lt;- ggplot(data = clean_short_df, \n             aes(x = Science,\n                 fill = GENDER)) +\n  geom_density(alpha=0.4) +\n  coord_cartesian(xlim = c(0, 1000)) +\n  labs(y = \"Density\", x = \"Science\", title = \"Relationship: Gender and Performance\") +\n  theme_minimal(base_size = 8) +\n  theme(legend.position = \"none\")\n\n#3. Read\np6 &lt;- ggplot(data = clean_short_df, \n             aes(x = Read,\n                 fill = GENDER)) +\n  geom_density(alpha=0.4) +\n  coord_cartesian(xlim = c(0, 1000)) +\n  labs(y = \"Density\", x = \"Read\") +\n  theme_minimal(base_size = 8)+\n  theme(legend.position = \"none\")\n\n(p5/p6)|p4\n\n\n\n\n\n\n\n\n\n\n\nAnalysis\n\n\n\n\nAmong all subjects, performance of females is all above males in mid scores.\nFor math and science, males perform better in high scores than females.\nFor reading, females perform better in high scores than males.\nMales tend to be good at science, while females are good at arts."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#relationship-bw-school-types-and-performance",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#relationship-bw-school-types-and-performance",
    "title": "Take-home Exercise 1: Study on Singapore students’ performance",
    "section": "5.3 Relationship b/w school types and performance",
    "text": "5.3 Relationship b/w school types and performance\nUse ggplot() and geom_boxplot to plot statistics summary of performance on each subject in terms of school types.\n\n\nCode\n##School and Subject\np7 &lt;- ggplot(data = clean_short_df, \n             aes(y = Math, \n                 x= SCHOOL)) +\n  geom_boxplot(notch=TRUE)+\n  theme_minimal(base_size=7)+\n  labs(x=\"\",title=\"Relationship: School Type and Performance\")\np8 &lt;- ggplot(data = clean_short_df, \n             aes(y = Science, \n                 x= SCHOOL)) +\n  geom_boxplot(notch=TRUE)+\n  theme_minimal(base_size=7)+\n  labs(x=\"School Type\")\np9 &lt;- ggplot(data = clean_short_df, \n             aes(y = Read, \n                 x= SCHOOL)) +\n  geom_boxplot(notch=TRUE)+\n  theme_minimal(base_size=7)+\n  labs(x=\"\")\np7+p8+p9\n\n\n\n\n\n\n\n\n\n\n\nAnalysis\n\n\n\n\nOn average, students from private school perform better than those from public school in all the subjects.\nStudents from public school show a greater level of difference on performance than those from private school.\nPrivate school tend to have better teaching quality."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#relationship-bw-socioeconomic-status-and-performance",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#relationship-bw-socioeconomic-status-and-performance",
    "title": "Take-home Exercise 1: Study on Singapore students’ performance",
    "section": "5.4 Relationship b/w socioeconomic status and performance",
    "text": "5.4 Relationship b/w socioeconomic status and performance\nUse ggplot() and geom_boxplot to plot statistics summary of performance on each subject in terms of socioeconomic status.\nPreviously in data overview stage, it’s found that ESC_status variable has 47 missing data point. Since the missing percentage is rather small, we will ignore the missing in this analysis stage. Use na.omit() to ignore missing data.\n\n\nCode\n##ESC Status and Subject\np10&lt;-ggplot(data = na.omit(clean_short_df), \n       aes(y = Math, \n           x= ESC_status)) +\n  geom_boxplot(notch=TRUE)+\n  theme_minimal(base_size=6)+\n  labs(x=\"ESC status\", title=\"Relationship: ESC status and Performance\")\np11&lt;-ggplot(data = na.omit(clean_short_df), \n            aes(y = Read, \n                x= ESC_status)) +\n  geom_boxplot(notch=TRUE)+\n  theme_minimal(base_size=6)+\n  labs(x=\"\")\n\np12&lt;-ggplot(data = na.omit(clean_short_df), \n       aes(y = Science, \n           x= ESC_status)) +\n  geom_boxplot(notch=TRUE)+\n  theme_minimal(base_size=6)+\n  labs(x=\"\")\np10+p11+p12\n\n\n\n\n\n\n\n\n\n\n\nAnalysis\n\n\n\n\nStudents from high class perform significantly well among those from other 3 classes.\nStudents from upper-middle class are ranked in the second place, followed by lower-middle class, then low class.\nStudents from a higher class tend to perform better in all subjects."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#data-limitation",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#data-limitation",
    "title": "Take-home Exercise 1: Study on Singapore students’ performance",
    "section": "6.1 Data Limitation",
    "text": "6.1 Data Limitation\nSince this study relies on only one dataset, it’s important to acknowledge some limitations:\n\nTemporal Specificity: The data is specific to the year 2022 and may not represent the current and long-term tendency occurred or after this period. Therefore, the findings are limited to this specific year.\nSampling Bias: PISA collected data from a sample of 15-year-old students, which may not represent all the population of students in Singapore. Therefore, this could influence the generalizability of the results to all students.\nVariable selection: The analysis is based on selected variables related to student performance, school, gender and ESC status. Other potentially relevant factors may not be included in the dataset."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#study-summary",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#study-summary",
    "title": "Take-home Exercise 1: Study on Singapore students’ performance",
    "section": "6.2 Study Summary",
    "text": "6.2 Study Summary\nAfter applying the exploratory data analysis methods to the dataset, there are several insights gained from it which may arouse the attention to the governers and the public of Singapore:\n\nMost students perform well in math, followed by science, then reading.\nMales tend to be good at science, while females are good at arts.\nPrivate school tend to have better teaching quality.\nStudents with a higher socioeconomic status tend to perform better in all subjects."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ISSS608-VAA",
    "section": "",
    "text": "Welcome to ISSS608 Visual Analytics and Applications! I’m Yanrui.\nIn this website, you will find my learning journey of Visual Analytics taught by Prof Kam Tin Seong."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#import-and-load-packages",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#import-and-load-packages",
    "title": "Hands-on Exercise 6: Visualising and Analysing Time-Oriented Data",
    "section": "2.1 Import and load packages",
    "text": "2.1 Import and load packages\nThe following R packages will be used in this exercise:\n\npacman::p_load(scales,viridis, lubridate,\n               ggthemes, gridExtra, readxl,\n               knitr, data.table,CGPfunctions,\n               ggHoriPlot, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#overview-of-calendar-heatmap",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#overview-of-calendar-heatmap",
    "title": "Hands-on Exercise 6: Visualising and Analysing Time-Oriented Data",
    "section": "3.1 Overview of Calendar Heatmap",
    "text": "3.1 Overview of Calendar Heatmap\nCalendar Heatmaps are particularly useful for highlighting the intensity or frequency of events over a calendar period.\nHere are several scenarios where calendar heatmaps can be effectively used for visualization:\n\nWebsite Traffic Analysis: To visualize daily website visits or user activity, highlighting peak usage times or identifying days with unusually low traffic.\nFitness Tracking: For displaying daily workout durations, steps taken, or calories burned, allowing users to easily see their activity patterns over time.\nSales Data Visualization: To show daily sales figures for a business, making it easy to spot trends, seasonal effects, or particular days with high sales volumes.\nSocial Media Engagement: To track daily likes, comments, or shares on social media posts, helping to identify content that performs well or times when engagement spikes.\nProject Management and Productivity: For tracking tasks completed, time spent on projects, or milestones reached on a daily basis, offering insights into productivity patterns.\nHealth and Medical Records: To monitor daily health-related metrics, such as blood sugar levels, blood pressure, or symptom frequency, useful for patients and healthcare providers to identify trends or triggers.\nEnvironmental Data Monitoring: To display daily temperature, rainfall, or air quality index readings, useful for environmental analysis and understanding seasonal variations.\nEnergy Consumption Analysis: For visualizing daily electricity, gas, or water usage, helping households or businesses to identify periods of high consumption and potential for savings.\nSoftware Development: To track daily commits, pull requests, or issues closed in a software project, highlighting productivity and collaboration patterns within a development team.\nCustomer Support: For visualizing the number of support tickets received or resolved each day, helping to identify peak times for customer issues and evaluate support team performance.\nAttendance and Absenteeism: To track daily attendance records in schools or workplaces, making it easier to spot absenteeism trends and address issues promptly.\nFinancial Markets: For tracking daily stock prices, trading volumes, or market indices, allowing investors to spot trends and make informed decisions based on historical performance.\nEvent Logging and Monitoring: For IT infrastructure, to visualize logs or alerts generated each day, helping in identifying patterns of system behavior or potential security breaches."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#importing-the-data",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#importing-the-data",
    "title": "Hands-on Exercise 6: Visualising and Analysing Time-Oriented Data",
    "section": "3.2 Importing the data",
    "text": "3.2 Importing the data\nFor the purpose of this hands-on exercise, eventlog.csv file will be used. This data file consists of 199,999 rows of time-series cyber attack records by country.\n\nattacks &lt;- read_csv(\"data/eventlog.csv\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#examining-the-data-structure",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#examining-the-data-structure",
    "title": "Hands-on Exercise 6: Visualising and Analysing Time-Oriented Data",
    "section": "3.3 Examining the data structure",
    "text": "3.3 Examining the data structure\nIt is always a good practice to examine the imported data frame before further analysis is performed.\nFor example, kable() can be used to review the structure of the imported data frame.\n\nkable(head(attacks))\n\n\n\n\ntimestamp\nsource_country\ntz\n\n\n\n\n2015-03-12 15:59:16\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:00:48\nFR\nEurope/Paris\n\n\n2015-03-12 16:02:26\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:02:38\nUS\nAmerica/Chicago\n\n\n2015-03-12 16:03:22\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:03:45\nCN\nAsia/Shanghai\n\n\n\n\n\n\n\n\n\n\n\nCode Notes\n\n\n\n\nkable(head()) vs. head()\n1.1 head(): used to view the first 6 rows (excluding header lines) by default of a DataFrame or Matrix. This method is simple and direct, suitable for quickly viewing the structure and content of the data.\n1.2 kable(head()): from the knitr package, is a table formatting function used to create a more visually appealing table presentation. It formats the first 6 rows of the dataset (excluding header line) into a table more suitable for display in Markdown, HTML, or PDF documents. Better visual appearance, suitable for inclusion in reports and documents, offering more customization options, such as table alignment and column formatting.\n\n\n\nThere are three columns, namely timestamp, source_country and tz.\n\ntimestamp field stores date-time values in POSIXct format.\nsource_country field stores the source of the attack. It is in ISO 3166-1 alpha-2 country code.\ntz field stores time zone of the source IP address."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#data-preparation",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#data-preparation",
    "title": "Hands-on Exercise 6: Visualising and Analysing Time-Oriented Data",
    "section": "3.4 Data Preparation",
    "text": "3.4 Data Preparation\nStep 1: Deriving weekday and hour of day fields\nBefore we can plot the calender heatmap, two new fields namely wkday and hour need to be derived. In this step, we will write a function to perform the task.\n\nmake_hr_wkday &lt;- function(ts,sc,tz){\n  real_times &lt;- ymd_hms(ts,\n                        tz=tz[1],\n                        quiet = TRUE)\n  dt &lt;- data.table(source_country = sc,\n                   wkday = weekdays(real_times),\n                   hour = hour(real_times))\n  return(dt)\n}\n\n\n\n\n\n\n\nCode Notes\n\n\n\n\nmake_hr_wkday &lt;- function(ts,sc,tz){}: is a function with three parameters: ts, sc, tz\nymd_hms(): from the lubridate package, is used to parse the ts vector of timestamps into POSIXct/POSIXlt objects, e.g. “2023-02-20 14:30:00”\n2.1 tz[1]: the first element of the tz vector, used as the time zone for all timestamps. It applies the first element (first row: “Asia/Shanghai”) of the tz vector as the timezone for all timestamps in the ts vector. The timestamps originally belongs to other timezone will be directly adjusted to the specific timezone without any transformation. The table will be grouped by tz later on to address this issue.\n2.2 quiet=TRUE: suppresses warning messages that might arise from any parsing issues.\ndata.table(): a is created with three columns: source_country, wkday, hour\n3.1 weekdays(): a base R function, will return result as the name of the weekdays, e.g. “Monday”\n3.2 hour(): from the lubridate package. Extract the hour of the day for each timestamp.\n\n\n\n\nStep 2: Deriving the attacks tibble data frame\n\nwkday_levels &lt;- c('Saturday', 'Friday', \n                  'Thursday', 'Wednesday', \n                  'Tuesday', 'Monday', \n                  'Sunday')\n\nattacks &lt;- attacks %&gt;%\n  group_by(tz) %&gt;%\n  do(make_hr_wkday(.$timestamp,\n                   .$source_country,\n                   .$tz)) %&gt;%\n  ungroup() %&gt;%\n  mutate(wkday = factor(\n    wkday, levels = wkday_levels),\n    hour = factor(\n      hour, levels = 0:23))\n\n\n\n\n\n\n\nCode Notes\n\n\n\n\ndo(): is used to apply custom functions to grouped data, allowing for complex operations to be performed on each group.\n1.1 .$col_name: Instead of using datatable$col_name, .$col_name is used in pipe operations, allowing to directly reference a column of a data frame or data table as it is passed through the pipe. This approach avoids the need to repeatedly specify the name of the data frame, making the code more concise.\nfactor(col_name,levels=): convert the column into factor, which is a method for handling categorical data in R. levels parameter is to control the levels of the factor (i.e., the order of the categories), which is very useful in data analysis and visualization.\n\n\n\nTable below shows the tidy tibble table after processing.\n\nkable(head(attacks))\n\n\n\n\ntz\nsource_country\nwkday\nhour\n\n\n\n\nAfrica/Cairo\nBG\nSaturday\n20\n\n\nAfrica/Cairo\nTW\nSunday\n6\n\n\nAfrica/Cairo\nTW\nSunday\n8\n\n\nAfrica/Cairo\nCN\nSunday\n11\n\n\nAfrica/Cairo\nUS\nSunday\n15\n\n\nAfrica/Cairo\nCA\nMonday\n11"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#building-the-calendar-heatmaps",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#building-the-calendar-heatmaps",
    "title": "Hands-on Exercise 6: Visualising and Analysing Time-Oriented Data",
    "section": "3.5 Building the Calendar Heatmaps",
    "text": "3.5 Building the Calendar Heatmaps\n\ngrouped &lt;- attacks %&gt;%\n  count(wkday, hour) %&gt;%\n  ungroup() %&gt;%\n  na.omit()\n\nggplot(grouped,\n       aes(hour,\n           wkday,\n           fill=n))+\n  geom_tile(color=\"white\",\n            size=0.1)+\n  theme_tufte(base_family = \"Helvetica\")+\n  coord_equal()+\n  scale_fill_gradient(name=\"# of attacks\",\n                      low=\"skyblue\",\n                      high=\"darkblue\")+\n  labs(x=NULL,\n       y=NULL,\n       title=\"Attacks by weekday and time of day\")+\n  theme(axis.ticks=element_blank(),\n        plot.title=element_text(hjust=0.5),\n        legend.title=element_text(size=8),\n        legend.text=element_text(size=6))\n\n\n\n\n\n\n\n\n\n\nCode Notes\n\n\n\n\ngrouped: a tibble data table called grouped is derived by aggregating the attack by wkday and hour fields.\n1.1 count(): will return the only columns used to be grouped and counted, here refer to wkday and hour, and create a new column called n by default to record the number of occurence with combination of unique wkday and hour.\nungroup(): After using count(), the resulting data frame is still grouped by the variables specified in count(). The ungroup() function is used to remove this grouping. While count() automatically ungroups the data in most cases, explicitly calling ungroup() can be a good practice for clarity or in preparation for subsequent operations that should not be affected by the previous grouping.\nna.omit(): removes all rows from the data frame that contain NA (missing values) in any column.\ngeom_tile(): is used to plot tiles (grids) at each x and y position. Color and size arguments are used to specify the border color and line size of the tiles.\ncoord_equal(): Ensures that one unit on the x-axis is the same length as one unit on the y-axis, which can help make the plot easier to interpret, especially for spatial representations like heatmaps.\nscale_fill_gradient(): Defines a gradient scale for the fill color based on the count of attacks. name parameter provides a label for the legend that explains what the colors represent.\nlabs(x=NULL,y=NULL): removes x and y labels"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#building-the-calendar-heatmap",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#building-the-calendar-heatmap",
    "title": "Hands-on Exercise 6: Visualising and Analysing Time-Oriented Data",
    "section": "3.5 Building the Calendar Heatmap",
    "text": "3.5 Building the Calendar Heatmap\n\ngrouped &lt;- attacks %&gt;%\n  count(wkday, hour) %&gt;%\n  ungroup() %&gt;%\n  na.omit()\n\nggplot(grouped,\n       aes(hour,\n           wkday,\n           fill=n))+\n  geom_tile(color=\"white\",\n            size=0.1)+\n  theme_tufte(base_family = \"Helvetica\")+\n  coord_equal()+\n  scale_fill_gradient(name=\"# of attacks\",\n                      low=\"skyblue\",\n                      high=\"darkblue\")+\n  labs(x=NULL,\n       y=NULL,\n       title=\"Attacks by weekday and time of day\")+\n  theme(axis.ticks=element_blank(),\n        plot.title=element_text(hjust=0.5),\n        legend.title=element_text(size=8),\n        legend.text=element_text(size=6))\n\n\n\n\n\n\n\n\n\n\nCode Notes\n\n\n\n\ngrouped: a tibble data table called grouped is derived by aggregating the attack by wkday and hour fields.\n1.1 count(): will return the only columns used to be grouped and counted, here refer to wkday and hour, and create a new column called n by default to record the number of occurence with combination of unique wkday and hour.\nungroup(): After using count(), the resulting data frame is still grouped by the variables specified in count(). The ungroup() function is used to remove this grouping. While count() automatically ungroups the data in most cases, explicitly calling ungroup() can be a good practice for clarity or in preparation for subsequent operations that should not be affected by the previous grouping.\nna.omit(): removes all rows from the data frame that contain NA (missing values) in any column.\ngeom_tile(): is used to plot tiles (grids) at each x and y position. Color and size arguments are used to specify the border color and line size of the tiles.\ncoord_equal(): Ensures that one unit on the x-axis is the same length as one unit on the y-axis, which can help make the plot easier to interpret, especially for spatial representations like heatmaps.\nscale_fill_gradient(): Defines a gradient scale for the fill color based on the count of attacks. name parameter provides a label for the legend that explains what the colors represent.\nlabs(x=NULL,y=NULL): removes x and y labels"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#building-multiple-calendar-heatmaps",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#building-multiple-calendar-heatmaps",
    "title": "Hands-on Exercise 6: Visualising and Analysing Time-Oriented Data",
    "section": "3.6 Building Multiple Calendar Heatmaps",
    "text": "3.6 Building Multiple Calendar Heatmaps\nIn this sector, we will extend one calendar heatmap to multiple heatmaps for the top four countries with the highest number of attacks.\nStep 1: Deriving attack by country object\nIn order to identify the top 4 countries with the highest number of attacks, you are required to do the followings:\n\ncount the number of attacks by country\ncalculate the percent of attackes by country\nsave the results in a tibble data frame\n\n\nattacks_by_country &lt;- count(\n  attacks, source_country) %&gt;%\n  mutate(percent=percent(n/sum(n))) %&gt;%\n  arrange(desc(n))\n\n\n\n\n\n\n\nCode Notes\n\n\n\n\ncount(data,vars,...,wt=NULL,sort=FALSE,name=\"n\")\n1.1 data: data frame. No need to define it if there is ‘data frame %&gt;%’ in front of it.\n1.2 vars: column name used to be grouped.\n1.3 ...: more column names to be grouped.\n1.4 wt: optional parameter, indicate weight.\n1.5 sort: boolean value, define the order of counting number.\n1.6 name: the column name of counting number, it’s called ‘n’ by default. The ‘n’ in this code chunk refer to the counting number resulted from this count function, not the previous one from ‘grouped’ data frame.\n\n\n\n\nStep 2: Preparing the tidy data frame\nIn this step, you are required to extract the attack records of the top 4 countries from attacks data frame and save the data in a new tibble data frame (i.e. top4_attacks).\n\ntop4 &lt;- attacks_by_country$source_country[1:4]\n\ntop4_attacks &lt;- attacks %&gt;%\n  filter(source_country %in% top4) %&gt;%\n  count(source_country,wkday,hour) %&gt;%\n  ungroup() %&gt;%\n  mutate(source_country = factor(\n    source_country, levels=top4)) %&gt;%\n  na.omit()\n\n\n\n\n\n\n\nCode Notes\n\n\n\n\nfilter( %in% ): %in% operator is used to check whether the element on the left side is in the vector on the right side.\n\n\n\nStep 3: Plotting the Multiple Calender Heatmap by using ggplot2 package.\n\nggplot(top4_attacks,\n       aes(hour,\n           wkday,\n           fill=n))+\n  geom_tile(color=\"white\",\n            size=0.1)+\n  theme_tufte(base_family = \"Helvetica\")+\n  coord_equal()+\n  scale_fill_gradient(name=\"# of attacks\",\n                      low=\"skyblue\",\n                      high=\"darkblue\")+\n  facet_wrap(~source_country, ncol=2)+\n  labs(x=NULL,y=NULL,\n       title=\"Attacks on top 4 countries by weekday and time of day\")+\n  theme(axis.ticks=element_blank(),\n        axis.text.x = element_text(size=7),\n        plot.title = element_text(hjust=0.5),\n        legend.title = element_text(size=8),\n        legend.text = element_text(size=6))\n\n\n\n\n\n\n\n\n\n\nCode Notes\n\n\n\n\nfacet_wrap(~source_country, ncol = 2): to create a separate plot or panel (facet) for each level of the source_country factor within the top4_attacks data frame.\n~source_country: indicates that the facets should be created based on the unique values of the source_country column in the top4_attacks data frame. Each country will have its own panel in the resulting plot.\nncol=2: This parameter specifies the number of columns in the grid layout."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#overview",
    "title": "Hands-on Exercise 6: Visualising and Analysing Time-Oriented Data",
    "section": "4.1 Overview",
    "text": "4.1 Overview\nCycle plots are a type of visualization used to explore trends and patterns over time, particularly when data exhibits seasonal or cyclical patterns.\nHere are several scenarios where cycle plots can be particularly useful for visualization:\n\nSales and Retail Analysis: For tracking monthly or quarterly sales data to identify seasonal trends, such as increased sales during holiday periods or summer slumps.\nWeather and Climate Studies: To examine temperature, precipitation, or other weather variables across different seasons or years, helping to identify long-term climate trends alongside seasonal variability.\nEnergy Consumption: Analyzing daily or monthly electricity or gas usage to identify patterns of consumption that vary by season, such as higher energy use in winter for heating or in summer for air conditioning.\nTourism and Hospitality: Understanding seasonal trends in hotel bookings, flights, or tourist arrivals, which can be critical for planning and resource allocation.\nAgriculture: Monitoring crop yields or pest activity across different planting seasons to assist in planning for planting, harvesting, and pest control.\nFinancial Markets: Analyzing seasonal effects in stock market returns or the performance of certain sectors, such as the retail sector’s performance during the holiday shopping season.\nWebsite Traffic: Examining patterns in website visits or user engagement metrics to identify times of the year when traffic peaks or dips, which can inform content and marketing strategies.\nHealthcare and Epidemiology: Tracking the occurrence of infectious diseases or hospital admissions to identify seasonal patterns, such as flu season peaks or variations in certain conditions related to weather or seasonal activities.\nTransportation and Traffic Analysis: Understanding seasonal variations in traffic patterns, public transportation usage, or air travel to improve planning and infrastructure development.\nProduct Lifecycle Management: Analyzing the sales cycle of different products to understand seasonal demand patterns, which can guide inventory management and promotional activities."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#import-data",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#import-data",
    "title": "Hands-on Exercise 6: Visualising and Analysing Time-Oriented Data",
    "section": "4.2 Import data",
    "text": "4.2 Import data\nFor the purpose of this hands-on exercise, arrivals_by_air.xlsx will be used.\nThe code chunk below imports arrivals_by_air.xlsx by using read_excel() of readxl package and save it as a tibble data frame called air.\n\nair &lt;- read_excel(\"data/arrivals_by_air.xlsx\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#data-preparation-1",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#data-preparation-1",
    "title": "Hands-on Exercise 6: Visualising and Analysing Time-Oriented Data",
    "section": "4.3 Data Preparation",
    "text": "4.3 Data Preparation\nStep 1: Deriving month and year fields\nTwo new fields called month and year are derived from Month-Year field.\n\nair$month &lt;- factor(month(air$`Month-Year`),\n                    levels=1:12,\n                    labels=month.abb,\n                    ordered=TRUE)\n\nair$year &lt;- year(ymd(air$`Month-Year`))\n\n\n\n\n\n\n\nCode Notes\n\n\n\n\nlabels: to define the name of each factor./ 1.1 month.abb: abbreviation of month, e.g. c(‘Jan’,‘Feb’,‘Mar’,…)/\nymd(): intelligently recognizes many different year, month, and day formats and converts them to Date objects for R.\n\n\n\nStep 2: Extracting the target country\nthe code chunk below is use to extract data for the target country (i.e. Vietnam)\n\nVietnam &lt;- air %&gt;%\n  select(`Vietnam`,\n         month,\n         year) %&gt;%\n  filter(year &gt;= 2010)\n\nStep 3: Computing year average arrivals by month\nThe code chunk below uses group_by() and summarise() of dplyr to compute year average arrivals by month.\n\nhline.data &lt;- Vietnam %&gt;%\n  group_by(month) %&gt;%\n  summarise(avgvalue=mean(`Vietnam`))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#plotting-the-cycle-plot",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#plotting-the-cycle-plot",
    "title": "Hands-on Exercise 6: Visualising and Analysing Time-Oriented Data",
    "section": "4.4 Plotting the cycle plot",
    "text": "4.4 Plotting the cycle plot\n\nggplot()+\n  geom_line(data=Vietnam,\n            aes(x=year,\n                y=`Vietnam`,\n                group=month),\n            colour=\"black\")+\n  geom_hline(aes(yintercept=avgvalue),\n             data=hline.data,\n             linetype=6,\n             colour=\"red\",\n             size=0.5)+\n  facet_grid(~month)+\n  labs(title=\"Visitor arrivals from Vietnam by air, Jan 2010-Dec 2019\")+\n  theme(axis.text.x=element_blank())+\n  xlab(\"\")+\n  ylab(\"No. of Visitors\")+\n  theme_tufte(base_family = \"Helvetica\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#overview-1",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#overview-1",
    "title": "Hands-on Exercise 6: Visualising and Analysing Time-Oriented Data",
    "section": "5.1 Overview",
    "text": "5.1 Overview\nSlopegraphs are a type of visualization that display change between two points in time or between two conditions for multiple subjects or items.\nThey are particularly useful for highlighting the magnitude of change, ranking shifts, or comparing individual trajectories.\nHere are several scenarios where slopegraphs can be effectively used for visualization:\n\nPre and Post Analysis: Comparing metrics before and after a specific event or intervention, such as sales figures before and after a marketing campaign, to visualize the impact.\nYear-over-Year Performance: Tracking changes in performance metrics, such as revenue or customer satisfaction scores, across different years for multiple departments or products.\nEducational Growth or Decline: Visualizing test scores or graduation rates across different schools or districts from one year to the next, to identify trends in educational outcomes.\nHealthcare Trends: Comparing patient outcomes, such as recovery rates or disease incidence, before and after implementing new healthcare policies or treatments.\nEnvironmental Changes: Tracking changes in environmental data, like air quality or temperature averages, between two time periods to highlight climate change or the impact of environmental policies.\nEconomic Indicators: Visualizing shifts in economic indicators like GDP, unemployment rates, or inflation between two time points for different countries or regions.\nTechnology Adoption Rates: Comparing the adoption rates of different technologies or software versions between two time points to analyze market trends.\nEmployee or Team Performance: Evaluating changes in employee productivity or team performance metrics between two evaluation periods.\nSocial Media Engagement: Tracking changes in social media metrics, such as follower count or engagement rates, before and after a campaign or event.\nSports Statistics: Comparing athletes’ performance stats, such as race times or points scored, across seasons or before and after a coaching change."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#import-data-1",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#import-data-1",
    "title": "Hands-on Exercise 6: Visualising and Analysing Time-Oriented Data",
    "section": "5.2 Import data",
    "text": "5.2 Import data\nImport the rice data set into R environment by using the code chunk below.\n\nrice &lt;- read_csv(\"data/rice.csv\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#plotting-the-slopegraph",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#plotting-the-slopegraph",
    "title": "Hands-on Exercise 6: Visualising and Analysing Time-Oriented Data",
    "section": "5.3 Plotting the slopegraph",
    "text": "5.3 Plotting the slopegraph\nCode chunk below will be used to plot a basic slopegraph as shown below.\n\nrice %&gt;%\n  mutate(Year = factor(Year)) %&gt;%\n  filter(Year %in% c(1961,1980)) %&gt;%\n  newggslopegraph(Year, Yield, Country,\n                  Title = \"Rice Yield of Top 11 Asian Countries\",\n                  SubTitle = \"1961-1980\",\n                  Caption = \"Prepared by: Wei Yanrui\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html",
    "title": "Hands-on Exercise 6: Visualising and Analysing Time-Oriented Data",
    "section": "",
    "text": "roadmap_6"
  },
  {
    "objectID": "In-Class_Ex/In-Class_Ex06/In-Class_Ex06.html",
    "href": "In-Class_Ex/In-Class_Ex06/In-Class_Ex06.html",
    "title": "Horizon Plot",
    "section": "",
    "text": "Install packages\n\npacman::p_load(ggHoriPlot, ggthemes,\n               tidyverse)\n\nImport data\n\naverp &lt;- read_csv(\"data/AVERP.csv\")\n\nDate is treated as character type, need to change to date type\n\naverp&lt;- averp %&gt;%\n  mutate(`Date` = dmy(`Date`))\n\nPlot\n\naverp %&gt;% \n  filter(Date &gt;= \"2018-01-01\") %&gt;%\n  ggplot() +\n  geom_horizon(aes(x = Date, y=Values), \n               origin = \"midpoint\", \n               horizonscale = 6)+\n  facet_grid(`Consumer Items`~.) +\n    theme_few() +\n  scale_fill_hcl(palette = 'RdBu') +# diverging color is used to show positive and negative difference\n  theme(panel.spacing.y=unit(0, \"lines\"), strip.text.y = element_text(\n    size = 5, angle = 0, hjust = 0),\n    legend.position = 'none',\n    axis.text.y = element_blank(),\n    axis.text.x = element_text(size=7),\n    axis.title.y = element_blank(),\n    axis.title.x = element_blank(),\n    axis.ticks.y = element_blank(),\n    panel.border = element_blank()\n    ) +\n    scale_x_date(expand=c(0,0), date_breaks = \"3 month\", date_labels = \"%b%y\") +\n  ggtitle('Average Retail Prices of Selected Consumer Items (Jan 2018 to Dec 2022)')"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#overview",
    "title": "Hands-on Exercise 7: Visualising and Analysing Geographic Data",
    "section": "2.1 Overview",
    "text": "2.1 Overview\nWhat is Choropleth Maps?\n\na type of map that uses colors to represent the magnitude of specific data values within certain areas, such as population density, income levels, or any other statistical data.\n\nWhy use Choropleth Maps?\n\nVisual representation: shows the differences in data between geographical areas, allowing one to see at a glance where values are high or low.\nEasy to understand.\nWide application: climate change, economic development, happy index, social issues, and other different fields.\n\nHow does it work?\n\nEach area is filled with a color corresponding to its data value. Higher data values are usually represented by darker colors, while lower data values are represented by lighter colors."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#getting-started",
    "title": "Hands-on Exercise 7: Visualising and Analysing Geographic Data",
    "section": "2.2 Getting Started",
    "text": "2.2 Getting Started\nIn this hands-on exercise, the key R package use is tmap package in R. Beside tmap package, four other R packages will be used. They are:\n\nreadr for importing delimited text file,\ntidyr for tidying data,\ndplyr for wrangling data and\nsf for handling geospatial data.\n\nTo install and load these packages:\n\npacman::p_load(tidyverse, sf, tmap)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#importing-data",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#importing-data",
    "title": "Hands-on Exercise 7: Visualising and Analysing Geographic Data",
    "section": "2.3 Importing Data",
    "text": "2.3 Importing Data\n\n2.3.1 The Data\nTwo data set will be used to create the choropleth map. They are:\n\nMaster Plan 2014 Subzone Boundary (Web) (i.e. MP14_SUBZONE_WEB_PL) in ESRI shapefile format. It can be downloaded at data.gov.sg This is a geospatial data. It consists of the geographical boundary of Singapore at the planning subzone level. The data is based on URA Master Plan 2014.\nSingapore Residents by Planning Area / Subzone, Age Group, Sex and Type of Dwelling, June 2011-2020 in csv format (i.e. respopagesextod2011to2020.csv). This is an aspatial data fie. It can be downloaded at Department of Statistics, Singapore. Although it does not contain any coordinates values, but it’s PA and SZ fields can be used as unique identifiers to geocode to MP14_SUBZONE_WEB_PL shapefile.\n\n\n\n2.3.2 Importing Geospatial Data into R\nThe code chunk below uses the st_read() function of sf package to import MP14_SUBZONE_WEB_PL shapefile into R as a simple feature data frame called mpsz.\nSimple Feature Data Frame\n\na data structure used for storing and manipulating spatial data.\na standard for describing the geometries of spatial data, such as points (locations), lines (paths), polygons (areas), etc.\nallows data scientists and geographic information specialists to process and analyze geospatial data in a consistent manner across various GIS software and programming environments.\nBeyond the regular data columns (such as numeric, character, and logical values), a simple feature data frame includes at least one special column (known as the geometry column) that stores the spatial geometry data for each observation which enables the simple feature data frame to directly visualize and analyze spatial data within R, without the dependency on external GIS software.\n\n\nmpsz &lt;- st_read(dsn=\"data/geospatial\",\n                layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `D:\\yanrui-w\\ISSS608-VAA\\Hands-on_Ex\\Hands-on_Ex07\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\n\n\n\n\n\n\nCode Notes\n\n\n\n\ndsn: the data source name or directory where the geospatial files are located.\nlayer: the specific layer within the shapefile\nshapefile: a popular geospatial vector data format that typically comprises several different files, which together define the attributes and geometric information of a map.\n3.1 .shp: the main file that contains the geospatial geometry data.\n3.2 .shx: the index file, which allows GIS programs to quickly find the geometries of the data.\n3.3 .dbf: the attribute file that contains attribute data associated with each shape, stored in the dBASE format.\n3.4 .prf: the projection file, which describes the coordinate system and projection information used by the geospatial data.\n\n\n\nYou can examine the content of mpsz (first 10 records) by using the code chunk below.\n\nmpsz\n\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\nFirst 10 features:\n   OBJECTID SUBZONE_NO       SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N\n1         1          1    MARINA SOUTH    MSSZ01      Y    MARINA SOUTH\n2         2          1    PEARL'S HILL    OTSZ01      Y          OUTRAM\n3         3          3       BOAT QUAY    SRSZ03      Y SINGAPORE RIVER\n4         4          8  HENDERSON HILL    BMSZ08      N     BUKIT MERAH\n5         5          3         REDHILL    BMSZ03      N     BUKIT MERAH\n6         6          7  ALEXANDRA HILL    BMSZ07      N     BUKIT MERAH\n7         7          9   BUKIT HO SWEE    BMSZ09      N     BUKIT MERAH\n8         8          2     CLARKE QUAY    SRSZ02      Y SINGAPORE RIVER\n9         9         13 PASIR PANJANG 1    QTSZ13      N      QUEENSTOWN\n10       10          7       QUEENSWAY    QTSZ07      N      QUEENSTOWN\n   PLN_AREA_C       REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR\n1          MS CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84\n2          OT CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06\n3          SR CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96\n4          BM CENTRAL REGION       CR 3775D82C5DDBEFBD 2014-12-05 26782.83\n5          BM CENTRAL REGION       CR 85D9ABEF0A40678F 2014-12-05 26201.96\n6          BM CENTRAL REGION       CR 9D286521EF5E3B59 2014-12-05 25358.82\n7          BM CENTRAL REGION       CR 7839A8577144EFE2 2014-12-05 27680.06\n8          SR CENTRAL REGION       CR 48661DC0FBA09F7A 2014-12-05 29253.21\n9          QT CENTRAL REGION       CR 1F721290C421BFAB 2014-12-05 22077.34\n10         QT CENTRAL REGION       CR 3580D2AFFBEE914C 2014-12-05 24168.31\n     Y_ADDR SHAPE_Leng SHAPE_Area                       geometry\n1  29220.19   5267.381  1630379.3 MULTIPOLYGON (((31495.56 30...\n2  29782.05   3506.107   559816.2 MULTIPOLYGON (((29092.28 30...\n3  29974.66   1740.926   160807.5 MULTIPOLYGON (((29932.33 29...\n4  29933.77   3313.625   595428.9 MULTIPOLYGON (((27131.28 30...\n5  30005.70   2825.594   387429.4 MULTIPOLYGON (((26451.03 30...\n6  29991.38   4428.913  1030378.8 MULTIPOLYGON (((25899.7 297...\n7  30230.86   3275.312   551732.0 MULTIPOLYGON (((27746.95 30...\n8  30222.86   2208.619   290184.7 MULTIPOLYGON (((29351.26 29...\n9  29893.78   6571.323  1084792.3 MULTIPOLYGON (((20996.49 30...\n10 30104.18   3454.239   631644.3 MULTIPOLYGON (((24472.11 29...\n\n\n\n\n2.3.3 Importing Attribute Data into R\nNext, we will import respopagsex2011to2020.csv file into RStudio and save the file into an R dataframe called popagsex.\n\npopdata &lt;- read_csv(\"data/aspatial/respopagesextod2011to2020.csv\")\n\n\n\n2.3.4 Data Preparation\nBefore a thematic map can be prepared, you are required to prepare a data table with year 2020 values. The data table should include the variables PA, SZ, YOUNG, ECONOMY ACTIVE, AGED, TOTAL, DEPENDENCY.\n\nYOUNG: age group 0 to 4 until age groyup 20 to 24,\nECONOMY ACTIVE: age group 25-29 until age group 60-64,\nAGED: age group 65 and above,\nTOTAL: all age group, and\nDEPENDENCY: the ratio between young and aged against economy active group\n\n\n2.3.4.1 Data Wrangling\nThe following data wrangling and transformation functions will be used:\n\npivot_wider() of tidyr package, and\nmutate(), filter(), group_by() and select() of dplyr package\n\n\npopdata2020 &lt;- popdata %&gt;%\n  filter(Time == 2020) %&gt;%\n  group_by(PA,SZ,AG) %&gt;%\n  summarise(`POP` = sum(`Pop`)) %&gt;%\n  ungroup() %&gt;%\n  pivot_wider(names_from=AG,\n              values_from=POP) %&gt;%\n  mutate(YOUNG=rowSums(.[3:6])\n         +rowSums(.[12])) %&gt;%\n  mutate(`ECONOMY ACTIVE`=rowSums(.[7:11])\n         +rowSums(.[13:15])) %&gt;%\n  mutate(`AGED`=rowSums(.[16:21])) %&gt;%\n  mutate(`TOTAL`=rowSums(.[3:21])) %&gt;%\n  mutate(`DEPENDENCY`=(`YOUNG`+`AGED`)/`ECONOMY ACTIVE`) %&gt;%\n  select(`PA`,`SZ`,`YOUNG`,`ECONOMY ACTIVE`,`AGED`,`TOTAL`,`DEPENDENCY`)\n\n\n\n2.3.4.2 Joining the attribute data and geospatial data\nBefore we can perform the georelational join, one extra step is required to convert the values in PA and SZ fields to uppercase. This is because the values of SUBZONE_N and PLN_AREA_N are in uppercase. We need to convert the ones of PA and SZ to uppercase as well so that they can be matched with SUBZONE_N and PLN_AREA_N.\n\npopdata2020 &lt;- popdata2020 %&gt;%\n  mutate_at(.vars = vars(PA,SZ),\n            .funs = funs(toupper)) %&gt;%\n  filter(`ECONOMY ACTIVE` &gt; 0)\n\n\n\n\n\n\n\nCode Notes\n\n\n\n\nwhy mutate_at(), not mutate(): mutate_at() provides a more convenient way to apply the same operation or conversion to multiple columns.\n.vars=vars(): .vars indicates which columns to be selected (could be vector of column name, vector of column index). vars() is a select helper enables you to choose multiple columns.\n.funs=funs(toupper): .funs indicates what function to be used to selected columns. funs(toupper) is the function to capitalize all characters.\nfilter(ECONOMY ACTIVE&gt; 0): to remove the rows where DEPENDENCY is NaN.\n\n\n\nNext, left_join() of dplyr is used to join the geographical data and attribute table using planning subzone name e.g. SUBZONE_N and SZ as the common identifier.\n\nmpsz_pop2020 &lt;- left_join(mpsz,popdata2020,\n                          by=c(\"SUBZONE_N\"=\"SZ\"))\n\n\n\n\n\n\n\nCode Notes\n\n\n\n\nleft_join(): is used with mpsz simple feature data frame as the left data table is to ensure that the output will be a simple features data frame.\n\n\n\nTo save the R object to directory:\n\nwrite_rds(mpsz_pop2020,\"data/rds/mpszpop2020.rds\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#section",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#section",
    "title": "Hands-on Exercise 7: Visualising and Analysing Geographic Data",
    "section": "2.5",
    "text": "2.5"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#section-1",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#section-1",
    "title": "Hands-on Exercise 7: Visualising and Analysing Geographic Data",
    "section": "2.6",
    "text": "2.6"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#section-2",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#section-2",
    "title": "Hands-on Exercise 7: Visualising and Analysing Geographic Data",
    "section": "2.6",
    "text": "2.6"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#plotting-choropleth-maps-with-tmap",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#plotting-choropleth-maps-with-tmap",
    "title": "Hands-on Exercise 7: Visualising and Analysing Geographic Data",
    "section": "2.4 Plotting Choropleth Maps with tmap",
    "text": "2.4 Plotting Choropleth Maps with tmap\nTwo approaches can be used to prepare thematic map using tmap, they are:\n\nPlotting a thematic map quickly by using qtm().\nPlotting highly customisable thematic map by using tmap elements.\n\n\n2.4.1 Plotting a choropleth map quickly by using qtm()\nThe easiest and quickest to draw a choropleth map using tmap is using qtm(). It is concise and provides a good default visualisation in many cases.\nThe code chunk below will draw a cartographic standard choropleth map as shown below.\n\ntmap_mode(\"plot\")\nqtm(mpsz_pop2020,\n    fill=\"DEPENDENCY\")\n\n\n\n\n\n\n\n\n\n\nCode Notes\n\n\n\n\ntmap_mode(): is used to set the mode of tmap.\n1.1 “plot” mode: produce a static map.\n1.2 “view” mode: produce an interactive map.\n\n\n\n\n\n2.4.2 Plotting a choropleth map by using tmap’s elements\nDespite its usefulness of drawing a choropleth map quickly and easily, the disadvantge of qtm() is that it makes aesthetics of individual layers harder to control. To draw a high quality cartographic choropleth map as shown in the figure below, tmap’s drawing elements should be used.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"Blues\",\n          title = \"Dependency ratio\")+\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45,\n            legend.width = 0.35,\n            frame = TRUE)+\n  tm_borders(alpha = 0.5)+\n  tm_compass(type=\"8star\", size = 2)+\n  tm_scale_bar()+\n  tm_grid(alpha = 0.2)+\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\",\n             position = c(\"left\",\"bottom\"))\n\n\n\n\n\n\n\n\n\n\nCode Notes\n\n\n\n\ntm_shape(): indicates the foundation layer of the map which is the mpsz_pop2020 dataset.\ntm_fill(): one filling layer based on DEPENDENCY attribute.\n2.1 style=\"quantile\": indicates the rule of change on color. In this case, the color assignment is based on the quantiles of values of DEPENDENCY.\n2.2 palette=\"Blues\": select a set of blue colors to represent values of DEPENDENCY.\n2.3 title=\"...\": set the title for legend.\n\ntm_layout(): design the overall layout and format of map.\n3.1 frame=TRUE: adds frame line to the map.\n\ntm_borders: border line layer to visually seperate each subzone.\n4.1 col=: border color.\n4.2 lwd=: border line width, default is 1.\n4.3 lty=: border line type, default is “solid”.\n4.4 alpha=: border line transparency.\n\ntm_compass(): compass layer to recognize directions.\ntm_scale_bar(): measuring scale layer.\ntm_grid(): add grid lines to the map.\ntm_credits(): add copyright info and data source.\n\n\n\n\n\n2.4.3 Data classification methods of tmap\nMost choropleth maps employ some methods of data classification. The point of classification is to take a large number of observations and group them into data ranges or classes. (different ways to group numerical data into categories or classes.)\ntmap provides a total ten data classification methods, namely: fixed, sd, equal, pretty (default), quantile, kmeans, hclust, bclust, fisher, and jenks.\n\nfixed: Class boundaries are based on specific values you choose.\nsd: Classes are created based on the standard deviation from the mean of the data. This method is useful for data that is normally distributed. If most of your data points are close to the average, they fall into one class, and points far from the average fall into other classes.\nequal: This method divides the range of your data into equal-sized intervals.\npretty (default): This method tries to create classes that are easy to understand and are “pretty” or neatly aligned to round numbers. It’s like the equal method but adjusts the class limits to more round numbers.\nQuantile: Each class contains an equal number of data points. If you have 100 data points and you want 5 classes, each class will have 20 data points, regardless of the numerical range they cover.\nKMeans: It tries to group data points into clusters based on their value so that the points in each cluster are as similar as possible.\nHClust(Hierarchical Clustering): Data is classified based on hierarchical clustering, which creates a tree of clusters. You can then decide where to cut the tree to form classes. It groups data points that are closely related into the same class.\nBClust(Birch Clustering): Similar to HClust, but specifically uses the Birch clustering algorithm. This method is efficient for large datasets and tries to build a tree structure where the final clusters can be refined to form classes.\nFisher: this method is often used to reduce the variance within classes and maximize the variance between classes. It’s particularly good for data with natural breaks or clusters.\nJenks: it’s focused on finding natural groupings in your data. The Jenks optimization method minimizes variance within classes and maximizes it between classes, ideally highlighting natural groupings or patterns in the data.\n\nTo define a data classification method, the style argument of tm_fill() or tm_polygons() will be used.\n\n2.4.3.1 Plotting choropleth maps with built-in classification methods\nThe code chunk below shows a quantile data classification that used 5 classes.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n=5,\n          style=\"jenks\")+\n  tm_borders(alpha=0.5)\n\n\n\n\nIn the code chunk below, equal data classification method is used.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n=5,\n          style = \"equal\")+\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n2.4.3.2 Plotting choropleth map with custome break\nFor all the built-in styles, the category breaks are computed internally. In order to override these defaults, the breakpoints can be set explicitly by means of the breaks argument to the tm_fill(). It is important to note that, in tmap the breaks include a minimum and maximum. As a result, in order to end up with n categories, n+1 elements must be specified in the breaks option (the values must be in increasing order).\nBefore we get started, it is always a good practice to get some descriptive statistics on the variable before setting the break points. Code chunk below will be used to compute and display the descriptive statistics of DEPENDENCY field.\n\nsummary(mpsz_pop2020$DEPENDENCY)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n 0.1111  0.7147  0.7866  0.8585  0.8763 19.0000      92 \n\n\nWith reference to the results above, we set break point at 0.60, 0.70, 0.80, and 0.90. In addition, we also need to include a minimum and maximum, which we set at 0 and 100. Our breaks vector is thus c(0, 0.60, 0.70, 0.80, 0.90, 1.00)\nNow, we will plot the choropleth map by using the code chunk below.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          breaks = c(0,0.60,0.70,0.80,0.90,1.00))+\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\nCode Notes\n\n\n\n\nWhen you use the breaks parameter, you have already explicitly specified how to group the data, so there’s no need for the style parameter to automatically determine these groups. The breaks parameter allows you to precisely control the range of each category, which is very useful when you already know how to group based on specific characteristics or values of the data.\nThe selected values of DEPENDENCY is from 0 to 1. Since the max value is 19, this value will not be displayed in the map, but it doesn’t mean any area will not be displayed because one area consists of many values.\n\n\n\n\n\n\n2.4.4 Colour Scheme\ntmap supports colour ramps either defined by the user or a set of predefined colour ramps from the RColorBrewer package.\n\n2.4.4.1 Using ColourBrewer palette\nTo change the colour, we assign the preferred colour to palette argument of tm_fill() as shown in the code chunk below.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n=6,\n          style = \"quantile\",\n          palette = \"Blues\")+\n  tm_borders(alpha=0.5)\n\n\n\n\nNotice that the choropleth map is shaded in green.\nTo reverse the colour shading, add a “-” prefix.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"-Greens\")+\n  tm_borders(alpha = 0.5)\n\n\n\n\nNotice that the colour scheme has been reversed.\n\n\n\n2.4.5 Map Layouts\n\n2.4.5.1 Map Legend\nIn tmap, several legend options are provided to change the placement, format and appearance of the legend.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          style = \"jenks\",\n          palette = \"Blues\",\n          legend.hist = TRUE,\n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1)+\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone \\n(Jenks classification)\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            legend.height = 0.45,\n            legend.width = 0.35,\n            legend.outside = FALSE,\n            legend.position = c(\"right\",\"bottom\"),\n            frame = FALSE)+\n  tm_borders(alpha=0.5)\n\n\n\n\n\n\n\n\n\n\nCode Notes\n\n\n\n\nlegend.hist=TRUE: This enables a histogram in the legend, showing the distribution of the data values across the categories defined by the Jenks classification. The histogram provides a visual representation of how many data points fall into each category.\nlegend.hist.z=0.1: This parameter adjusts the z-scale of the histogram in the legend, essentially controlling the height of the bars in the histogram relative to the size of the legend. A value of 0.1 reduces the height, making the histogram bars smaller.\n\n3 legend.is.portrait=TRUE: This setting controls the orientation of the legend. Setting it to TRUE makes the legend vertical (portrait mode). If it were FALSE, the legend would be horizontal (landscape mode).\n\nlegend.height,legend.width: legend.height sets the height of the legend relative to the height of the entire map, and legend.width sets the width relative to the width of the map. Both are expressed as fractions of the total height and width, so values of 0.45 and 0.35 mean the legend’s height is 45% of the map’s height, and its width is 35% of the map’s width, respectively. (the size of the legend)\nlegend.outside=FALSE: This determines whether the legend should be placed outside the map area. Setting it to FALSE keeps the legend inside the main map area. If it were TRUE, the legend would be placed outside the map, which could be useful if the map area is crowded or if you want a clearer separation between the map and its legend.\nlegend.position=c(\"right\",\"bottom\"): This sets the position of the legend on the map. (the position of the legend)\n\n\n\n\n\n2.4.5.2 Map style\ntmap allows a wide variety of layout settings to be changed. They can be called by using tmap_style().\nThe code chunk below shows the classic style is used.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"-Greens\")+\n  tm_borders(alpha = 0.5)+\n  tmap_style(\"classic\")\n\n\n\n\n\n\n2.4.5.3 Cartographic furniture\nBeside map style, tmap also also provides arguments to draw other map furniture such as compass, scale bar and grid lines.\nIn the code chunk below, tm_compass(), tm_scale_bar() and tm_grid() are used to add compass, scale bar and grid lines onto the choropleth map.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"Blues\",\n          title = \"No. of persons\")+\n  tm_layout(main.title = \"Distribution of Dependency Ratio \\nby planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45,\n            legend.width = 0.35,\n            frame = TRUE)+\n  tm_borders(alpha = 0.5)+\n  tm_compass(type = \"8star\", size=2)+\n  tm_scale_bar(width=0.15)+\n  tm_grid(lwd=0.1,alpha=0.2)+\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorith (URA)\\n and Population data from Department of Statistics DOS\",\n             position = c(\"left\",\"bottom\"))\n\n\n\n\n\n\n\n\n\n\nCode Notes\n\n\n\n\ntm_credits(): add a text label on the map, which is often used for attribution, to provide credits for the data source, or to include any other necessary textual information directly on the map visualization. This feature is particularly important for acknowledging sources or providing additional context to the map’s viewers.\n\n\n\nTo reset the default style, refer to the code chunk below.\n\ntmap_style(\"white\")\n\n\n\n\n2.4.6 Drawing Small Multiple Choropleth Maps\nSmall multiple maps, also referred to as facet maps, are composed of many maps arrange side-by-side, and sometimes stacked vertically. Small multiple maps enable the visualisation of how spatial relationships change with respect to another variable, such as time.\nIn tmap, small multiple maps can be plotted in three ways:\n\nby assigning multiple values to at least one of the asthetic arguments,\nby defining a group-by variable in tm_facets(), and\nby creating multiple stand-alone maps with tmap_arrange().\n\n\n2.4.6.1 By assigning multiple values to at least one of the aesthetic arguments\nIn this example, small multiple choropleth maps are created by defining ncols in tm_fill().\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(c(\"YOUNG\",\"AGED\"),\n          style=\"equal\",\n          palette=\"Blues\")+\n  tm_layout(legend.position=c(\"right\",\"bottom\"))+\n  tm_borders(alpha=0.5)+\n  tmap_style(\"white\")\n\n\n\n\nIn this example, small multiple choropleth maps are created by assigning multiple values to at least one of the aesthetic arguments.\n\ntm_shape(mpsz_pop2020)+\n  tm_polygons(c(\"DEPENDENCY\",\"AGED\"),\n              style=c(\"equal\",\"quantile\"),\n              palette=list(\"Blues\",\"Greens\"))+\n  tm_layout(legend.position = c(\"right\",\"bottom\"))\n\n\n\n\n\n\n\n\n\n\nCode Notes\n\n\n\n\nBoth tm_fill and tm_polygons can be used for filling areas on maps, but tm_polygons offers more control over the style of boundaries, making it more suitable for scenarios where it’s important to emphasize geographical borders. On the other hand, tm_fill focuses more on representing data through the color filling of areas, applicable to scenarios that require highlighting data differences rather than the clarity of borders. In practice, these two functions can be combined to achieve both the differentiation of data and the emphasis on geographical boundaries.\n\n\n\n\n\n2.4.6.2 By defining a group-by variable in tm_facets()\nIn this example, multiple small choropleth maps are created by using tm_facets()\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"Blues\",\n          thres.poly = 0)+\n  tm_facets(by=\"REGION_N\",\n            free.coords = TRUE,\n            drop.shapes = FALSE)+\n  tm_layout(legend.show = FALSE,\n            title.position = c(\"center\",\"center\"),\n            title.size = 20)+\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\nCode Notes\n\n\n\n\nthres.poly = 0: is used to set a threshold applied when drawing polygons. Here, it is set to 0, meaning that no polygons will be filtered out based on their size.\nfree.coords=TRUE: indicates that each panel can have its own coordinate axis range, allowing for the optimal display of each region.\ndrop.shapes=FALSE: means that even if some regions do not have data in the REGION_N classification, their shapes will not be removed from the map.\n\n\n\n\n\n2.4.6.3 By creating multiple stand-alone maps with tmap_arrange()\nIn this example, multiple small choropleth maps are created by creating multiple stand-alone maps with tmap_arrange()\n\nyoungmap &lt;- tm_shape(mpsz_pop2020)+\n  tm_polygons(\"YOUNG\",\n              style=\"quantile\",\n              palette=\"Blues\")\n\nagedmap &lt;- tm_shape(mpsz_pop2020)+\n  tm_polygons(\"AGED\",\n              style=\"quantile\",\n              palette=\"Blues\")\n\ntmap_arrange(youngmap,agedmap,asp=1, ncol=2)\n\n\n\n\n\n\n\n\n\n\nCode Notes\n\n\n\n\nasp=1: sets the aspect ratio (width/height) of each map in the arrangement. An aspect ratio of 1 means that the height and width of each map are equal, leading to a square shape for each map plot.\nncol=2: specifies the number of columns in the arrangement. Setting ncol=2 means that the maps will be arranged side by side in two columns.\n\n\n\n\n\n\n2.4.7 Mappping Spatial Object Meeting a Selection Criterion\nInstead of creating small multiple choropleth map, you can also use selection funtion to map spatial objects meeting the selection criterion.\n\ntm_shape(mpsz_pop2020[mpsz_pop2020$REGION_N==\"CENTRAL REGION\",])+\n  tm_fill(\"DEPENDENCY\",\n          style=\"quantile\",\n          palette = \"Blues\",\n          legend.hist = TRUE,\n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1)+\n  tm_layout(legend.outside = TRUE,\n            legend.height = 0.45,\n            legend.width = 5.0,\n            legend.position = c(\"right\",\"bottom\"),\n            frame = FALSE)+\n  tm_borders(alpha=0.5)\n\n\n\n\n\n\n\n\n\n\nCode Notes\n\n\n\n\n[mpsz_pop2020$REGION_N==\"CENTRAL REGION\", ]: [row constraints, col constraints], in this case, it filters all the cols corresponding to the constrained rows."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#getting-started-1",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#getting-started-1",
    "title": "Hands-on Exercise 7: Visualising and Analysing Geographic Data",
    "section": "3.1 Getting Started",
    "text": "3.1 Getting Started\n\npacman::p_load(sf,tmap,tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#data-wrangling-1",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#data-wrangling-1",
    "title": "Hands-on Exercise 7: Visualising and Analysing Geographic Data",
    "section": "3.2 Data Wrangling",
    "text": "3.2 Data Wrangling\n\n3.2.1 The Data\nThe data set use for this hands-on exercise is called SGPools_svy21. The data is in csv file format.\n\n\n3.2.2 Data Import and Preparation\n\nsgpools &lt;- read_csv(\"data/aspatial/SGPools_svy21.csv\")\n\nIt consists of seven columns. The XCOORD and YCOORD columns are the x-coordinates and y-coordinates of SingPools outlets and branches.\nAfter importing the data file into R, it is important for us to examine if the data file has been imported correctly.\nThe code chunk below shows list() is used to do the job.\n\nlist(sgpools)\n\n[[1]]\n# A tibble: 306 × 7\n   NAME           ADDRESS POSTCODE XCOORD YCOORD `OUTLET TYPE` `Gp1Gp2 Winnings`\n   &lt;chr&gt;          &lt;chr&gt;      &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;                     &lt;dbl&gt;\n 1 Livewire (Mar… 2 Bayf…    18972 30842. 29599. Branch                        5\n 2 Livewire (Res… 26 Sen…    98138 26704. 26526. Branch                       11\n 3 SportsBuzz (K… Lotus …   738078 20118. 44888. Branch                        0\n 4 SportsBuzz (P… 1 Sele…   188306 29777. 31382. Branch                       44\n 5 Prime Serango… Blk 54…   552542 32239. 39519. Branch                        0\n 6 Singapore Poo… 1A Woo…   731001 21012. 46987. Branch                        3\n 7 Singapore Poo… Blk 64…   370064 33990. 34356. Branch                       17\n 8 Singapore Poo… Blk 88…   370088 33847. 33976. Branch                       16\n 9 Singapore Poo… Blk 30…   540308 33910. 41275. Branch                       21\n10 Singapore Poo… Blk 20…   560202 29246. 38943. Branch                       25\n# ℹ 296 more rows\n\n\nNotice that the sgpools data in tibble data frame and not the common R data frame.\n\n\n3.2.3 Create a sf data frame from an aspatial data frame\nThe code chunk below converts sgpools data frame into a simple feature data frame by using st_as_sf() of sf packages\n\nsgpools_sf &lt;- st_as_sf(sgpools,\n                    coords = c(\"XCOORD\",\"YCOORD\"),\n                    crs=3414)\n\nNotice that a new column called geometry has been added into the data frame sgpools_sf.\n\n\n\n\n\n\nCode Notes\n\n\n\n\ncoords=: requires you to provide the column name of the x-coordinates first then followed by the column name of the y-coordinates.\ncrs=: requires you to provide the coordinates system in epsg format. EPSG: 3414 is Singapore SVY21 Projected Coordinate System. You can search for other country’s epsg code by refering to epsg.io.\n\n\n\nYou can display the basic information of the newly created sgpools_sf by using the code chunk below.\n\nlist(sgpools_sf)\n\n[[1]]\nSimple feature collection with 306 features and 5 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 7844.194 ymin: 26525.7 xmax: 45176.57 ymax: 47987.13\nProjected CRS: SVY21 / Singapore TM\n# A tibble: 306 × 6\n   NAME                         ADDRESS POSTCODE `OUTLET TYPE` `Gp1Gp2 Winnings`\n * &lt;chr&gt;                        &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;                     &lt;dbl&gt;\n 1 Livewire (Marina Bay Sands)  2 Bayf…    18972 Branch                        5\n 2 Livewire (Resorts World Sen… 26 Sen…    98138 Branch                       11\n 3 SportsBuzz (Kranji)          Lotus …   738078 Branch                        0\n 4 SportsBuzz (PoMo)            1 Sele…   188306 Branch                       44\n 5 Prime Serangoon North        Blk 54…   552542 Branch                        0\n 6 Singapore Pools Woodlands C… 1A Woo…   731001 Branch                        3\n 7 Singapore Pools 64 Circuit … Blk 64…   370064 Branch                       17\n 8 Singapore Pools 88 Circuit … Blk 88…   370088 Branch                       16\n 9 Singapore Pools Anchorvale … Blk 30…   540308 Branch                       21\n10 Singapore Pools Ang Mo Kio … Blk 20…   560202 Branch                       25\n# ℹ 296 more rows\n# ℹ 1 more variable: geometry &lt;POINT [m]&gt;\n\n\n\n\n\n\n\n\nCode Notes\n\n\n\n\nGeometry type: POINT: shows that sgppols_sf is in point feature class. It’s epsg ID is 3414. The bbox provides information of the extend of the geospatial data."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#drawing-proportional-symbol-map",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#drawing-proportional-symbol-map",
    "title": "Hands-on Exercise 7: Visualising and Analysing Geographic Data",
    "section": "3.3 Drawing Proportional Symbol Map",
    "text": "3.3 Drawing Proportional Symbol Map\nTo create an interactive proportional symbol map in R, the view mode of tmap will be used.\nThe code churn below will turn on the interactive mode of tmap.\n\ntmap_mode(\"view\")\n\n\n3.3.1 Start with an interactive point symbol map (points with same size)\nThe code chunks below are used to create an interactive point symbol map.\n\ntm_shape(sgpools_sf)+\n  tm_bubbles(col=\"red\",\n             size=1,\n             border.col=\"black\",\n             border.lwd=1)\n\n\n\n\n\n\n\n\n\n\n\n\nCode Notes\n\n\n\n\ncol=\"red\": fills the colour of points in red.\nsize=1: defines the size of points.\nborder.col = \"black\": fills the border line of points in black.\nborder.lwd=1: defines the width of border line of points.\n\n\n\n\n\n3.3.2 Make it proportional (points with different size)\nTo draw a proportional symbol map, we need to assign a numerical variable to the size visual attribute. The code chunks below show that the variable Gp1Gp2Winnings is assigned to size visual attribute.\n\ntm_shape(sgpools_sf)+\n  tm_bubbles(col=\"red\",\n             size=\"Gp1Gp2 Winnings\",\n             border.col=\"black\",\n             border.lwd=1)\n\n\n\n\n\n\n\n\n3.3.3 Give it a different colour\nThe proportional symbol map can be further improved by using the colour visual attribute. In the code chunks below, OUTLET_TYPE variable is used as the colour attribute variable.\n\ntm_shape(sgpools_sf)+\n  tm_bubbles(col=\"OUTLET TYPE\",\n             size=\"Gp1Gp2 Winnings\",\n             border.col=\"black\",\n             border.lwd=1)\n\n\n\n\n\n\n\n\n\n\n\n\nCode Notes\n\n\n\n\ncol=\"OUTLET TYPE\": assigns different colours to different outlet type variable.\n\n\n\n\n\n3.3.4 Have a twin maps\nAn impressive and little-know feature of tmap’s view mode is that it also works with faceted plots. The argument sync in tm_facets() can be used in this case to produce multiple maps with synchronised zoom and pan settings.\n\ntm_shape(sgpools_sf)+\n  tm_bubbles(col=\"OUTLET TYPE\",\n             size=\"Gp1Gp2 Winnings\",\n             border.col=\"black\",\n             border.lwd=1)+\n  tm_facets(by=\"OUTLET TYPE\",\n            nrow=1,\n            sync = TRUE)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#getting-started-2",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#getting-started-2",
    "title": "Hands-on Exercise 7: Visualising and Analysing Geographic Data",
    "section": "4.1 Getting Started",
    "text": "4.1 Getting Started\n\n4.1.1 Installing and loading packages\n\npacman::p_load(sf, tmap, tidyverse)\n\n\n\n4.1.2 Importing data\nFor the purpose of this hands-on exercise, a prepared data set called NGA_wp.rds will be used. The data set is a polygon feature data.frame providing information on water point of Nigeria at the LGA level. You can find the data set in the rds sub-direct of the hands-on data folder.\n\nSimple Feature Dataframe: provides a flexible way to handle various types of spatial data\nPolygon Feature Dataframe: offers a focused approach for processing and analyzing polygon spatial data.\n\n\nNGA_wp &lt;- read_rds(\"data/rds/NGA_wp.rds\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#analyzing-basic-choropleth-mapping",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#analyzing-basic-choropleth-mapping",
    "title": "Hands-on Exercise 7: Visualising and Analysing Geographic Data",
    "section": "4.2 Analyzing: Basic Choropleth Mapping",
    "text": "4.2 Analyzing: Basic Choropleth Mapping\n\n4.2.1 Visualizing distribution of functional water point\nPlot a choropleth map showing the distribution of function water point by LGA.\n\ntmap_mode(\"plot\")\n\np1 &lt;- tm_shape(NGA_wp)+\n  tm_fill(\"wp_functional\",\n          n=10,\n          style = \"equal\",\n          palette = \"Blues\")+\n  tm_borders(lwd=0.1,\n             alpha = 1)+\n  tm_layout(main.title = \"Distribution of functional water point by LGAs\",\n            legend.outside = FALSE)\n\np2 &lt;- tm_shape(NGA_wp)+\n  tm_fill(\"total_wp\",\n          n=10,\n          style = \"equal\",\n          palette = \"Blues\")+\n  tm_borders(lwd=0.1,\n             alpha = 1)+\n  tm_layout(main.title = \"Distribution of total water point by LGAs\",\n            legend.outside = FALSE)\n\ntmap_arrange(p2,p1,nrow=1)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#analyzing-choropleth-map-for-rates",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#analyzing-choropleth-map-for-rates",
    "title": "Hands-on Exercise 7: Visualising and Analysing Geographic Data",
    "section": "4.3 Analyzing: Choropleth Map for Rates",
    "text": "4.3 Analyzing: Choropleth Map for Rates\n\n4.3.1 Deriving Proportion of Functional Water Points and Non-Functional Water Points\nWe will tabulate the proportion of functional water points and the proportion of non-functional water points in each LGA. In the following code chunk, mutate() from dplyr package is used to derive two fields, namely pct_functional and pct_nonfunctional.\n\nNGA_wp &lt;- NGA_wp %&gt;%\n  mutate(pct_functional = wp_functional / total_wp) %&gt;%\n  mutate(pct_nonfunctional = wp_nonfunctional / total_wp)\n\n\n\n4.3.2 Plotting map of rate\nPlot a choropleth map showing the distribution of percentage functional water point by LGA.\n\ntm_shape(NGA_wp)+\n  tm_fill(\"pct_functional\",\n          n=10,\n          style = \"equal\",\n          palette = \"Blues\",\n          legend.hist = TRUE)+\n  tm_borders(lwd=0.1,\n             alpha = 1)+\n  tm_layout(main.title = \"Rate map of functional water point by LGAs\",\n            legend.outside = TRUE)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#analyzing-extreme-value-maps",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#analyzing-extreme-value-maps",
    "title": "Hands-on Exercise 7: Visualising and Analysing Geographic Data",
    "section": "4.4 Analyzing: Extreme Value Maps",
    "text": "4.4 Analyzing: Extreme Value Maps\nExtreme value maps are variations of common choropleth maps where the classification is designed to highlight extreme values at the lower and upper end of the scale, with the goal of identifying outliers. These maps were developed in the spirit of spatializing EDA, i.e., adding spatial features to commonly used approaches in non-spatial EDA (Anselin 1994).\n\n4.4.1 Percentile Map\nThe percentile map is a special type of quantile map with six specific categories: 0-1%,1-10%, 10-50%,50-90%,90-99%, and 99-100%. The corresponding breakpoints can be derived by means of the base R quantile command, passing an explicit vector of cumulative probabilities as c(0,.01,.1,.5,.9,.99,1). Note that the begin and endpoint need to be included.\n\nquantile map: shows the points in a collection of numbers that divide it into groups of equal size.\npercentile map: focuses on showing the points dividing the collection into 100 equal parts. Each part represents a percentile.\n\n\n4.4.1.1 Data Preparation\nStep 1: Exclude records with NA by using the code chunk below.\n\nNGA_wp &lt;- NGA_wp %&gt;%\n  drop_na()\n\nStep 2: Creating customised classification and extracting values.\n\npercent &lt;- c(0,.01,.1,.5,.9,.99,1)\nvar &lt;- NGA_wp[\"pct_functional\"] %&gt;%\n  st_set_geometry(NULL)\nquantile(var[,1],percent)\n\n       0%        1%       10%       50%       90%       99%      100% \n0.0000000 0.0000000 0.2169811 0.4791667 0.8611111 1.0000000 1.0000000 \n\n\n\n\n\n\n\n\nCode Notes\n\n\n\n\nst_set_geometry(NULL): to remove the spatial geometry from the data, essentially converting it from a spatial object to a regular dataframe or vector. When variables are extracted from an sf data.frame, the geometry is extracted as well. For mapping and spatial manipulation, this is the expected behavior, but many base R functions cannot deal with the geometry. Specifically, the quantile() gives an error. As a result st_set_geomtry(NULL) is used to drop geomtry field.\nvar[,1]: refers to the first column of var\nquantile(given data, give probabilities): computes the quantiles of the given data for the given probabilities. The result will be the values at the 0th (minimum), 1st, 10th, 50th (median), 90th, 99th percentiles, and 100th (maximum) of the distribution of pct_functional.\n\n\n\n\n\n4.4.1.2 Creating the get.var function\nWriting a function has three big advantages over using copy-and-paste:\n\nYou can give a function an evocative name that makes your code easier to understand.\nAs requirements change, you only need to update code in one place, instead of many.\nYou eliminate the chance of making incidental mistakes when you copy and paste (i.e. updating a variable name in one place, but not in another).\n\nSource: Chapter 19: Functions of R for Data Science.\nFirstly, we will write an R function as shown below to extract a variable (i.e. wp_nonfunctional) as a vector out of an sf data.frame.\n\narguments:\n\nvname: variable name (as character, in quotes)\ndf: name of sf data frame\n\nreturns:\n\nv: vector with values (without a column name)\n\n\n\nget.var &lt;- function(vname,df){\n  v &lt;- df[vname] %&gt;%\n    st_set_geometry(NULL)\n  v &lt;- unname(v[,1])\n  return(v)\n}\n\n\n\n4.4.1.3 A percentile mapping function\nNext, we will write a percentile mapping function by using the code chunk below.\n\npercentmap &lt;- function(vname,df,legtitle=NA,mtitle=\"Percentile Map\"){\n  percent &lt;- c(0,.01,.1,.5,.9,.99,1)\n  var &lt;- get.var(vname,df)\n  bperc &lt;- quantile(var,percent)\n  tm_shape(df)+\n    tm_polygons()+\n    tm_shape(df)+\n    tm_fill(vname,\n            title=legtitle,\n            breaks=bperc,\n            palette = \"Blues\",\n            labels = c(\"&lt;1%\",\"1%-10%\",\"10%-50%\",\"50%-90%\",\"90%-99%\",\"&gt;99%\"))+\n    tm_borders()+\n    tm_layout(main.title = mtitle,\n              title.position = c(\"right\",\"bottom\"))\n}\n\n\n\n4.4.1.4 Test drive the percentile mapping function\nTo run the function, type the code chunk as shown below.\n\npercentmap(\"total_wp\", NGA_wp)\n\n\n\n\nNote that this is just a bare bones implementation. Additional arguments such as the title, legend positioning just to name a few of them, could be passed to customise various features of the map.\n\n\n\n4.4.2 Box Map\nIn essence, a box map is an augmented quartile map, with an additional lower and upper category. When there are lower outliers, then the starting point for the breaks is the minimum value, and the second break is the lower fence. In contrast, when there are no lower outliers, then the starting point for the breaks will be the lower fence, and the second break is the minimum value (there will be no observations that fall in the interval between the lower fence and the minimum value).\n\nDisplaying summary statistics on a choropleth map by using the basic principles of boxplot.\nTo create a box map, a custom breaks specification will be used. However, there is a complication. The break points for the box map vary depending on whether lower or upper outliers are present.\n\n\nggplot(data=NGA_wp,\n       aes(x=\"\",\n           y=wp_nonfunctional))+\n  geom_boxplot()\n\n\n\n\n\n4.4.2.1 Creating the boxbreaks function\nThe code chunk below is an R function that creating break points for a box map.\n\narguments:\n\nv: vector with observations\nmult: multiplier for IQR (default 1.5)\n\nreturns:\n\nbb: vector with 7 break points compute quartile and fences\n\n\n\nboxbreaks &lt;- function(v,mult=1.5){\n  qv &lt;- unname(quantile(v))\n  iqr &lt;- qv[4] - qv[2]\n  upfence &lt;- qv[4]+mult*iqr\n  lowfence &lt;- qv[2]-mult*iqr\n  # initialize break points vector\n  bb &lt;- vector(mode=\"numeric\",length = 7)\n  # logic for lower and upper fences\n  if (lowfence &lt; qv[1]){ # no lower outliers (lowfence beyond the actual data range)\n    bb[1] &lt;- lowfence\n    bb[2] &lt;- floor(qv[1])\n  } else{\n    bb[2] &lt;- lowfence\n    bb[1] &lt;- qv[1]\n  }\n  if (upfence &gt; qv[5]){ # no upper outliers (upfence beyond the actual data range)\n    bb[7] &lt;- upfence\n    bb[6] &lt;- ceiling(qv[5])\n  } else{\n    bb[6] &lt;- upfence\n    bb[7] &lt;- qv[5]\n  }\n  bb[3:5] &lt;- qv[2:4]\n  return(bb)\n}\n\n\n\n\n\n\n\nCode Notes\n\n\n\n\nquantile(v): in quantile() function, if the second argument(a vector of given probability) is not defined, it will return a standard quantiles which is c(0%,25%,50%,75%,100%).\nunname(): a function to remove labels and keep values of a vector. In this case, the labels are the quantiles(25%,50%,etc) and the values are the ones of variable v at the corresponding quantiles.\nvector(mode=\"numeric\",length = 7): create a vector with length of 7 (7 elements), and each element is in numeric type.\nlower and upper fences beyond the actual data range:\n\n\nIf all data points are closely clustered around the quartiles, then the fences calculated according to the above rules may exceed the actual data range. In such cases, data points below the lower fence or above the upper fence would be considered outliers. If no data points exceed these fences, it means that no outliers have been identified by this method.\nThe 1.5 times IQR rule used for calculating fences is based on the assumption that the data approximately follows a normal distribution. However, actual data may exhibit different distribution characteristics, such as skewness or multimodality. In these situations, even normal variations in the data can lead to calculated fences exceeding the actual data range.\nThis situation of exceeding the actual range reveals an important feature of the fences: they are not directly determined by the actual extremities (maximum and minimum values) of the data, but are defined based on the quartiles and variability of the data distribution. This allows the fences to be somewhat independent of extreme values, serving to identify potential outliers that are far removed from the majority of data points.\n\n\nfloor(): returns a maximum integer which does not exceed the given parameter.\nceiling(): returns a minimum integer which exceeds the given parameter.\n\n\n\n\n\n4.4.2.2 Creating the grt.var function\n\nget.var &lt;- function(vname,df){\n  v &lt;- df[vname] %&gt;% st_set_geometry(NULL)\n  v &lt;- unname(v[,1])\n  return(v)\n}\n\n\n\n4.4.2.3 Test drive the newly created function\nLet’s test the newly created function.\n\nvar &lt;- get.var(\"wp_nonfunctional\", NGA_wp)\nboxbreaks(var)\n\n[1] -56.5   0.0  14.0  34.0  61.0 131.5 278.0\n\n\n\n\n4.4.2.4 Boxmap function\nThe code chunk below is an R function to create a box map.\n\narguments:\n\nvname: variable name (as character, in quotes)\ndf: simple features polygon layer\n\nlegtitle: legend title\nmtitle: map title\nmult: multiplier for IQR\nreturns: a tmap\n\n\nboxmap &lt;- function(vname,df,legtitle=NA,\n                   mtitle=\"Box Map\",\n                   mult=1.5){\n  var &lt;- get.var(vname,df)\n  bb &lt;-boxbreaks(var)\n  tm_shape(df)+\n    tm_polygons()+\n    tm_shape(df)+\n    tm_fill(vname,title=legtitle,\n            breaks=bb,\n            palette = \"Blues\",\n            labels = c(\"lower outlier\",\n                       \"&lt;25%\",\n                       \"25%-50%\",\n                       \"50%-75%\",\n                       \"&gt;75%\",\n                       \"upper outlier\"))+\n    tm_borders()+\n    tm_layout(main.title = mtitle,\n              title.position = c(\"left\",\"top\"))\n}\n\n\n\n\n\n\n\nCode Notes\n\n\n\n\ntm_shape(): Note that in this case, there are two layers, one is tm_polygens, the other is tm_fill, so we need to use two tm_shape() with each in front of each layer.\n\n\n\n\ntmap_mode(\"plot\")\nboxmap(\"wp_nonfunctional\",NGA_wp)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html",
    "title": "Hands-on Exercise 7: Visualising and Analysing Geographic Data",
    "section": "",
    "text": "roadmap_7"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html",
    "title": "Hands-on Exercise 8: Network Data Visualisation and Analysis",
    "section": "",
    "text": "roadmap_8"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#installing-and-launching-r-packages",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#installing-and-launching-r-packages",
    "title": "Hands-on Exercise 8: Network Data Visualisation and Analysis",
    "section": "2.1 Installing and launching R packages",
    "text": "2.1 Installing and launching R packages\nIn this hands-on exercise, four network data modelling and visualisation packages will be installed and launched. They are igraph, tidygraph, ggraph and visNetwork. Beside these four packages, tidyverse and lubridate, an R package specially designed to handle and wrangling time data will be installed and launched too.\n\npacman::p_load(igraph,tidygraph,ggraph,\n               visNetwork,lubridate,clock,\n               tidyverse,graphlayouts)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#the-edges-data",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#the-edges-data",
    "title": "Hands-on Exercise 8: Network Data Visualisation and Analysis",
    "section": "3.1 The edges data",
    "text": "3.1 The edges data\nGAStech-email_edges-v2.csv which consists of two weeks of 9063 emails correspondances between 55 employees.\n\n\n\nedgesdata"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#the-nodes-data",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#the-nodes-data",
    "title": "Hands-on Exercise 8: Network Data Visualisation and Analysis",
    "section": "3.2 The nodes data",
    "text": "3.2 The nodes data\nGAStech_email_nodes.csv which consist of the names, department and title of the 55 employees.\n\n\n\nnodesdata"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#importing-network-data-from-files",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#importing-network-data-from-files",
    "title": "Hands-on Exercise 8: Network Data Visualisation and Analysis",
    "section": "3.3 Importing network data from files",
    "text": "3.3 Importing network data from files\nImport GAStech_email_node.csv and GAStech_email_edges-v2.csv into RStudio environment by using read_csv() of readr package.\n\nGAStech_nodes &lt;- read_csv(\"data/GAStech_email_node.csv\")\nGAStech_edges &lt;- read_csv(\"data/GAStech_email_edge-v2.csv\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#reviewing-the-imported-data",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#reviewing-the-imported-data",
    "title": "Hands-on Exercise 8: Network Data Visualisation and Analysis",
    "section": "3.4 Reviewing the imported data",
    "text": "3.4 Reviewing the imported data\nNext, we will examine the structure of the data frame using glimpse() of dplyr.\n\nglimpse(GAStech_edges)\n\nRows: 9,063\nColumns: 8\n$ source      &lt;dbl&gt; 43, 43, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 26, 26, 26…\n$ target      &lt;dbl&gt; 41, 40, 51, 52, 53, 45, 44, 46, 48, 49, 47, 54, 27, 28, 29…\n$ SentDate    &lt;chr&gt; \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\"…\n$ SentTime    &lt;time&gt; 08:39:00, 08:39:00, 08:58:00, 08:58:00, 08:58:00, 08:58:0…\n$ Subject     &lt;chr&gt; \"GT-SeismicProcessorPro Bug Report\", \"GT-SeismicProcessorP…\n$ MainSubject &lt;chr&gt; \"Work related\", \"Work related\", \"Work related\", \"Work rela…\n$ sourceLabel &lt;chr&gt; \"Sven.Flecha\", \"Sven.Flecha\", \"Kanon.Herrero\", \"Kanon.Herr…\n$ targetLabel &lt;chr&gt; \"Isak.Baza\", \"Lucas.Alcazar\", \"Felix.Resumir\", \"Hideki.Coc…\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThe output report of GAStech_edges above reveals that the SentDate is treated as “Character” data type instead of date data type. This is an error! Before we continue, it is important for us to change the data type of SentDate field back to “Date” data type."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#wrangling-time",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#wrangling-time",
    "title": "Hands-on Exercise 8: Network Data Visualisation and Analysis",
    "section": "3.5 Wrangling time",
    "text": "3.5 Wrangling time\nThe code chunk below will be used to perform the changes.\n\nGAStech_edges &lt;- GAStech_edges %&gt;%\n  mutate(SentDate = dmy(SentDate)) %&gt;%\n  mutate(Weekday=wday(SentDate,\n                      label=TRUE,\n                      abbr=FALSE))\n\n\n\n\n\n\n\nCode Notes\n\n\n\n\ndmy(): a function of lubridate package which transforms the SentDate to Date data type.\nwday(): a function of lubridate package which returns the day of the week as a decimal number or an ordered factor if label is TRUE. The argument abbr is FALSE keep the daya spells in full, i.e. Monday. The function will create a new column in the data.frame i.e. Weekday and the output of wday() will save in this newly created field. The values in the Weekday field are in ordinal scale."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#reviewing-the-revised-data-fields",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#reviewing-the-revised-data-fields",
    "title": "Hands-on Exercise 8: Network Data Visualisation and Analysis",
    "section": "3.6 Reviewing the revised data fields",
    "text": "3.6 Reviewing the revised data fields\nTable below shows the data structure of the reformatted GAStech_edges data frame.\n\nglimpse(GAStech_edges)\n\nRows: 9,063\nColumns: 9\n$ source      &lt;dbl&gt; 43, 43, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 26, 26, 26…\n$ target      &lt;dbl&gt; 41, 40, 51, 52, 53, 45, 44, 46, 48, 49, 47, 54, 27, 28, 29…\n$ SentDate    &lt;date&gt; 2014-01-06, 2014-01-06, 2014-01-06, 2014-01-06, 2014-01-0…\n$ SentTime    &lt;time&gt; 08:39:00, 08:39:00, 08:58:00, 08:58:00, 08:58:00, 08:58:0…\n$ Subject     &lt;chr&gt; \"GT-SeismicProcessorPro Bug Report\", \"GT-SeismicProcessorP…\n$ MainSubject &lt;chr&gt; \"Work related\", \"Work related\", \"Work related\", \"Work rela…\n$ sourceLabel &lt;chr&gt; \"Sven.Flecha\", \"Sven.Flecha\", \"Kanon.Herrero\", \"Kanon.Herr…\n$ targetLabel &lt;chr&gt; \"Isak.Baza\", \"Lucas.Alcazar\", \"Felix.Resumir\", \"Hideki.Coc…\n$ Weekday     &lt;ord&gt; Monday, Monday, Monday, Monday, Monday, Monday, Monday, Mo…"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#wrangling-attributes",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#wrangling-attributes",
    "title": "Hands-on Exercise 8: Network Data Visualisation and Analysis",
    "section": "3.7 Wrangling attributes",
    "text": "3.7 Wrangling attributes\nA close examination of GAStech_edges data.frame reveals that it consists of individual e-mail flow records. This is not very useful for visualisation.\nIn view of this, we will aggregate the individual by date, senders, receivers, main subject and day of the week.\nThe code chunk:\n\nGAStech_edges_aggregated &lt;- GAStech_edges %&gt;%\n  filter(MainSubject == \"Work related\") %&gt;%\n  group_by(source,target,Weekday) %&gt;%\n  summarise(Weight=n()) %&gt;%\n  filter(source!=target) %&gt;%\n  filter(Weight&gt;1) %&gt;%\n  ungroup()"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#reviewing-the-revised-edges-file",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#reviewing-the-revised-edges-file",
    "title": "Hands-on Exercise 8: Network Data Visualisation and Analysis",
    "section": "3.8 Reviewing the revised edges file",
    "text": "3.8 Reviewing the revised edges file\nTable below shows the data structure of the reformatted GAStech_edges_aggregated data frame.\n\nglimpse(GAStech_edges_aggregated)\n\nRows: 1,456\nColumns: 4\n$ source  &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ target  &lt;dbl&gt; 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5, 6, 6, 6, 6, 7,…\n$ Weekday &lt;ord&gt; Monday, Tuesday, Wednesday, Friday, Monday, Tuesday, Wednesday…\n$ Weight  &lt;int&gt; 4, 3, 5, 8, 4, 3, 5, 8, 4, 3, 5, 8, 4, 3, 5, 8, 4, 3, 5, 8, 4,…"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#the-tbl_graph-object",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#the-tbl_graph-object",
    "title": "Hands-on Exercise 8: Network Data Visualisation and Analysis",
    "section": "4.1 The tbl_graph object",
    "text": "4.1 The tbl_graph object\nTwo functions of tidygraph package can be used to create network objects, they are:\n\ntbl_graph() creates a new tbl_graph network object from nodes and edges data.\nas_tbl_graph() converts an existed network data and objects to a tbl_graph network. Below are network data and objects supported by as_tbl_graph()\n\na node data.frame and an edge data.frame,\ndata.frame, list, matrix from base,\nigraph from igraph, - network from network,\ndendrogram and hclust from stats,\nNode from data.tree,\nphylo and evonet from ape, and\ngraphNEL, graphAM, graphBAM from graph (in Bioconductor)."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#the-dplyr-verbs-in-tidygraph",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#the-dplyr-verbs-in-tidygraph",
    "title": "Hands-on Exercise 8: Network Data Visualisation and Analysis",
    "section": "4.2 The dplyr verbs in tidygraph",
    "text": "4.2 The dplyr verbs in tidygraph\n\nactivate() verb from tidygraph serves as a switch between tibbles for nodes and edges. All dplyr verbs applied to tbl_graph object are applied to the active tibble.\nN() function is used to gain access to the node data while manipulating the edge data. Similarly .E() will give you the edge data and .G() will give you the tbl_graph object itself."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#using-tbl_graph-to-build-tidygraph-data-model",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#using-tbl_graph-to-build-tidygraph-data-model",
    "title": "Hands-on Exercise 8: Network Data Visualisation and Analysis",
    "section": "4.3 Using tbl_graph() to build tidygraph data model",
    "text": "4.3 Using tbl_graph() to build tidygraph data model\nIn this section, tbl_graph() of tidygraph package will be used to build an tidygraph’s network graph data.frame.\nBefore typing the codes, it’s recommended to review to reference guide of tbl_graph()\n\nGAStech_graph &lt;- tbl_graph(nodes=GAStech_nodes,\n                           edges=GAStech_edges_aggregated,\n                           directed =TRUE)\n\n\n\n\n\n\n\nCode Notes\n\n\n\n\ndirected=TRUE: directed graph where edges have directions."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#reviewing-the-output-tidygraphs-graph-object",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#reviewing-the-output-tidygraphs-graph-object",
    "title": "Hands-on Exercise 8: Network Data Visualisation and Analysis",
    "section": "4.4 Reviewing the output tidygraph’s graph object",
    "text": "4.4 Reviewing the output tidygraph’s graph object\n\nGAStech_graph\n\n# A tbl_graph: 54 nodes and 1456 edges\n#\n# A directed multigraph with 1 component\n#\n# Node Data: 54 × 4 (active)\n      id label               Department     Title                               \n   &lt;dbl&gt; &lt;chr&gt;               &lt;chr&gt;          &lt;chr&gt;                               \n 1     1 Mat.Bramar          Administration Assistant to CEO                    \n 2     2 Anda.Ribera         Administration Assistant to CFO                    \n 3     3 Rachel.Pantanal     Administration Assistant to CIO                    \n 4     4 Linda.Lagos         Administration Assistant to COO                    \n 5     5 Ruscella.Mies.Haber Administration Assistant to Engineering Group Mana…\n 6     6 Carla.Forluniau     Administration Assistant to IT Group Manager       \n 7     7 Cornelia.Lais       Administration Assistant to Security Group Manager \n 8    44 Kanon.Herrero       Security       Badging Office                      \n 9    45 Varja.Lagos         Security       Badging Office                      \n10    46 Stenig.Fusil        Security       Building Control                    \n# ℹ 44 more rows\n#\n# Edge Data: 1,456 × 4\n   from    to Weekday   Weight\n  &lt;int&gt; &lt;int&gt; &lt;ord&gt;      &lt;int&gt;\n1     1     2 Monday         4\n2     1     2 Tuesday        3\n3     1     2 Wednesday      5\n# ℹ 1,453 more rows"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#changing-the-active-object",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#changing-the-active-object",
    "title": "Hands-on Exercise 8: Network Data Visualisation and Analysis",
    "section": "4.5 Changing the active object",
    "text": "4.5 Changing the active object\nThe nodes tibble data frame is activated by default, but you can change which tibble data frame is active with the activate() function. Thus, if we wanted to rearrange the rows in the edges tibble to list those with the highest weight first, we could use activate() and then arrange().\nThe notion of an active tibble within a tbl_graph object makes it possible to manipulate the data in one tibble at a time.\nFor example,\n\nGAStech_graph %&gt;%\n  activate(edges) %&gt;%\n  arrange(desc(Weight))\n\n# A tbl_graph: 54 nodes and 1456 edges\n#\n# A directed multigraph with 1 component\n#\n# Edge Data: 1,456 × 4 (active)\n    from    to Weekday   Weight\n   &lt;int&gt; &lt;int&gt; &lt;ord&gt;      &lt;int&gt;\n 1    40    41 Tuesday       23\n 2    40    43 Tuesday       19\n 3    41    43 Tuesday       15\n 4    41    40 Tuesday       14\n 5    42    41 Tuesday       13\n 6    42    40 Tuesday       12\n 7    42    43 Tuesday       11\n 8    43    42 Wednesday     11\n 9    36    32 Wednesday      9\n10    40    41 Monday         9\n# ℹ 1,446 more rows\n#\n# Node Data: 54 × 4\n     id label           Department     Title           \n  &lt;dbl&gt; &lt;chr&gt;           &lt;chr&gt;          &lt;chr&gt;           \n1     1 Mat.Bramar      Administration Assistant to CEO\n2     2 Anda.Ribera     Administration Assistant to CFO\n3     3 Rachel.Pantanal Administration Assistant to CIO\n# ℹ 51 more rows\n\n\nVisit the reference guide of activate() to find out more about the function."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#plotting-a-basic-network-graph",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#plotting-a-basic-network-graph",
    "title": "Hands-on Exercise 8: Network Data Visualisation and Analysis",
    "section": "5.1 Plotting a basic network graph",
    "text": "5.1 Plotting a basic network graph\nThe code chunk below uses ggraph(), geom-edge_link() and geom_node_point() to plot a network graph by using GAStech_graph. Before getting started, it is advisable to read their respective reference guide at least once.\n\nggraph(GAStech_graph)+\n  geom_edge_link()+\n  geom_node_point()\n\n\n\n\n\n\n\n\n\n\nCode Notes\n\n\n\n\nggraph(): has two key arguments: data and layout. ggraph() can use either an igraph object or a tbl_graph object. Layout decides the way to display nodes and edges.\ngeom_edge_link(): a layer used to draw the links between nodes.\ngeom_node_point(): a layer used to draw the nodes."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#changing-the-default-network-graph-theme",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#changing-the-default-network-graph-theme",
    "title": "Hands-on Exercise 8: Network Data Visualisation and Analysis",
    "section": "5.2 Changing the default network graph theme",
    "text": "5.2 Changing the default network graph theme\nIn this section, you will use theme_graph() to remove the x and y axes.\n\ng &lt;- ggraph(GAStech_graph)+\n  geom_edge_link(aes())+\n  geom_node_point(aes())\n\ng+theme_graph()\n\n\n\n\n\n\n\n\n\n\nCode Notes\n\n\n\n\ntheme_graph(): ggraph introduces a special ggplot theme that provides better defaults for network graphs than the normal ggplot defaults. theme_graph(), besides removing axes, grids, and border, changes the font to Arial Narrow (this can be overridden).\n\n\nThe ggraph theme can be set for a series of plots with the set_graph_style() command run before the graphs are plotted or by using theme_graph() in the individual plots."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#changing-the-coloring-of-the-plot",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#changing-the-coloring-of-the-plot",
    "title": "Hands-on Exercise 8: Network Data Visualisation and Analysis",
    "section": "5.3 Changing the coloring of the plot",
    "text": "5.3 Changing the coloring of the plot\nFurthermore, theme_graph() makes it easy to change the coloring of the plot.\n\ng &lt;- ggraph(GAStech_graph)+\n  geom_edge_link(aes(colour=\"grey50\"))+\n  geom_node_point(aes(colour=\"grey40\"))\n\ng+theme_graph(background = \"grey10\",\n              text_colour = \"white\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#working-with-ggraphs-layouts",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#working-with-ggraphs-layouts",
    "title": "Hands-on Exercise 8: Network Data Visualisation and Analysis",
    "section": "5.4 Working with ggraph’s layouts",
    "text": "5.4 Working with ggraph’s layouts\nggraph support many layout for standard used, they are: star, circle, nicely (default), dh, gem, graphopt, grid, mds, spahere, randomly, fr, kk, drl and lgl. Figures below and on the right show layouts supported by ggraph()."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#fruchterman-and-reingold-layout",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#fruchterman-and-reingold-layout",
    "title": "Hands-on Exercise 8: Network Data Visualisation and Analysis",
    "section": "5.5 Fruchterman and Reingold layout",
    "text": "5.5 Fruchterman and Reingold layout\nThe code chunks below will be used to plot the network graph using Fruchterman and Reingold layout.\n\ng &lt;- ggraph(GAStech_graph,\n            layout = \"fr\")+\n  geom_edge_link(aes())+\n  geom_node_point(aes())\n\ng+theme_graph()"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#modifying-network-nodes",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#modifying-network-nodes",
    "title": "Hands-on Exercise 8: Network Data Visualisation and Analysis",
    "section": "5.6 Modifying network nodes",
    "text": "5.6 Modifying network nodes\nIn this section, I will colour each node by referring to their respective departments.\n\ng &lt;- ggraph(GAStech_graph,\n            layout = \"nicely\")+\n  geom_edge_link(aes())+\n  geom_node_point(aes(colour=Department,\n                      size=3))\n\ng+theme_graph()\n\n\n\n\n\n\n\n\n\n\nCode Notes\n\n\n\n\ngeom_node_point: is equivalent in functionality to geo_point of ggplot2. It allows for simple plotting of nodes in different shapes, colours and sizes. In the codes chnuks above colour and size are used."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#modifying-edges",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#modifying-edges",
    "title": "Hands-on Exercise 8: Network Data Visualisation and Analysis",
    "section": "5.7 Modifying edges",
    "text": "5.7 Modifying edges\nIn the code chunk below, the thickness of the edges will be mapped with the Weight variable.\n\ng &lt;- ggraph(GAStech_graph,\n            layout = \"nicely\")+\n  geom_edge_link(aes(width=Weight),\n                 alpha=0.2)+\n  scale_edge_width(range = c(0.1,5))+\n  geom_node_point(aes(colour=Department),\n                  size=3)\n\ng+theme_graph()\n\n\n\n\n\n\n\n\n\n\nCode Notes\n\n\n\n\ngeom_edge_link: draws edges in the simplest way - as straight lines between the start and end nodes. But, it can do more that that. In the example above, argument width is used to map the width of the line in proportional to the Weight attribute and argument alpha is used to introduce opacity on the line."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#working-with-facet_edges",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#working-with-facet_edges",
    "title": "Hands-on Exercise 8: Network Data Visualisation and Analysis",
    "section": "6.1 Working with facet_edges()",
    "text": "6.1 Working with facet_edges()\nIn the code chunk below, facet_edges() is used.\n\nset_graph_style()\n\ng &lt;- ggraph(GAStech_graph,\n            layout = \"nicely\")+\n  geom_edge_link(aes(width=Weight),\n                 alpha=0.2)+\n  geom_node_point(aes(colour=Department),\n                  size=2)\n\ng+facet_edges(~Weekday)\n\n\n\n\nThe code chunk below uses theme() to change the position of the legend.\n\nset_graph_style()\n\ng &lt;- ggraph(GAStech_graph,\n            layout = \"nicely\")+\n  geom_edge_link(aes(width=Weight),\n                 alpha=0.2)+\n  scale_edge_width(range=c(0.1,5))+\n  geom_node_point(aes(colour=Department),\n                  size=2)+\n  theme(legend.position = \"bottom\")\n\ng+facet_edges(~Weekday)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#a-framed-facet-graph",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#a-framed-facet-graph",
    "title": "Hands-on Exercise 8: Network Data Visualisation and Analysis",
    "section": "6.2 A framed facet graph",
    "text": "6.2 A framed facet graph\nThe code chunk below adds frame to each graph.\n\nset_graph_style()\n\ng &lt;- ggraph(GAStech_graph,\n            layout = \"nicely\")+\n  geom_edge_link(aes(width=Weight),\n                 alpha=0.2)+\n  scale_edge_width(range = c(0.1,5))+\n  geom_node_point(aes(colour=Department),\n                  size=2)\n\ng+facet_edges(~Weekday)+\n  th_foreground(foreground = \"grey80\",\n                border = TRUE)+\n  theme(legend.position = \"bottom\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#working-with-facet_nodes",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#working-with-facet_nodes",
    "title": "Hands-on Exercise 8: Network Data Visualisation and Analysis",
    "section": "6.3 Working with facet_nodes()",
    "text": "6.3 Working with facet_nodes()\nIn the code chunk below, facet_nodes() is used.\n\nset_graph_style()\n\ng &lt;- ggraph(GAStech_graph,\n            layout = \"nicely\")+\n  geom_edge_link(aes(width=Weight),\n                 alpha=0.2)+\n  scale_edge_width(range=c(0.1,5))+\n  geom_node_point(aes(colour=Department),\n                  size=2)\n\ng+facet_nodes(~Department)+\n  th_foreground(foreground = \"grey80\",\n                border = TRUE)+\n  theme(legend.position = \"bottom\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#computing-centrality-indices",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#computing-centrality-indices",
    "title": "Hands-on Exercise 8: Network Data Visualisation and Analysis",
    "section": "7.1 Computing centrality indices",
    "text": "7.1 Computing centrality indices\nCentrality measures are a collection of statistical indices use to describe the relative important of the actors are to a network. There are four well-known centrality measures, namely: degree, betweenness, closeness and eigenvector.\nRefer to Chapter 7: Actor Prominence of A User’s Guide to Network Analysis in R* to gain better understanding of theses network measures.\n\ng &lt;- GAStech_graph %&gt;%\n  mutate(betweenness_centrality = centrality_betweenness()) %&gt;%\n  ggraph(layout = \"fr\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department,\n            size=betweenness_centrality))\ng + theme_graph()\n\n\n\n\n\n\n\n\n\n\nCode Notes\n\n\n\n\ncentrality_betweenness(): the result of this function defines a property for each node which is the amount of centrality."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#visualizing-network-metrics",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#visualizing-network-metrics",
    "title": "Hands-on Exercise 8: Network Data Visualisation and Analysis",
    "section": "7.2 Visualizing network metrics",
    "text": "7.2 Visualizing network metrics\nt is important to note that from ggraph v2.0 onward tidygraph algorithms such as centrality measures can be accessed directly in ggraph calls. This means that it is no longer necessary to precompute and store derived node and edge centrality measures on the graph in order to use them in a plot.\n\ng &lt;- GAStech_graph %&gt;%\n  ggraph(layout=\"fr\")+\n  geom_edge_link(aes(width=Weight),\n                 alpha=0.2)+\n  scale_edge_width(range=c(0.1,5))+\n  geom_node_point(aes(colour=Department,\n                  size= centrality_betweenness()))\n\ng+theme_graph()"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#visualizing-community",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#visualizing-community",
    "title": "Hands-on Exercise 8: Network Data Visualisation and Analysis",
    "section": "7.3 Visualizing community",
    "text": "7.3 Visualizing community\ntidygraph package inherits many of the community detection algorithms imbedded into igraph and makes them available to us, including Edge-betweenness (group_edge_betweenness), Leading eigenvector (group_leading_eigen), Fast-greedy (group_fast_greedy), Louvain (group_louvain), Walktrap (group_walktrap), Label propagation (group_label_prop), InfoMAP (group_infomap), Spinglass (group_spinglass), and Optimal (group_optimal). Some community algorithms are designed to take into account direction or weight, while others ignore it.\nFor more about community detection, refer to Group nodes and edges based on community structure.\nIn the code chunk below group_edge_betweenness() is used.\n\ng &lt;- GAStech_graph %&gt;%\n  mutate(community = as.factor(group_edge_betweenness(weights=Weight, directed=TRUE))) %&gt;%\n  ggraph(layout=\"fr\")+\n  geom_edge_link(aes(width=Weight),\n                 alpha=0.2)+\n  scale_edge_width(range=c(0.1,5))+\n  geom_node_point(aes(colour=community))\n\ng+theme_graph()\n\n\n\n\n\n\n\n\n\n\nCode Notes\n\n\n\n\nEdge Betweenness Centrality: The number of times the shortest paths between all pairs of nodes pass through a particular edge. If an edge has a high betweenness centrality value, it means that many shortest paths go through this edge. In other words, this edge plays a significant role in connecting different parts of the network.\ncommunity detection algorithm (edge betweenness centrality based):\n2.1 Identify the edges that connect different communities in a network. These edges often have a higher edge betweenness centrality because they lie between different communities, and many of the shortest paths connecting nodes within these communities pass through them.\n2.2 Remove these edges.\n2.3 More clearly reveal the different community structures within the network."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#data-preparation",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#data-preparation",
    "title": "Hands-on Exercise 8: Network Data Visualisation and Analysis",
    "section": "8.1 Data Preparation",
    "text": "8.1 Data Preparation\nBefore we can plot the interactive network graph, we need to prepare the data model by using the code chunk below.\n\nGAStech_edges_aggregated &lt;- GAStech_edges %&gt;%\n  left_join(GAStech_nodes, by=c(\"sourceLabel\"=\"label\")) %&gt;%\n  rename(from=id) %&gt;%\n  left_join(GAStech_nodes, by=c(\"targetLabel\"=\"label\")) %&gt;%\n  rename(to=id) %&gt;%\n  filter(MainSubject==\"Work related\") %&gt;%\n  group_by(from,to) %&gt;%\n  summarise(weight=n()) %&gt;%\n  filter(from!=to) %&gt;%\n  filter(weight&gt;1) %&gt;%\n  ungroup()\n\n\n\n\n\n\n\nCode Notes\n\n\n\n\nrename(A=B): rename the existing column name B as A."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#plotting-the-first-interactive-network-graph",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#plotting-the-first-interactive-network-graph",
    "title": "Hands-on Exercise 8: Network Data Visualisation and Analysis",
    "section": "8.2 Plotting the first interactive network graph",
    "text": "8.2 Plotting the first interactive network graph\nThe code chunk below will be used to plot an interactive network graph by using the data prepared.\n\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#working-with-layout",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#working-with-layout",
    "title": "Hands-on Exercise 8: Network Data Visualisation and Analysis",
    "section": "8.3 Working with layout",
    "text": "8.3 Working with layout\nIn the code chunk below, Fruchterman and Reingold layout is used.\n\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %&gt;%\n  visIgraphLayout(layout=\"layout_with_fr\")\n\n\n\n\n\nVisit Igraph to find out more about visIgraphLayout’s argument."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#working-with-visual-attributes---nodes",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#working-with-visual-attributes---nodes",
    "title": "Hands-on Exercise 8: Network Data Visualisation and Analysis",
    "section": "8.4 Working with visual attributes - Nodes",
    "text": "8.4 Working with visual attributes - Nodes\nvisNetwork() looks for a field called “group” in the nodes object and colour the nodes according to the values of the group field.\nThe code chunk below rename Department field to group.\n\nGAStech_nodes &lt;- GAStech_nodes %&gt;%\n  rename(group=Department)\n\nWhen we rerun the code chunk below, visNetwork shades the nodes by assigning unique colour to each category in the group field.\n\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %&gt;%\n  visIgraphLayout(layout=\"layout_with_fr\") %&gt;%\n  visLegend() %&gt;%\n  visLayout(randomSeed = 123)\n\n\n\n\n\n\n\n\n\n\n\nCode Notes\n\n\n\n\nvisIgraphLayout(): defines vision configuration of the layout (how to display nodes and edges).\nvisLayout(): defines the overall layout algorithm after visIgraphLayout() setting (size, margin, whether to fix the position of nodes).\nrandomSeed=123: the algorithm will introduce randomness when calculating the position of nodes. Setting randomSeed=123 ensures the same output from multiple generations."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#working-with-visual-attributes---edges",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#working-with-visual-attributes---edges",
    "title": "Hands-on Exercise 8: Network Data Visualisation and Analysis",
    "section": "8.5 Working with visual attributes - Edges",
    "text": "8.5 Working with visual attributes - Edges\nIn the code run below visEdges() is used to symbolise the edges.\n\nThe argument arrows is used to define where to place the arrow.\nThe smooth argument is used to plot the edges using a smooth curve.\n\n\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %&gt;%\n  visIgraphLayout(layout=\"layout_with_fr\") %&gt;%\n  visEdges(arrows=\"to\",\n           smooth=list(enabled=TRUE,\n                       type=\"curvedCW\")) %&gt;%\n  visLegend() %&gt;%\n  visLayout(randomSeed = 123)\n\n\n\n\n\nVisit Option to find out more about visEdges’s argument."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#interactivity",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#interactivity",
    "title": "Hands-on Exercise 8: Network Data Visualisation and Analysis",
    "section": "8.6 Interactivity",
    "text": "8.6 Interactivity\nIn the code chunk below, visOptions() is used to incorporate interactivity features in the data visualisation.\n\nThe argument highlightNearest highlights nearest when clicking a node.\nThe argument nodesIdSelection adds an id node selection creating an HTML select element.\n\n\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %&gt;%\n  visIgraphLayout(layout=\"layout_with_fr\") %&gt;%\n  visOptions(highlightNearest = TRUE,\n             nodesIdSelection = TRUE) %&gt;%\n  visLegend() %&gt;%\n  visLayout(randomSeed = 123)\n\n\n\n\n\nVisit Option to find out more about visOption’s argument."
  },
  {
    "objectID": "In-Class_Ex/In-Class_Ex07/In-Class_Ex07.html",
    "href": "In-Class_Ex/In-Class_Ex07/In-Class_Ex07.html",
    "title": "In-Class Exercise 7: Deal with Geospatial Data",
    "section": "",
    "text": "1. Getting Started\n\npacman::p_load(sf,terra,gstat,tmap,\n               viridis,tidyverse)\n\n\n\n2. Import Data\nImport attribute data (WGS84)\n\nrfstations &lt;- read_csv(\"data/aspatial/RainfallStation.csv\")\n\nImport rainfall data and process data extraction and transformation to get monthly rainfall data.\n\nrfdata &lt;- read_csv(\"data/aspatial/DAILYDATA_202402.csv\") %&gt;%\n  select(c(1,5)) %&gt;%\n  group_by(Station) %&gt;%\n  summarise(MONTHSUM = sum(`Daily Rainfall Total (mm)`)) %&gt;%\n  ungroup()\n\nAdd in attribute data. Reference layer is rainfall data, and left join with rainfall station data.\n\nEnsure there is a same and identifier variable in both table for join statement.\n\n\nrfdata &lt;- rfdata %&gt;%\n  left_join(rfstations)\n\nAfter that, sort the variables Longitude and Latitude to check if there is missing data.\n\n\n3. Create Single Frame Data\n\nrfdata_sf &lt;- st_as_sf(rfdata,\n                      coords=c(\"Longitude\",\"Latitude\"),\n                      crs=4326) %&gt;%\n  st_transform(crs=3414)\n\nNote:\n\nThe order of coords should be longitude first, and then followed by latitude.\ncrs=4326: WGS 84 (World Geodetic System 1984)\ncrs=3414: SVY21 (Singapore Variable Coordinate System 21). Transform decimal degree into meter degree\n\n\n\n4. Create boundary map\nMultipolygen sub zone.\n\nmpsz2019 &lt;- st_read(dsn=\"data/geospatial\",\n                    layer=\"MPSZ-2019\") %&gt;%\n  st_transform(crs=3414)\n\nReading layer `MPSZ-2019' from data source \n  `D:\\yanrui-w\\ISSS608-VAA\\In-Class_Ex\\In-Class_Ex07\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 332 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 103.6057 ymin: 1.158699 xmax: 104.0885 ymax: 1.470775\nGeodetic CRS:  WGS 84\n\n\n\n\n5. Plot the themtic map\nLayer1: boundary map Layer2: rainfall data\n\ntmap_options(check.and.fix = TRUE)\ntmap_mode(\"view\")\ntm_shape(mpsz2019)+\n  tm_borders()+\n  tm_shape(rfdata_sf)+\n  tm_dots(col=\"MONTHSUM\")\n\n\n\n\n\ntmap_mode(\"plot\")\n\nNote:\n\ntm_polygen(fill=) equals to tm_fill(colour=)+tm_borders()\ntm_dots(col=\"MONTHSUM\"): col is parameter for colour code according to the variable assigned to it.\n\n\n\n6. Plot the thematic map with raster\n(xmax-xmin)/resolution chosen\n\ngrid &lt;- terra::rast(mpsz2019,\n                    nrows=690,\n                    ncols=1075)\nxy &lt;- terra::xyFromCell(grid,\n                        1:ncell(grid))\n\nCreate sf layer\nCreate the raster layer\nPredit"
  },
  {
    "objectID": "In-Class_Ex/In-Class_Ex07/data/geospatial/MPSZ-2019.html",
    "href": "In-Class_Ex/In-Class_Ex07/data/geospatial/MPSZ-2019.html",
    "title": "ISSS608-VAA",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n        0 0     false"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html",
    "href": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html",
    "title": "Take-Home Exercise 4: Prototyping Modules for Visual Analytics Shiny Application Project",
    "section": "",
    "text": "In this take-home exercise, I’m required to select one of the module of our proposed Shiny application project and complete the following tasks:\n\nTo evaluate and determine the necessary R packages needed for your Shiny application are supported in R CRAN,\nTo prepare and test the specific R codes can be run and returned the correct output as expected,\nTo determine the parameters and outputs that will be exposed on the Shiny applications, and\nTo select the appropriate Shiny UI components for exposing the parameters determine above."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#install-and-load-r-packages",
    "href": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#install-and-load-r-packages",
    "title": "Take-Home Exercise 4: Prototyping Modules for Visual Analytics Shiny Application Project",
    "section": "3.1 Install and load R packages",
    "text": "3.1 Install and load R packages\nIn this take-home exercise, I’ll use several R packages shown as below.\n\npacman::p_load(readxl, ggdist, ggridges, tidyverse,\n               ggthemes,colorspace,ggiraph, plotly, \n               DT)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#import-data",
    "href": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#import-data",
    "title": "Take-Home Exercise 4: Prototyping Modules for Visual Analytics Shiny Application Project",
    "section": "3.2 Import data",
    "text": "3.2 Import data\nI’ll import all data downloaded from website.\n\nconversion &lt;- read_xlsx(\"data/Elec_Supply_Demand.xlsx\",\"conversion\")\nsubsec_consumption &lt;- read_xlsx(\"data/Elec_Supply_Demand.xlsx\",\"subsector_consumption\")\nsubsec_account &lt;- read_xlsx(\"data/Elec_Supply_Demand.xlsx\",\"subsector_account\")\n\n\nconversion table: shows the amount of energy supplied to generate electricity and amount of electricity generated by energy from 2005 to June 2023,\nsubsec_consumption table: shows the amount of electricity consumption by sub sectors from 2005 to June 2023 and\nsubsec_account table: shows the number of electricity accounts owned by sub sectors."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#data-wrangling",
    "href": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#data-wrangling",
    "title": "Take-Home Exercise 4: Prototyping Modules for Visual Analytics Shiny Application Project",
    "section": "3.3 Data Wrangling",
    "text": "3.3 Data Wrangling\n\n3.3.1 Data Wrangling on conversion table\n1. Take a look at the data\n\nglimpse(conversion)\n\nRows: 190\nColumns: 4\n$ year            &lt;dbl&gt; 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, …\n$ energy_flow     &lt;chr&gt; \"Energy Inputs into Main Power Producers\", \"Energy Inp…\n$ energy_products &lt;chr&gt; \"Petroleum Products\", \"Natural Gas\", \"Coal and Peat\", …\n$ value_ktoe      &lt;dbl&gt; 2445.600, 4744.620, 0.000, 0.000, 0.000, 3285.701, 0.0…\n\n\nIt’s made up of 4 variables: - year: from 2005 to Jun 2023, - energy_flow: includes two direction of flow, one is energy inputs into power producers, the other is electricity generated by energy, - energy_products: indicates the type of energy used to produce electricity, - value_ktoe: amount in ktoe unit.\n2. Data Wrangling\n\nRemove sub-total and total lines\n\nNoted that the data downloaded on website is transformed from a tabular format, there are some rows representing sub-total or total information which should be removed.\n\nDivide energy_flow column into two columns(Inputs and Outputs) with corresponding values\n\nIn our project, we want to display a whole picture of the supply and demand of electricity by showing the energy inputs, electricity outputs and the conversion rate separately for each year. So it is a need to tell the inputs and outputs apart from energy_flow column, and convert it into wide format so that each of it will become an individual column and have its corresponding values of each year listed in the column.\n\nTransfer ktoe into GWh\n\nGWh is the more frequent used unit to describe electricity. I’ll transfer the values in ktoe into Gwh (1ktoe=11.63GWh)\n\nCalculate conversion rate\n\nBesides showing the inputs and outputs, it will be clearer to exam the performance of generation by showing the conversion rate.\n\nRemove data for the year 2023\n\nThe data for 2023 is as at Jun 2023. It is inconsistent to take the half year data into comparison while others are the whole year data. So the data for 2023 should be removed.\nThe code of data wrangling is shown as below:\n\n# Remove sub-total and total lines\nconversion &lt;- conversion %&gt;%\n  filter(energy_products != \n           \"Others (Of which: Biomass excluding Municipal Waste)\")\n\n# Remove irrelevant columns\nconversion &lt;- select(conversion, -energy_products)\n\n# Divide energy_flow into two columns\nconversion &lt;- conversion %&gt;%\n  mutate(energy_flow = case_when(\n    str_detect(energy_flow, \"Inputs\") ~ \"Inputs\",\n    str_detect(energy_flow, \"Gross\") ~ \"Outputs\",\n    TRUE ~ energy_flow \n  )) %&gt;%\n  pivot_wider(\n    names_from = energy_flow,\n    values_from = value_ktoe,\n    values_fn = sum\n  )\n\n# Transfer ktoe into GWh and calculate conversion rate\nconversion &lt;- conversion %&gt;%\n  mutate(\n    Inputs=round(Inputs*11.63,0),\n    Outputs=round(Outputs*11.63,0),\n    Conversion_rate=round(Outputs/Inputs,3)\n  )\n\n# Remove data for 2023\nconversion &lt;- conversion %&gt;%\n  filter(year != 2023)\n\nAfter data wrangling, the variables in conversion table are:\n\nyear: from 2005 to 2022,\nInputs: the amount of energy used to generate electricity (GWh),\nOutputs: the amount of electricity generated by energy (GWh) and\nConversion_rate: conversion rate for each year.\n\n\n\n3.3.2 Data Wrangling on subsec_consumption table\n1. Take a look at the data\n\nglimpse(subsec_consumption)\n\nRows: 323\nColumns: 4\n$ year            &lt;dbl&gt; 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, …\n$ sector          &lt;chr&gt; \"Commerce and Services-related\", \"Commerce and Service…\n$ sub_sector      &lt;chr&gt; \"Accommodation and Food Services\", \"Commerce and Servi…\n$ consumption_GWh &lt;dbl&gt; 1031.3, 13075.6, 1291.9, 667.2, 3612.7, 650.2, 3712.4,…\n\n\nIt’s made up of 4 variables: - year: from 2005 to Jun 2023, - sector: main sectors, - sub_sector: main sub sectors under each sector, - consumption_GWh: amount of consumption of each sub sector in GWh unit.\n2. Data Wrangling\n\nRemove sub-total and total lines\n\nSame as above.\n\nRemove data for the year 2023\n\nSame as above.\nThe code of data wrangling is shown as below:\n\n# Remove sub-total and total lines\nsubsec_consumption &lt;- subsec_consumption %&gt;%\n  filter(!sub_sector %in% c(\n    \"Commerce and Services-related\", \n    \"Industrial-related\",\n    \"Overall\"))\n\n# Remove data for 2023\nsubsec_consumption &lt;- subsec_consumption %&gt;%\n  filter(year != 2023)\n\n\n\n3.3.3 Data Wrangling on subsec_account table\n1. Take a look at the data\n\nglimpse(subsec_account)\n\nRows: 323\nColumns: 4\n$ year                           &lt;dbl&gt; 2005, 2005, 2005, 2005, 2005, 2005, 200…\n$ sector                         &lt;chr&gt; \"Commerce and Services-related\", \"Comme…\n$ sub_sector                     &lt;chr&gt; \"Accommodation and Food Services\", \"Com…\n$ Number_of_Electricity_Accounts &lt;dbl&gt; 7700, 100770, 3860, 8590, 20260, 8380, …\n\n\nIt’s made up of 4 variables: - year: from 2005 to Jun 2023, - sector: main sectors, - sub_sector: main sub sectors under each sector, - Number_of_Electricity_Accounts: number of electricity accounts owned by each sub sector.\n2. Data Wrangling\n\nRemove sub-total and total lines\n\nSame as above.\n\nRemove data for the year 2023\n\nSame as above.\n\nCombine two tables\n\nIn our project, we want to put the consumption and number of accounts together to cross-review the consumption status.\nThe code of data wrangling is shown as below:\n\n# Remove sub-total and total line, data for 2023\nsubsec_account &lt;- subsec_account %&gt;%\n  filter(!sub_sector %in% c(\n    \"Commerce and Services-related\", \n    \"Industrial-related\",\n    \"Overall\")) %&gt;%\n  filter(year != 2023)\n\n# Combine two tables\nconsump_account &lt;- subsec_consumption %&gt;%\n  left_join(subsec_account, by = c(\"year\", \"sub_sector\"))\n\n# Rename columns\nconsump_account &lt;- consump_account %&gt;%\n  select(-sector.y) %&gt;%\n  rename(\"sector\"=\"sector.x\") %&gt;%\n  rename(\"Account_number\"=\"Number_of_Electricity_Accounts\",\n         \"consumption\"=\"consumption_GWh\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#visualization-on-electricity-generation",
    "href": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#visualization-on-electricity-generation",
    "title": "Take-Home Exercise 4: Prototyping Modules for Visual Analytics Shiny Application Project",
    "section": "4.1 Visualization on Electricity Generation",
    "text": "4.1 Visualization on Electricity Generation\n\nVisualization methods\n\nI choose grouped bar chart to display the absolute amount of energy input and electricity output together and line chart to show the conversion rate for each year. In this way, readers can get to know not only the amount of input and output but also the performance of electricity generation of each year.\n\nX-axis and Y-axis\n\nThe x-axis represents the years, the left y-axis represents the amount of inputs and outputs in GWh, and the right y-axis represents the conversion rate.\n\nScale\n\nThe input amount is around 80,000-120,000 GWh, the output amount is around 30,000-60,000 GWh, while the conversion rate is around 40%-50%. To better display both charts without disturbing each other, I set the scale of left y-axis (amount of input and output) to be from 0-120,000 GWh, and the scale of right y-axis (conversion rate) to be from 0%-50%.\n\nInteractivity\n\nI create interactive visualization by showing the detail text when hovering the mouse pointer on each bar and point on the line.\nThe code is shown as below:\n\nconversion &lt;- conversion %&gt;%\n  mutate(Fyear=factor(year))\n\n# Plot 1\np1 &lt;- conversion %&gt;%\n  plot_ly(x=~Fyear,\n          y=~Inputs,\n          text =~Inputs, \n          textposition = 'auto',\n          type = \"bar\",\n          name=\"Energy Inputs(GWh)\",\n          marker=list(color='rgb(49,130,189)')) %&gt;%\n  add_trace(y=~Outputs,\n            text =~Outputs, \n            textposition = 'auto',\n            name=\"Electricity Outputs(GWh)\",\n            marker=list(color='rgb(204,204,204)')) %&gt;%\n  add_trace(y=~Conversion_rate,\n            yaxis='y2',\n            type=\"scatter\",\n            mode=\"lines+markers\",\n            name=\"Conversion rate\",\n            line = list(color = 'rgba(67,67,67,1)', \n                        width = 1)) %&gt;%\n  layout(barmode = \"group\",\n         title = \"Review on Electricity Generation\",\n         xaxis = list(title = \"\",tickangle=-45, tickfont=list(size=8)),\n         yaxis = list(title=\"\", tickfont=list(size=8)),\n         yaxis2 = list(title=\"\",\n                       tickfont=list(size=8),\n                       overlaying=\"y\",\n                       side=\"right\",\n                       range = c(0, 0.5),\n                       tickformat = \".0%\",\n                       showgrid = FALSE),\n         legend=list(font=list(size=10)))\np1"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#visualization-on-electricity-consumption",
    "href": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#visualization-on-electricity-consumption",
    "title": "Take-Home Exercise 4: Prototyping Modules for Visual Analytics Shiny Application Project",
    "section": "4.2 Visualization on Electricity Consumption",
    "text": "4.2 Visualization on Electricity Consumption\n\nVisualization methods\n\nI choose bubble plot to display the cross-review on electricity consumption and number of electricity accounts by each sub sector.\n\nX-axis and Y-axis\n\nThe x-axis represents the number of electricity accounts, the y-axis represents the amount of electricity consumption. Points will be coloured in terms of sub sector.\n\nInteractivity\n\nI create interactive visualization by showing the movement over the years.\nThe code is shown as below:\n\n# plot 2: \nbp &lt;- consump_account %&gt;%\n  plot_ly(x=~Account_number,\n          y=~consumption,\n          size=2,\n          color=~sub_sector,\n          frame=~year,\n          text=~paste(sub_sector, \"&lt;br&gt;No. of Accounts:\", Account_number,\"&lt;br&gt;Consumption:\",consumption),\n          hoverinfo=\"text\",\n          type=\"scatter\",\n          mode=\"markers\",\n          alpha = 0.8) %&gt;%\n  layout(showlegend=TRUE,\n         hovermode=\"closest\",\n         legend = list(font = list(size = 8)),\n         title=\"Review on Electricity Consumption and Number of Accounts by Sub-sector\",\n         xaxis=list(title=\"No. of Accounts\",\n                    titlefont = list(size = 8),\n                    tickfont = list(size = 8)),\n         yaxis=list(title=\"Consumption\",\n                    titlefont = list(size = 8),\n                    tickfont = list(size = 8)))\nbp"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#draft-storyboard",
    "href": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#draft-storyboard",
    "title": "Take-Home Exercise 4: Prototyping Modules for Visual Analytics Shiny Application Project",
    "section": "5.1 Draft storyboard",
    "text": "5.1 Draft storyboard\nThe draft storyboard is shown as below:\n\n\n\ndraft_storyboard"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#ui-design-1",
    "href": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#ui-design-1",
    "title": "Take-Home Exercise 4: Prototyping Modules for Visual Analytics Shiny Application Project",
    "section": "5.2 UI Design",
    "text": "5.2 UI Design\nThe UI design is shown as below:\n\n\n\nui_design\n\n\n\n\n\nui_design_2"
  }
]